<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  

  <title>Support Vector Machine | Machine Learning Medium</title>
  <meta name="description" content="A SVM is a discriminative classifier formally defined by a separating hyperplane. Given labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
  
  <meta property="og:site_name" content="Machine Learning Medium" />
  <meta property="og:title" content="Support Vector Machine"/>
  
  <meta property="og:description" content="A SVM is a discriminative classifier formally defined by a separating hyperplane. Given labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples." />
  
  <meta property="og:image" content="https://machinelearningmedium.com/assets/images/hyperplane.png" />
  <meta property="og:url" content="https://machinelearningmedium.com/2018/04/10/support-vector-machines/" >
  <meta property="og:type" content="blog" />
  <meta property="article:published_time" content="2018-04-10T00:00:00+00:00">

  <link rel="canonical" href="https://machinelearningmedium.com/2018/04/10/support-vector-machines/"/>
  <link rel="shortcut icon" href="/public/fav.png" type="image/png"/>
  <link rel="stylesheet" href="//brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
  <link rel="stylesheet" type="text/css" href="/css/custom.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/share_bar.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/syntax.css" />
  <link href="https://fonts.googleapis.com/css?family=Fira+Sans:400,600|Open+Sans:400,700" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" >
  <link rel="shortcut icon" href="/public/fav.ico?v1">


  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

</head>

  <body itemscope itemtype="http://schema.org/Article">
    <div class="metabar metabar-header">
  <div class="metabar-inner">
    <div class="quote-div">
      <a class="icon-quote" href="/">
        <i class="fa fa-quote-left fa-pull-left fa-border" aria-hidden="true"></i>
      </a>
    </div>
  </div>
</div>

<div class="sectionbar sectionbar-header">
  <div class="sectionbar-inner">
    <div class="table">
      <div class="category-links table-cell">
        
          <a href=/>Home</a>
        
        
        <a href="/tag/machine-learning">Machine Learning</a>
        
        <a href="/tag/mathematics">Mathematics</a>
        
        <a href="/tag/papers">Papers</a>
        
        <span class="right-padding-22">|</span>
        <a href=/collections/ class="">Collections</a>
        <a href=/tags/ class="">Tags</a>
        
          <span class="right-padding-22">|</span>
          <a class="icon-search" href="/search/"><i class="fa fa-search"></i></a>
        
      </div>
      <div class="social-links table-cell">
        <!-- 
          
              <a class="icon-github-alt" href="https://github.com/shams-sam/shams-sam.github.io"  title="Gihub Project" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-github-alt fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-facebook" href="https://facebook.com/shams-sam"  title="Facebook" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-facebook fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-twitter" href="https://twitter.com/sshamssam"  title="Twitter" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-twitter fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-linkedin" href="https://linkedin.com/in/shams-sheikh-20328154"  title="LinkedIn" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-envelope" href="mailto:shams.sam@live.com"  title="E-mail" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-envelope fa-stack-1x"></i>
                </span>
              </a>
          
         -->
        <div class="fb-like" data-href="https://www.facebook.com/machinelearningmedium/" data-layout="button" data-action="like" data-size="large" data-show-faces="false" data-share="false"></div>
        <a href="https://twitter.com/sshamssam" class="twitter-follow-button" data-size="large" data-show-screen-name="false" data-dnt="true" data-show-count="false"></a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
      </div>
    </div>
  </div>
</div>
    <div class="author-strip">
      <div class="author-strip-inner">
        <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
        <div class="author-detail">
          <h4 class="author-name" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/about/" style="color: #000000; text-decoration: none;">Shams S</a></h4>
          <h5 class="author-name"> Research Assistant @ Purdue University </h5>
          <time datetime="2018-04-10T00:00:00+00:00">Apr 10</time>
          <span class="middot">&#183;</span>
          <div class="post-reading">
            <span class="post-reading-time"></span> read
          </div>
        </div>
      </div>
    </div>
    
    <div class="postcover">
    <div class="postimage">
        <div class="postimage-image"  style="background-image: url(/assets/images/hyperplane.png) ">
        </div>
    </div>
    
    <figcaption>Image Source: http://cfss.uchicago.edu/persp009_svm_files/figure-html/hyperplane-1.png</figcaption>
    
    </div>
    <main class="content post-content" role="main">
      <article class="post">
        <div class="noarticleimage">
          <div class="post-meta">
            <h1 class="post-title">Support Vector Machine</h1>
            <section class="share">
            <span id="share-bar">

    <span><i class="fa fa-share-alt"></i></span>

    <span class="share-buttons">
        <span>
        <a  href="https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmedium.com/2018/04/10/support-vector-machines/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Facebook" >
            <i class="fa fa-facebook-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://twitter.com/intent/tweet?text=Support Vector Machine&url=https://machinelearningmedium.com/2018/04/10/support-vector-machines/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Twitter" >
            <i class="fa fa-twitter-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://plus.google.com/share?url=https://machinelearningmedium.com/2018/04/10/support-vector-machines/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Google+" >
            <i class="fa fa-google-plus-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://pinterest.com/pin/create/button/?url=https://machinelearningmedium.com/2018/04/10/support-vector-machines/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Pinterest" >
            <i class="fa fa-pinterest-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.tumblr.com/share/link?url=https://machinelearningmedium.com/2018/04/10/support-vector-machines/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Tumblr" >
            <i class="fa fa-tumblr-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.reddit.com/submit?url=https://machinelearningmedium.com/2018/04/10/support-vector-machines/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Reddit" >
            <i class="fa fa-reddit-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://www.linkedin.com/shareArticle?mini=true&url=https://machinelearningmedium.com/2018/04/10/support-vector-machines/&title=Support Vector Machine&summary=A SVM is a discriminative classifier formally defined by a separating hyperplane. Given labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples.&source=Machine Learning Medium"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on LinkedIn" >
            <i class="fa fa-linkedin-square"></i>
        </a>
        </span>
        <span>
        <a  href="mailto:?subject=Support Vector Machine&amp;body=Check out this site https://machinelearningmedium.com/2018/04/10/support-vector-machines/"
            title="Share via Email" >
            <i class="fa fa-envelope-square"></i>
        </a>
        </span>
    </span>

</span>
          </section>
            <h2 class="post-description">A SVM is a discriminative classifier formally defined by a separating hyperplane. Given labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples.</h2>
          </div>
        </div>
        <div class="horizontal-divider">&#183; &#183; &#183;</div>
        <section class="post-content">
          <a name="topofpage"></a>
          <h3 id="basics-of-machine-learning-series">Basics of Machine Learning Series</h3>

<blockquote>
  <p><a href="/collection/basics-of-machine-learning">Index</a></p>
</blockquote>

<div class="horizontal-divider">· · ·</div>

<h3 id="optimization-objective">Optimization Objective</h3>

<p>The support vector machine objective can seen as a modification to the cost of logistic regression. Consider the sigmoid function, given as,</p>

<script type="math/tex; mode=display">h_\theta(x) = \frac {1} {1 + e^{-z}} \tag{1} \label{1}</script>

<p>where \(z = \theta^T x \)</p>

<p>The cost function of logistic regression as in the post <a href="/2017/09/02/logistic-regression-model/#mjx-eqn-6"><strong>Logistic Regression Model</strong></a>, is given by,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
J(\theta) &= -{1 \over m} \sum_{i=1}^m \left( y^{(i)}\,log(h_\theta(x^{(i)}) + (1-y^{(i)})\,log(1 - h_\theta(x^{(i)})) \right) \\
    &= -{1 \over m} \sum_{i=1}^m \left( y^{(i)}\,log(\frac {1} {1 + e^{-\theta^T x}}) + (1-y^{(i)})\,log(1 - \frac {1} {1 + e^{-\theta^T x}}) \right)
\tag{2} \label{2}
\end{align} %]]></script>

<p>Each training instance contributes to the cost function the following term,</p>

<script type="math/tex; mode=display">-y\,log(\frac {1} {1 + e^{-z}}) - (1-y)\,log(1 - \frac {1} {1 + e^{-z}})</script>

<p>So when \(y = 1\), the contributed term is \(-log(\frac {1} {1 + e^{-z}})\), which can be seen in the plot below. The cost function of SVM, denoted as \(cost_1(z)\), is a modification the former and a close approximation.</p>

<p><img src="/assets/2018-04-10-support-vector-machines/fig-1-svm-cost-at-y-1.png" alt="Fig-1. SVM Cost function at y = 1" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">svm_cost_1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span> <span class="k">if</span> <span class="n">_</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="k">else</span> <span class="o">-</span><span class="mf">0.26</span><span class="o">*</span><span class="p">(</span><span class="n">_</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">negative</span><span class="p">(</span><span class="n">x</span><span class="p">)))))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">svm_cost_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'logistic regression cost function'</span><span class="p">,</span> <span class="s">'modified SVM cost function'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Similarly, when \(y = 0\), the contributed term is \(-log(1 - \frac {1} {1 + e^{-z}})\), which can be seen in the plot below. The cost function of SVM, denoted as \(cost_0(z)\), is a modification the former and a close approximation.</p>

<p><img src="/assets/2018-04-10-support-vector-machines/fig-2-svm-cost-at-y-0.png" alt="Fig-2. SVM Cost function at y = 0" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">svm_cost_0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span> <span class="k">if</span> <span class="n">_</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="mf">0.26</span><span class="o">*</span><span class="p">(</span><span class="n">_</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">negative</span><span class="p">(</span><span class="n">x</span><span class="p">))))))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">svm_cost_0</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'logistic regression cost function'</span><span class="p">,</span> <span class="s">'modified SVM cost function'</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<blockquote>
  <p>While the slope the straight line is not of as much importance, it is the linear approximation that gives SVMs computational advantages that helps in formulating an easier optimization problem.</p>
</blockquote>

<p>Regularized version of \eqref{2} can from the post <a href="/2017/09/15/regularized-logistic-regression/#mjx-eqn-1"><strong>Regularized Logistic Regression</strong></a> can rewritten as,</p>

<script type="math/tex; mode=display">J(\theta) = {1 \over m} \sum_{i=1}^m \left( y^{(i)}\,(-log(h_\theta(x^{(i)}))) + (1-y^{(i)})\,(-log(1 - h_\theta(x^{(i)}))) \right) + {\lambda \over 2m } \sum_{j=1}^n \theta_j^2  \tag{3} \label{3}</script>

<p>In order to come up with the cost function for the SVM, \eqref{3} is modified by replacing the corresponding cost terms, which gives,</p>

<script type="math/tex; mode=display">J(\theta) = {1 \over m} \sum_{i=1}^m \left( y^{(i)}\,cost_1(z) + (1-y^{(i)})\,cost_0(z) \right) + {\lambda \over 2m } \sum_{j=1}^n \theta_j^2 \tag{4} \label{4}</script>

<p>Following the conventions of SVM the following modifications are made to the cost in \eqref{4}, which effectively is a change in notation but not the underlying logic,</p>
<ul>
  <li>removing \({1 \over m}\) does not affect the minimization logic at all as the minima of a function is not changed by the linear scaling.</li>
  <li>change the form of parameterization from \(A + \lambda B\) to \(CA + B\) where it can be intuitively thought that \(C = {1 \over \lambda}\).</li>
</ul>

<p>After applying the above changes, \eqref{4} gives,</p>

<script type="math/tex; mode=display">J(\theta) = C \sum_{i=1}^m \left[ y^{(i)}\,cost_1(\theta^T x^{(i)}) + (1-y^{(i)})\,cost_0(\theta^T x^{(i)}) \right] + {1 \over 2 } \sum_{j=1}^n \theta_j^2 \tag{5} \label{5}</script>

<p>The SVM hypothesis does not predict probability, instead gives hard class labels,</p>

<script type="math/tex; mode=display">h_\theta(x) = 
\begin{cases}
1 \text{, if } \theta^Tx \geq 0 \\
0 \text{, otherwise}
\end{cases}
\tag{6} \label{6}</script>

<h3 id="large-margin-intuition">Large Margin Intuition</h3>

<p><img src="/assets/2018-04-10-support-vector-machines/fig-3-cost-plots.png?raw=true" alt="Fig-3. SVM Cost function plots" /></p>

<p>According to \eqref{5} and the plots of the cost function as shown in the image above, the following are two desirable states for SVM,</p>

<ul>
  <li>if \(y=1\), then \(\theta^Tx \geq 1\) (not just \(\geq 0\))</li>
  <li>if \(y=0\), then \(\theta^Tx \leq -1\) (not just \(\lt 0\))</li>
</ul>

<p>Let C in \eqref{5} be a large value. Consequently, in order to minimize the cost, the corresponding term \(\sum_{i=1}^m \left[ y^{(i)}\,cost_1(\theta^T x^{(i)}) + (1-y^{(i)})\,cost_0(\theta^T x^{(i)}) \right]\) must be close to 0.</p>

<p>Hence, in order to minimize the cost function, when \(y=1\), \(cost_1(\theta^T x)\) should be 0, and similarly, when \(y=0\), \(cost_0(\theta^T x)\) should be 0. And thus, from the plots in Fig.3, it is clear that it can only fulfilled by the two states listed above.</p>

<p>Following the above intuition, the cost function can we written as,</p>

<script type="math/tex; mode=display">min_\theta J(\theta) = min_\theta {1 \over 2 } \sum_{j=1}^n \theta_j^2 \tag{7} \label{7}</script>

<p>subject to contraints,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\theta^Tx^{(i)} &\geq 1 \text{, if } y^{(i)}=1 \\
\theta^Tx^{(i)} &\leq -1 \text{, if } y^{(i)}=0
\end{align} %]]></script>

<p>What this basically leads to is the selection of a decision boundary that tries to maximize the margin from the support vectors as shown in the plot below. This maximization of the margin as seen for decision boundary A increases the robustness over decision boundaries with lesser margins like B. And it is this property of the SVMs that attributes the name <strong>large margin classifier</strong> to it.</p>

<p><img src="/assets/2018-04-10-support-vector-machines/fig-4-large-margin-decision-boundary.png?raw=true" alt="Fig-4. Large Margin Decision Boundary" width="50%" /></p>

<h3 id="effect-of-parameter-c">Effect of Parameter C</h3>

<p><img src="/assets/2018-04-10-support-vector-machines/fig-5-effect-of-regularization.png?raw=true" alt="Fig-5. Effect of Parameter C" width="50%" /></p>

<p>As discussed in the <a href="#optimization-objective">section</a> above, the effect of C can be considered as reciprocal of regularization parameter, \(\lambda\). This is more clear from Fig-5. A single outlier, can make the model choose the decision boundary with smaller margin if the value of C is large. A small value of C ensures that the outliers are overlooked and best approximation of large margin boundary is determined.</p>

<h3 id="mathematical-background">Mathematical Background</h3>

<p><strong>Vector Inner Product:</strong> Consider two vectors, \(v\) and \(w\), given by,</p>

<script type="math/tex; mode=display">v = \begin{bmatrix}v_1 \\ v_2 \end{bmatrix}</script>

<script type="math/tex; mode=display">w = \begin{bmatrix}w_1 \\ w_2 \end{bmatrix}</script>

<p>Then, the <strong>inner product</strong> or the <strong>dot product</strong> is defined as \(v^Tw = w^Tv\).</p>

<p><strong>Norm</strong> of a vector, \(v\), denoted as \(\lVert v\rVert\) is the euclidean length of the vector given by the pythagoras theorem as,</p>

<script type="math/tex; mode=display">\lVert v\rVert = \sqrt{\sum_{i=0}^n v_i^2} \in \mathbb{R} \tag{8} \label{8}</script>

<p>The inner product can also be defined as,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{Inner_Product(v, w)} &= v^Tw = w^Tv = \sum_{i=0}^n v_i \cdot w_i \\
    &= \lVert v\rVert \cdot \lVert w\rVert \cdot cos \theta = p \cdot \lVert v\rVert \tag{9} \label{9}
\end{align} %]]></script>

<p>where \(p=\lVert w\rVert \cdot cos \theta\) can be described as the projection of vector \(w\) onto vector \(v\) which can be either positive or negative signed based on the angle \(\theta\) between the vectors as shown in the image below.</p>

<p><img src="/assets/2018-04-10-support-vector-machines/fig-6-dot-product.jpg?raw=true" alt="Fig-6. Dot Product" width="50%" /></p>

<p><strong>SVM Decision Boundary:</strong> From \eqref{7}, the optimization statement can be written as,</p>

<script type="math/tex; mode=display">min_\theta \, {1 \over 2 } \sum_{j=1}^n \theta_j^2 \tag{10} \label{10}</script>

<p>subject to contraints,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\theta^Tx^{(i)} &\geq 1 \text{, if } y^{(i)}=1 \\
\theta^Tx^{(i)} &\leq -1 \text{, if } y^{(i)}=0
\end{align}
\tag{11} \label{11} %]]></script>

<p>Let \(\theta_0 = 0\) and \(n=2\), i.e. number of features is 2 for simplicity, then \eqref{10} can be written as,</p>

<script type="math/tex; mode=display">min_\theta \, {1 \over 2 } (\theta_1^2 + \theta_1^2) = {1 \over 2 } \sqrt{(\theta_1^2 + \theta_1^2)}^2 =  {1 \over 2 } \lVert \theta \rVert^2 \tag{12} \label{12}</script>

<p>Using \eqref{9}, \(\theta^Tx^{(i)}\) in \eqref{11} can be written as,</p>

<script type="math/tex; mode=display">\theta^Tx^{(i)} = p^{(i)} \cdot \lVert \theta \rVert \tag{13} \label{13}</script>

<p>The plot of \eqref{13} can be seen below,</p>

<p><img src="/assets/2018-04-10-support-vector-machines/fig-7-dot-product-in-svm.png?raw=true" alt="Fig-7. Dot Product in SVM" width="50%" /></p>

<p>Hence, using \eqref{12} and \eqref{13}, the optimization objective in \eqref{10} and the constraints in \eqref{11} are written as,</p>

<script type="math/tex; mode=display">min_\theta \, {1 \over 2 } \lVert \theta \rVert^2 \tag{14} \label{14}</script>

<p>subject to contraints,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p^{(i)} \cdot \lVert \theta \rVert &\geq 1 \text{, if } y^{(i)}=1 \\
p^{(i)} \cdot \lVert \theta \rVert &\leq -1 \text{, if } y^{(i)}=0
\end{align}
\tag{15} \label{15} %]]></script>

<p>where \(p^{(i)}\) is the projection of \(x^{(i)}\) onto vector \(\theta\).</p>

<p>Consider two decision boundaries, A and B, and their respective perpendicular parameters, \(\theta_A\) and \(\theta_B\) as shown in the plot below. As a consequence of choosing \(\theta_0 = 0\) for simplification, all the corresponding decision boundaries pass through the origin.</p>

<p><img src="/assets/2018-04-10-support-vector-machines/fig-8-choosing-large-margin.png?raw=true" alt="Fig-8. Choosing Large Margin Classifier" width="50%" /></p>

<p>Based on the two training examples of either class chosen, close to the boundaries, it can be seen that the magnitude of projection is more in case of \(\theta_B\) than \(\theta_A\). This basically tells that it would be possible to choose smaller values of \(\theta\) and satisfy \eqref{14} and \eqref{15} if the value of projection \(p\) is bigger and hence, the decision boundary, B is more favourable to the optimization objective.</p>

<p><strong>Why is decision boundary perpendicular to the \(\theta\)?</strong></p>

<p>Consider two points \(x_1\) and \(x_2\) on the decision boundary given by,</p>

<script type="math/tex; mode=display">\theta\,x + c= 0 \tag{16} \label{16}</script>

<p>Since the two points are on the line, they must satisfy \eqref{16}. Substitution leads to the following,</p>

<script type="math/tex; mode=display">\theta\,x_1 + c= 0 \tag{17} \label{17}</script>

<script type="math/tex; mode=display">\theta\,x_2 + c= 0 \tag{18} \label{18}</script>

<p>Subtracting \eqref{18} from \eqref{17},</p>

<script type="math/tex; mode=display">\theta\,(x_1 - x_2) = 0 \tag{17} \label{19}</script>

<p>Since \(x_1\) and \(x_2\) lie on the line, the vector \((x_1 - x_2)\) is on the line too. Following the property of orthogonal vectors, \eqref{19} is possible only if \(\theta\) is orthogonal or perpendicular to \((x_1 - x_2)\), and hence perpendicular to the decision boundary.</p>

<h3 id="kernels">Kernels</h3>

<p>When dealing with non-linear decision boundaries, a learning method like logistic regression relies on high order polynomial features to find a complex decision boundary and fit the dataset, i.e. predict \(y=1\) if,</p>

<script type="math/tex; mode=display">\theta_0\,f_0 + \theta_1\,f_1 + \theta_2\,f_2 + \theta_3\,f_3 + \cdots \geq 0 \tag{20} \label{20}</script>

<p>where \(f_0 = x_0,\, f_1=x_1,\, f_2=x_2,\, f_3=x_1x_2,\, f_4=x_1^2,\, \cdots \).</p>

<p>A natural question that arises is if there are choices of better/different features than in \eqref{20}? A SVM does this by picking points in the space called <strong>landmarks</strong> and defining functions called <strong>similarity</strong> corresponding to the landmarks.</p>

<p><img src="/assets/2018-04-10-support-vector-machines/fig-9-svm-landmarks.png?raw=true" alt="Fig-9. SVM Landmarks" width="50%" /></p>

<p>Say, there are three landmarks defined, \(l^{(1)}\), \(l^{(2)}\) and \(l^{(3)}\) as shown in the plot above, the for any given x, \(f_1\), \(f_2\) and \(f_3\) are defined as follows,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
f_1 &= similarity(x, l^{(1)}) = exp \left(- \frac {\lVert x - l^{(1)} \rVert^2} {2 \sigma^2} \right) \\
f_2 &= similarity(x, l^{(2)}) = exp \left(- \frac {\lVert x - l^{(2)} \rVert^2} {2 \sigma^2} \right) \\
f_3 &= similarity(x, l^{(3)}) = exp \left(- \frac {\lVert x - l^{(3)} \rVert^2} {2 \sigma^2} \right) \\
    & \vdots
\end{align}
\tag{21} \label{21} %]]></script>

<p>Here, the similarity function is mathematically termed a <strong>kernel</strong>. The specific kernel used in \eqref{21} is called the \(Gaussian Kernel\). Kernels are sometimes also denoted as \(k(x, l^{(i)})\).</p>

<p>Consider \(f_1\) from \eqref{21}. If there exists \(x\) close to landmark \(l^{(1)}\), then \(\lVert x - l^{(1)} \rVert \approx 0\) and hence, \(f_1 \approx 1\). Similarly for a \(x\) far from the landmark, \(\lVert x - l^{(1)} \rVert\) will be a larger value and hence exponential fall will cause \(f_1 \approx 0\). So effectively the choice of landmarks has helped in increasing the number of features \(x\) had from 2 to 3. which can be helpful in discrimination.</p>

<p>For a gaussian kernel, the value of \(\sigma\) defines the spread of the normal distribution. If \(\sigma\) is small, the spread will be narrower and when its large the spread will be wider.</p>

<p>Also, the intuition is clear about how landmarks help in generating the new features. Along with the values of parameter, \(\theta\) and \(\sigma\), various different decision boundaries can be achieved.</p>

<h3 id="how-to-choose-optimal-landmarks">How to choose optimal landmarks?</h3>

<p>In a complex machine learning problem it would be advantageous to choose a lot more landmarks. This is generally acheived by choosing landmarks at the point of the training examples, i.e. landmarks equal to the number of training examples are chosen, ending up in \(l^{(1)}, l^{(2)}, \cdots l^{(m)}\) if there are \(m\) training examples. This translates to the fact that each feature is a measure of how close is an instance to the existing points of the class, leading to generation of new feature vectors.</p>

<blockquote>
  <p>For SVM training, given training examples, \(x\), features \(f\) are computed, and \(y=1\), if \(\theta^Tf \geq 0\)</p>
</blockquote>

<p>The training objective from \eqref{5} is modified as follows,</p>

<script type="math/tex; mode=display">min_\theta \, C \sum_{i=1}^m \left[ y^{(i)}\,cost_1(\theta^T f^{(i)}) + (1-y^{(i)})\,cost_0(\theta^T f^{(i)}) \right] + {1 \over 2 } \sum_{j=1}^m \theta_j^2 \tag{22} \label{22}</script>

<p>In this case, \(n=m\) in \eqref{5} by the virtue of procedure used to choose \(f\).</p>

<blockquote>
  <p>The regularization term in \eqref{22} can be written as \(\theta^T\theta\). But in practice most SVM libraries, instead \(\theta^TM\theta\), which can be considered a scaled version is used as it gives certain optimization benefits and scaling to bigger training sets, which will be taken up at a later point in maybe another post.</p>
</blockquote>

<p>While the kernels idea can be applied to other algorithms like logistic regression, the computational tricks that apply to SVMs do not generalize as well to other algorithms.</p>

<blockquote>
  <p>Hence, SVMs and Kernels tend to go particularly well together.</p>
</blockquote>

<h3 id="biasvariance">Bias/Variance</h3>

<p>Since \(C (= {1 \over \lambda})\),</p>

<ul>
  <li>Large C: Low bias, High Variance</li>
  <li>Small C: High bias, Low Variance</li>
</ul>

<p>Regarding \(\sigma\),</p>

<ul>
  <li>Large \(\sigma^2\): High Bias, Low Variance (Features vary more smoothly)</li>
  <li>Small \(\sigma^2\): Low Bias, High Variance (Features vary less smoothly)</li>
</ul>

<h3 id="choice-of-kernels">Choice of Kernels</h3>

<ul>
  <li><strong>Linear Kernel:</strong> is equivalent to a no kernel setting giving a standard linear classifier given by,</li>
</ul>

<script type="math/tex; mode=display">\theta_0\,x_0 + \theta_1\,x_1 + \theta_2\,x_2 + \theta_3\,x_3 + \cdots \geq 0 \tag{23} \label{23}</script>

<p>Linear kernels are used when the number of training data is less but the number of features in the training data is huge.</p>

<ul>
  <li><strong>Gaussian Kernel:</strong> Make a choice of \(\sigma^2\) to adjust the bias/variance trade-off.</li>
</ul>

<p>Gaussian kernels are generally used when the number of training data is huge and the number of features are small.</p>

<blockquote>
  <p>Feature scaling is important when using SVM, especially Gaussian Kernels, because if the ranges vary a lot then the similarity feature would be dominated by features with higher range of values.</p>
</blockquote>

<blockquote>
  <p>All the kernels used for SVM, must satisfy Mercer’s Theorem, to make sure that SVM optimizations do not diverge.</p>
</blockquote>

<p>Some other kernels known to be used with SVMs are:</p>

<ul>
  <li>Polynomial kernels, \(k(x, l) = (x^T l + constant)^degree\)</li>
  <li>Esoteric kernels, like string kernel, chi-square kernel, histogram intersection kernel, ..</li>
</ul>

<h3 id="multi-class-classification">Multi-Class Classification</h3>

<ul>
  <li>Most SVM libraries have multi-class classification.</li>
  <li>Alternatively, one may use one-vs-all technique to train \(k\) different SVMs and pick class with largest \(\theta^Tx\)</li>
</ul>

<h3 id="logistic-regression-vs-svm">Logistic Regression vs SVM</h3>

<ul>
  <li>If \(n\) is large relative to \(m\), use logistic regression or SVM with linear kernel, like if \(n=10000, m=10-1000\)</li>
  <li>If \(n\) is small and \(m\) is intermediate, use SVM with gaussian kernel, like if \(n=1-1000, m=10-10000\)</li>
  <li>If \(n\) is small and \(m\) is large, create/add more features, then use logistic regression or SVM with no kernel, as with huge datasets SVMs struggle with gaussian kernels, like if \(n=1-1000, m=50000+\)</li>
</ul>

<blockquote>
  <p>Logistic Regression and SVM without a kernel (with linear kernel) generally give very similar. A neural network would work well on these training data too, but would be slower to train.</p>
</blockquote>

<p>Also, the optimization problem of SVM is a convex problem, so the issue of getting stuck in local minima is non-existent for SVMs.</p>

<h2 id="references">REFERENCES:</h2>

<p><small><a href="https://www.coursera.org/learn/machine-learning/lecture/sHfVT/optimization-objective" target="_blank">Machine Learning: Coursera - Optimization Objective</a></small><br />
<small><a href="https://www.coursera.org/learn/machine-learning/lecture/wrjaS/large-margin-intuition" target="_blank">Machine Learning: Coursera - Large Margin Intuition</a></small><br />
<small><a href="https://www.coursera.org/learn/machine-learning/lecture/3eNnh/mathematics-behind-large-margin-classification" target="_blank">Machine Learning: Coursera - Mathematics of Large Margin Classification</a></small><br />
<small><a href="https://www.coursera.org/learn/machine-learning/lecture/YOMHn/kernels-i" target="_blank">Machine Learning: Coursera - Kernel I</a></small><br />
<small><a href="https://www.coursera.org/learn/machine-learning/lecture/hxdcH/kernels-ii" target="_blank">Machine Learning: Coursera - Kernel II</a></small><br />
<small><a href="https://www.coursera.org/learn/machine-learning/lecture/sKQoJ/using-an-svm" target="_blank">Machine Learning: Coursera - Using An SVM</a></small><br />
<small><a href="https://www.quora.com/Support-Vector-Machines-Why-is-theta-perpendicular-to-the-decision-boundary" target="_blank">Quora - Why is theta perpendicular to the decision boundary?</a></small><br />
<small><a href="https://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html" target="_blank">Introduction to support vector machines</a></small></p>

        </section>
        <footer class="post-footer">
          <div class="wrap">
            
              <div class="tile">
                <div class="text">
                <a href="/tag/machine-learning" class='category-links'><h1>machine-learning</h1></a>
                </div>
              </div>
            
              <div class="tile">
                <div class="text">
                <a href="/tag/andrew-ng" class='category-links'><h1>andrew-ng</h1></a>
                </div>
              </div>
            
          </div>
        </footer>
        <div class="horizontal-divider">· · ·</div>
        <!-- <div class="bottom-teaser cf">
          <div class="isLeft">
            <h5 class="index-headline featured"><span>Written by</span></h5>
            <section class="author">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4>Shams S</h4>
              <p class="bio">Research Assistant @ Purdue University</p>
              <hr>
              <p class="published">Published on <time datetime="2018-04-10 00:00">10 Apr 2018</time></p>
            </section>
          </div>
          <div class="isRight">
            <h5 class="index-headline featured"><span>Supported by</span></h5>
            <footer class="site-footer">
              <section class="poweredby">Proudly published with <a href="http://jekyllrb.com"> Jekyll</a></section>
              <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> You should subscribe to my feed.</span></a>
              <div class="inner">
                <section class="copyright">All content copyright <a href="https://machinelearningmedium.com/">Machine Learning Medium</a> &copy; 2019<br>All rights reserved.</section>
              </div>
            </footer>
          </div>
        </div> -->
        <br>
        
        <div id="vc-feelback-main" data-access-token="334e9d2a5dc341469cda15d8c4bd935e" data-display-type="4"></div>
        
        <nav class="page-navigation" role="navigation">
  
    <a href=/2018/04/08/error-metrics-for-skewed-data-and-large-datasets/ class="arrow-button left-arrow-button"><span class="left-arrow"></span><span class="label left-label">Error Metrics for Skewed Classes and Using Large Datasets</span></a>
  
  
    <a href=/2018/04/19/k-means-clustering/ class="arrow-button right-arrow-button"><span class="label right-label">K-Means Clustering</span><span class="right-arrow"></span></a>
  
</nav>
        
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE */
            var disqus_shortname = 'shams-sam'; /* required: replace example with your forum shortname */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        
      </article>
    </main>
    <!-- footer start -->
<div class="footer-container">

<footer class="site-footer">
  <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> Subscribe!</span></a>
    <div class="inner">
         <section class="copyright">All content copyright <a href="/">Machine Learning Medium</a> &copy; 2019 &bull; All rights reserved.</section>
         <section class="poweredby">Made with <a href="http://jekyllrb.com"> Jekyll</a></section>
         <section class="poweredby">Inspired by <a href="https://medium.com/"> Medium</a> and <a href="https://github.com/dirkfabisch/mediator">Mediator</a></section>
    </div>
</footer>

<div class="bottom-closer">
  <div class="background-closer-image"  style="background-image: url(/assets/images/footer.jpg)">
    Image
  </div>
  <div class="inner">
    <h1 class="footer-title">Machine Learning Medium</h1>
    <h2 class="footer-description">Recursing the Rabbit Hole</h2>
    <a href=/about/ class="btn">About Me</a>
  </div>
</div>

<!-- footer end -->
</div>

    <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script src="/public/js/typed.js"></script>
<script>
  $(function(){
    $(".blog-description").typed({
      strings: ['Recursing the Rabbit Hole'],
      typeSpeed: 50,
      backSpeed: 25,
      loop: true
    });
  });
</script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      },
      CommonHTML: {matchFontHeight: false},
      "HTML-CSS": {matchFontHeight: false},
      SVG: {matchFontHeight: false}
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>



<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-108148924-1', 'auto');
ga('send', 'pageview');

</script>


<div id="fb-root"></div>

<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = 'https://connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v2.10&appId=391866664565981';
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>


<script> 
(function() { 
  var v = document.createElement('script'); v.async = true; 
  v.src = "https://assets-prod.vicomi.com/vicomi.js?token=334e9d2a5dc341469cda15d8c4bd935e"; 
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(v, s); 
})(); 
</script>

  </body>
</html>
