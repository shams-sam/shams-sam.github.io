<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  

  <title>Backpropagation Implementation and Gradient Checking | Machine Learning Medium</title>
  <meta name="description" content="Approximate implementation of backpropagation using NumPy, and Gradient Checking for the Derivatives." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
  
  <meta property="og:site_name" content="Machine Learning Medium" />
  <meta property="og:title" content="Backpropagation Implementation and Gradient Checking"/>
  
  <meta property="og:description" content="Approximate implementation of backpropagation using NumPy, and Gradient Checking for the Derivatives." />
  
  <meta property="og:image" content="https://machinelearningmedium.com/assets/images/gradient-descent.png" />
  <meta property="og:url" content="https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/" >
  <meta property="og:type" content="blog" />
  <meta property="article:published_time" content="2018-03-29T00:00:00+05:30">

  <link rel="canonical" href="https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/"/>
  <link rel="shortcut icon" href="/public/fav.png" type="image/png"/>
  <link rel="stylesheet" href="//brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
  <link rel="stylesheet" type="text/css" href="/css/custom.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/share_bar.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/syntax.css" />
  <link href="https://fonts.googleapis.com/css?family=Fira+Sans:400,600|Open+Sans:400,700" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" >
  <link rel="shortcut icon" href="/public/fav.ico?v1">


  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

</head>

  <body itemscope itemtype="http://schema.org/Article">
    <div class="metabar metabar-header">
  <div class="metabar-inner">
    <div class="quote-div">
      <a class="icon-quote" href="/">
        <i class="fa fa-quote-left fa-pull-left fa-border" aria-hidden="true"></i>
      </a>
    </div>
  </div>
</div>

<div class="sectionbar sectionbar-header">
  <div class="sectionbar-inner">
    <div class="table">
      <div class="category-links table-cell">
        
          <a href=/>Home</a>
        
        
        <a href="/tag/machine-learning">Machine Learning</a>
        
        <a href="/tag/mathematics">Mathematics</a>
        
        <a href="/tag/papers">Papers</a>
        
        <span class="right-padding-22">|</span>
        <a href=/collections/ class="">Collections</a>
        <a href=/tags/ class="">Tags</a>
      </div>
      <div class="social-links table-cell">
        <!-- 
          
              <a class="icon-github-alt" href="https://github.com/shams-sam/shams-sam.github.io"  title="Gihub Project" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-github-alt fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-facebook" href="https://facebook.com/shams-sam"  title="Facebook" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-facebook fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-twitter" href="https://twitter.com/sshamssam"  title="Twitter" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-twitter fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-linkedin" href="https://linkedin.com/in/shams-sheikh-20328154"  title="LinkedIn" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-envelope" href="mailto:shams.sam@live.com"  title="E-mail" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-envelope fa-stack-1x"></i>
                </span>
              </a>
          
         -->
        <div class="fb-like" data-href="https://www.facebook.com/machinelearningmedium/" data-layout="button" data-action="like" data-size="large" data-show-faces="false" data-share="false"></div>
        <a href="https://twitter.com/sshamssam" class="twitter-follow-button" data-size="large" data-show-screen-name="false" data-dnt="true" data-show-count="false"></a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
      </div>
    </div>
  </div>
</div>
    <div class="author-strip">
      <div class="author-strip-inner">
        <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
        <div class="author-detail">
          <h4 class="author-name" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/about/" style="color: #000000; text-decoration: none;">Shams S</a></h4>
          <h5 class="author-name"> Data Scientist @ Practo </h5>
          <time datetime="2018-03-29T00:00:00+05:30">Mar 29</time>
          <span class="middot">&#183;</span>
          <div class="post-reading">
            <span class="post-reading-time"></span> read
          </div>
        </div>
      </div>
    </div>
    
    <div class="postcover">
    <div class="postimage">
        <div class="postimage-image"  style="background-image: url(/assets/images/gradient-descent.png) ">
        </div>
    </div>
    
    <figcaption>Image Source: https://upload.wikimedia.org/wikipedia/commons/3/31/Gradient99.png</figcaption>
    
    </div>
    <main class="content post-content" role="main">
      <article class="post">
        <div class="noarticleimage">
          <div class="post-meta">
            <h1 class="post-title">Backpropagation Implementation and Gradient Checking</h1>
            <h2 class="post-description">Approximate implementation of backpropagation using NumPy, and Gradient Checking for the Derivatives.</h2>
          </div>
        </div>
        <div class="horizontal-divider">&#183; &#183; &#183;</div>
        <section class="post-content">
          <a name="topofpage"></a>
          <h3 id="basics-of-machine-learning-series">Basics of Machine Learning Series</h3>

<blockquote>
  <p><a href="/collection/basics-of-machine-learning">Index</a></p>
</blockquote>

<div class="horizontal-divider">· · ·</div>

<h3 id="backpropagation-numpy-example">Backpropagation NumPy Example</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c"># define XOR training data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="p">])</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>

<span class="k">print</span><span class="p">(</span><span class="s">'X.shape:'</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y.shape:'</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c"># defining network parameters</span>
<span class="c"># [2, 2, 1] will also work for the XOR problem presented</span>
<span class="n">LAYERS</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">ETA</span> <span class="o">=</span> <span class="o">.</span><span class="mi">1</span>
<span class="n">THETA</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c"># sigmoid activation function</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c"># derivative of sigmoid activation function</span>
<span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c"># calculating cost given the prediction and actual vectors</span>
<span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">_</span> <span class="o">*</span> <span class="n">_</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)])</span>

<span class="c"># initialing THETA params for all the layers</span>
<span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">LAYERS</span><span class="p">)):</span>
        <span class="n">THETA</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">LAYERS</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">LAYERS</span><span class="p">[</span><span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

<span class="c"># vectorized forward propagation</span>
<span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">initialize</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">initialize</span><span class="p">:</span>
        <span class="n">initialize_parameters</span><span class="p">()</span>
    <span class="c"># adding bias column to the input X</span>
    <span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="n">X</span><span class="p">))]</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">activate</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">theta</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">THETA</span><span class="p">):</span>
        <span class="n">Z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">theta</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
        <span class="c"># adding bias column to the output of previous layer</span>
        <span class="n">A</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">Z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))))</span>
    <span class="c"># bias is not needed in the final output</span>
    <span class="n">A</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">y_hat</span>

<span class="c"># vectorized backpropagation</span>
<span class="k">def</span> <span class="nf">back_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">initialize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c"># run a forward pass</span>
    <span class="n">A</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">initialize</span><span class="p">)</span>
    <span class="c"># calculate delta at final output</span>
    <span class="n">del_</span> <span class="o">=</span> <span class="p">[(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">cost</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    <span class="c"># flag to signify whether a layer has bias column of not</span>
    <span class="n">bias_free</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="c"># running in reverse because delta is propagated backwards</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">THETA</span><span class="p">))):</span>
        <span class="k">if</span> <span class="n">bias_free</span><span class="p">:</span>
            <span class="c"># true only for the final layer where there is no bias</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">del_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
            <span class="n">bias_free</span><span class="o">=</span><span class="bp">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c"># true for all the layers except the input and output layer</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">del_</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">:],</span> <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
        <span class="n">del_</span> <span class="o">=</span> <span class="p">[</span><span class="n">temp</span><span class="p">]</span> <span class="o">+</span> <span class="n">del_</span>
    <span class="n">del_theta</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">bias_free</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="c"># calculation for the delta in the parameters</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">del_</span><span class="p">))):</span>
        <span class="k">if</span> <span class="n">bias_free</span><span class="p">:</span>
            <span class="c"># true only for the final layer where there is no bias</span>
            <span class="n">del_theta</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="n">ETA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">del_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="n">idx</span><span class="p">])]</span> <span class="o">+</span> <span class="n">del_theta</span>
            <span class="n">bias_free</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c"># true for all the layers except the input and output layer</span>
            <span class="n">del_theta</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="n">ETA</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">del_</span><span class="p">[</span><span class="n">idx</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="n">idx</span><span class="p">])]</span> <span class="o">+</span> <span class="n">del_theta</span>
    <span class="c"># update parameters</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">THETA</span><span class="p">)):</span>
        <span class="c"># asserting that the matrix sizes are same</span>
        <span class="k">assert</span> <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">del_theta</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">del_theta</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">debug</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">del_</span><span class="p">,</span> <span class="n">del_theta</span><span class="p">)</span>

<span class="c"># training epochs</span>
<span class="n">initialize</span><span class="o">=</span><span class="bp">True</span>
<span class="n">verbose</span><span class="o">=</span><span class="bp">True</span>
<span class="n">THETA</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span>
    <span class="n">back_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">initialize</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span>
    <span class="n">initialize</span><span class="o">=</span><span class="bp">False</span>

<span class="c"># inference after training</span>
<span class="n">A</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">initialize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c"># final output of the network</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Sometimes, it can been seen that the network get stuck over a few epochs and then continues to converge quickly. It might be due to the fact that the implementation of neural network above is not the most optimum because of using the mean squared error cost function which is not the recommended for classification purposes because of the issues explained in <a href="/2017/08/31/classification-and-representation/">Classification and Logistic Regression</a>.</strong></p>

<h3 id="gradient-checking">Gradient Checking</h3>

<p>Often times, it is normal for small bugs to creep in the backpropagtion code. There is a very simple way of checking if the written code is bug free. It is based on calculating the slope of cost function manually by taking marginal steps ahead and behind the point at which the gradient is returned by backpropagation.</p>

<p><img src="/assets/2018-03-29-backpropagation-implementation-and-gradient-checking/fig-1-gradient-checking.png?raw=true" alt="Fig-1. Gradient Checking" width="75%" /></p>

<p>As visible in the plot above, the gradient approximation can be calculated using the <strong>centred difference formula</strong> as follows,</p>

<script type="math/tex; mode=display">{d \over d \Theta} J(\Theta) \approx \frac{J(\Theta + \epsilon) - J(\Theta - \epsilon)} {2 \epsilon} \tag{1} \label{1}</script>

<p>The approximation in \eqref{1}, works better than the <strong>single sided difference</strong>, i.e.,</p>

<script type="math/tex; mode=display">{d \over d \Theta} J(\Theta) \approx \frac{J(\Theta + \epsilon) - J(\Theta)} {\epsilon} \tag{2} \label{2}</script>

<p>This is basically because if the error is approximated for the terms using <strong>Taylor expansion</strong>, it is observed that \eqref{1} has error of order \(O(h)\) while \eqref{2} has error of order \(O(h^2)\), i.e. <strong>second order approximation</strong>.</p>

<p>Using the NumPy example of backpropagation, gradient checking can be confirmed as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># set epsilon and the index of THETA to check</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c"># reset THETA and run backpropagation</span>
<span class="n">THETA</span><span class="o">=</span><span class="p">[]</span>
<span class="n">A</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">del_</span><span class="p">,</span> <span class="n">del_theta</span> <span class="o">=</span> <span class="n">back_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c"># calculate the approximate gradients using centered difference formula</span>
<span class="n">grad_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">epsilon</span>
        <span class="n">A</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">initialize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">J_plus_epsilon</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span> <span class="n">epsilon</span>
        <span class="n">A</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">initialize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">J_minus_epsilon</span> <span class="o">=</span> <span class="n">cost</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">grad_check</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">J_plus_epsilon</span> <span class="o">-</span> <span class="n">J_minus_epsilon</span><span class="p">)</span><span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">epsilon</span><span class="p">)</span>
        <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">THETA</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">epsilon</span>

<span class="k">print</span><span class="p">(</span><span class="s">"from backprop:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">del_theta</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">ETA</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"from grad check:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">grad_check</span><span class="p">)</span>
</code></pre></div></div>

<p>Gives output:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from backprop:
[[ 0.00351937  0.00136117  0.00141332]
 [ 0.00254422  0.00088405  0.00082788]]
from grad check:
[[ 0.00353115  0.00136459  0.00141917]
 [ 0.00255167  0.00088687  0.00082796]]
</code></pre></div></div>

<p><strong>It can be seen that the two values are very similar and hence it proves that the back propagation is working well.</strong></p>

<p><strong>Note:</strong> Gradient from backpropagation is adjusted linearly by division with 2. Because of the approximate implementation of backpropagation there is a linear scaling in the derivatives given by the backpropagation function. This still works fine for the overall network because the application of <strong>learning rate nullifies the minor scaling error</strong>. <strong>Let me know if anyone can point out the mistake.</strong></p>

<h2 id="references">REFERENCES:</h2>

<p><small><a href="https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking" target="_blank">Machine Learning: Coursera - Gradient Checking</a></small>&lt;/br&gt;
<small><a href="http://cs231n.github.io/neural-networks-3/#gradcheck" target="_blank">CS231n Gradient checks</a></small>&lt;/br&gt;</p>

        </section>
        <div class="horizontal-divider">· · ·</div>
        <footer class="post-footer">
          <div class="wrap">
            
              <div class="tile">
                <div class="text">
                <a href="/tag/machine-learning" class='category-links'><h1>machine-learning</h1></a>
                </div>
              </div>
            
              <div class="tile">
                <div class="text">
                <a href="/tag/andrew-ng" class='category-links'><h1>andrew-ng</h1></a>
                </div>
              </div>
            
          </div>
          <section class="share">
            <span id="share-bar">

    <span><i class="fa fa-share-alt"></i></span>

    <span class="share-buttons">
        <span>
        <a  href="https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Facebook" >
            <i class="fa fa-facebook-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://twitter.com/intent/tweet?text=Backpropagation Implementation and Gradient Checking&url=https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Twitter" >
            <i class="fa fa-twitter-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://plus.google.com/share?url=https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Google+" >
            <i class="fa fa-google-plus-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://pinterest.com/pin/create/button/?url=https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Pinterest" >
            <i class="fa fa-pinterest-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.tumblr.com/share/link?url=https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Tumblr" >
            <i class="fa fa-tumblr-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.reddit.com/submit?url=https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Reddit" >
            <i class="fa fa-reddit-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://www.linkedin.com/shareArticle?mini=true&url=https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/&title=Backpropagation Implementation and Gradient Checking&summary=Approximate implementation of backpropagation using NumPy, and Gradient Checking for the Derivatives.&source=Machine Learning Medium"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on LinkedIn" >
            <i class="fa fa-linkedin-square"></i>
        </a>
        </span>
        <span>
        <a  href="mailto:?subject=Backpropagation Implementation and Gradient Checking&amp;body=Check out this site https://machinelearningmedium.com/2018/03/29/backpropagation-implementation-and-gradient-checking/"
            title="Share via Email" >
            <i class="fa fa-envelope-square"></i>
        </a>
        </span>
    </span>

</span>
          </section>
        </footer>
        <!-- <div class="bottom-teaser cf">
          <div class="isLeft">
            <h5 class="index-headline featured"><span>Written by</span></h5>
            <section class="author">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4>Shams S</h4>
              <p class="bio">Data Scientist @ Practo</p>
              <hr>
              <p class="published">Published on <time datetime="2018-03-29 00:00">29 Mar 2018</time></p>
            </section>
          </div>
          <div class="isRight">
            <h5 class="index-headline featured"><span>Supported by</span></h5>
            <footer class="site-footer">
              <section class="poweredby">Proudly published with <a href="http://jekyllrb.com"> Jekyll</a></section>
              <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> You should subscribe to my feed.</span></a>
              <div class="inner">
                <section class="copyright">All content copyright <a href="https://machinelearningmedium.com/">Machine Learning Medium</a> &copy; 2018<br>All rights reserved.</section>
              </div>
            </footer>
          </div>
        </div> -->
        
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE */
            var disqus_shortname = 'shams-sam'; /* required: replace example with your forum shortname */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        
      </article>
    </main>
    <!-- footer start -->
<div class="footer-container">

<footer class="site-footer">
  <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> Subscribe!</span></a>
    <div class="inner">
         <section class="copyright">All content copyright <a href="/">Machine Learning Medium</a> &copy; 2018 &bull; All rights reserved.</section>
         <section class="poweredby">Made with <a href="http://jekyllrb.com"> Jekyll</a></section>
         <section class="poweredby">Inspired by <a href="https://medium.com/"> Medium</a> and <a href="https://github.com/dirkfabisch/mediator">Mediator</a></section>
    </div>
</footer>

<div class="bottom-closer">
  <div class="background-closer-image"  style="background-image: url(/assets/images/footer.jpg)">
    Image
  </div>
  <div class="inner">
    <h1 class="footer-title">Machine Learning Medium</h1>
    <h2 class="footer-description">A step away from the illusion of knowledge.</h2>
    <a href=/about/ class="btn">About Me</a>
  </div>
</div>

<!-- footer end -->
</div>

    <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script src="/public/js/typed.js"></script>
<script>
  $(function(){
    $(".blog-description").typed({
      strings: ['A step away from the illusion of knowledge.'],
      typeSpeed: 50,
      backSpeed: 25,
      loop: true
    });
  });
</script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      },
      CommonHTML: {matchFontHeight: false},
      "HTML-CSS": {matchFontHeight: false},
      SVG: {matchFontHeight: false}
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>



<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-108148924-1', 'auto');
ga('send', 'pageview');

</script>


<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = 'https://connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v2.10&appId=391866664565981';
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>
  </body>
</html>
