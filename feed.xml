<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning Medium</title>
    <description>Recursing the Rabbit Hole</description>
    <link>https://machinelearningmedium.com/</link>
    <atom:link href="https://machinelearningmedium.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 24 Oct 2022 04:24:10 +0000</pubDate>
    <lastBuildDate>Mon, 24 Oct 2022 04:24:10 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Deep Learning without Poor Local Minima</title>
        <description>&lt;h3 id=&quot;importance&quot;&gt;Importance&lt;/h3&gt;

&lt;p&gt;It has long been an open question to understand why is training a deep neural network tractable. Specifically, we know that deep neural networks are non-convex functions with possible local minima. We also know that stochastic gradient descent is prone to getting stuck in such local minima. What has confounded researchers for a long time is the fact that &lt;strong&gt;while searching such a huge solution space, why does a neural network not get stuck in local minima” more often&lt;/strong&gt; and learn bad solutions. This work tries to give some understanding about these open questions.&lt;/p&gt;

&lt;h3 id=&quot;prior-works&quot;&gt;Prior works&lt;/h3&gt;

&lt;p&gt;There has an earlier paper in the field published back in 1989, that proves similar statements but under much tighter assumptions. This work builds on top of that using different techniques although and relaxing the assumptions and using only 2 or the original 7 used in the prior work.&lt;/p&gt;

&lt;h3 id=&quot;interesting-results&quot;&gt;Interesting Results&lt;/h3&gt;

&lt;p&gt;Part of the main result says “Every critical point that is not a global minimum is a saddle point”, which is interesting because it can be loosely translated to “there are no local maxima” which can be quite unintuitive to imagine at first.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There are no local maxima.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;

&lt;p&gt;A brief understanding of the following topics would be good to have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear Algebra. duh!&lt;/li&gt;
  &lt;li&gt;First order and second order necessary conditions for local minima.&lt;/li&gt;
  &lt;li&gt;Familiarity with graph interpretation of neural networks.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contributions&quot;&gt;Contributions&lt;/h3&gt;

&lt;p&gt;This paper proves the following statements for squared loss function of deep linear networks with any depth and any widths:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The function is non-convex and non-concave.&lt;/li&gt;
  &lt;li&gt;Every local minimum is a global minimum.&lt;/li&gt;
  &lt;li&gt;Every critical point that is not a global minimum is a saddle point.&lt;/li&gt;
  &lt;li&gt;Bad saddle points exist for deeper networks s.t. Hessian has no negative eigenvalue&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Shallow networks don’t have this issue of bad saddle points. This work further proves the same 4 statements via reduction about deep non-linear neural networks&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2016/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html&quot; target=&quot;_blank&quot;&gt;Deep Learning without Poor Local Minima&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2020/11/10/deep-learning-without-poor-local-minima/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2020/11/10/deep-learning-without-poor-local-minima/</guid>
        
        <category>machine-learning</category>
        
        <category>mathematics</category>
        
        <category>papers</category>
        
        <category>theorems</category>
        
        
      </item>
    
      <item>
        <title>Generative Adversarial Networks</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;The basic &lt;strong&gt;adversarial&lt;/strong&gt; framework of the GAN architecture can be broken down into the following &lt;strong&gt;two players&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;generative&lt;/strong&gt; model &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;, that tries to capture the latent data distribution.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;discriminative&lt;/strong&gt; model &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;, that estimates the probability that a sample came from training data rather than &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The framework is adversarial in the sense that the training procedure for &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; tries to &lt;strong&gt;maximize the probability of &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; making a mistake&lt;/strong&gt;. The framework thus corresponds to a minimax two-player game.&lt;/p&gt;

&lt;h3 id=&quot;related-generative-models&quot;&gt;Related Generative Models&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Restricted Boltzmann Machines&lt;/li&gt;
  &lt;li&gt;Deep Boltzmann Machines&lt;/li&gt;
  &lt;li&gt;Deep Belief Networks&lt;/li&gt;
  &lt;li&gt;Denoising Autoencoders&lt;/li&gt;
  &lt;li&gt;Contractive Autoencoders&lt;/li&gt;
  &lt;li&gt;Generative Stochastic Network&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;notations&quot;&gt;Notations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Easiest to implement GANs when the models are &lt;strong&gt;multilayer perceptrons&lt;/strong&gt; for both generator and discriminator.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p_g&lt;/script&gt; is the generator’s distribution over data &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;p_z(z)&lt;/script&gt; is an input noise function and &lt;script type=&quot;math/tex&quot;&gt;G(z; \theta_g)&lt;/script&gt; is the mapping to data space.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; is differentiable function represented by a paramter &lt;script type=&quot;math/tex&quot;&gt;\theta_g&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; is another differentiable function that outputs a scalar.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;D(x)&lt;/script&gt; represents the probability of assigning the correct label to both training examples and samples from &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; is simultaneously trained to minimize &lt;script type=&quot;math/tex&quot;&gt;log(1-D(G(z)))&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimization-objective&quot;&gt;Optimization Objective&lt;/h3&gt;

&lt;p&gt;The training framework between &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; can be represented by a two player minimax game in value function &lt;script type=&quot;math/tex&quot;&gt;V(G,D)&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_G \max_D V(D, G) = \mathbb{E}_{x\sim p_{data}(x)} [log D(x)] + \mathbb{E}_{z\sim p_z(z)} [log(1 - D(G(z)))]&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-29-generative-adversarial-networks/fig-2-Generative-Adversarial-Network-GAN.png?raw=true&quot; alt=&quot;GAN Architecture&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; are trained iteratively one after the other&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; is not optimized to completion as it would lead to overfitting&lt;/li&gt;
  &lt;li&gt;Alternate between &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; steps of optimizing &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; and one step of &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Results in &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; near its optimal, so long as &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; changes slowly.&lt;/li&gt;
  &lt;li&gt;Early in learning when &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; is poor &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; can reject samples with high confidence which causes &lt;script type=&quot;math/tex&quot;&gt;log(1-D(G(z)))&lt;/script&gt; to saturate&lt;/li&gt;
  &lt;li&gt;Instead of minimizing &lt;script type=&quot;math/tex&quot;&gt;log(1-D(G(z)))&lt;/script&gt;, maximize &lt;script type=&quot;math/tex&quot;&gt;log(D(G(z)))&lt;/script&gt; for stronger gradients early in the learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-29-generative-adversarial-networks/fig-1-gan-algorithm.png?raw=true&quot; alt=&quot;Algorithm for GAN training&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;theoretical-results&quot;&gt;Theoretical Results&lt;/h3&gt;

&lt;p&gt;For a fixed &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt;, the optimal discriminator can be found by differentiating the objective function w.r.t. &lt;script type=&quot;math/tex&quot;&gt;D(x)&lt;/script&gt;. The objective function is of the form,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) = a\,log\,y + b\,log\,(1-y)&lt;/script&gt;

&lt;p&gt;Differentiating w.r.t $y$ gives,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df(y)}{dy} = \frac{a}{y} - \frac{b}{1-y}&lt;/script&gt;

&lt;p&gt;Since we are maximising this, the maximum can be found by estimating the point of 0 derivative, i.e,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    \frac{df(y)}{dy} &amp;= 0 \\
    \frac{a}{y} - \frac{b}{1-y} &amp;= 0 \\
    \frac{a}{y} &amp;= \frac{b}{1-y} \\
    y &amp;= \frac{a}{a+b}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;So the optimal discriminator for a fixed &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_G^*(x) = \frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}&lt;/script&gt;

&lt;p&gt;For this maximized &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;, the optimization objective can be rewritten as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C(G) = \mathbb{E}_{x\sim p_{data}} \left[log \frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}\right] + \mathbb{E}_{x\sim p_g} \left[log\frac{p_{g }(x)}{p_{data}(x)+p_{g}(x)}\right]&lt;/script&gt;

&lt;p&gt;We can show that this expression is minimized for &lt;script type=&quot;math/tex&quot;&gt;p_g=p_{data}&lt;/script&gt;. The value of &lt;script type=&quot;math/tex&quot;&gt;D_G^*(x)&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;1/2&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;p_g=p_{data}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;C(G) = -log\,4&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;To see that this is the minimu possible value, consider the following modification to the &lt;script type=&quot;math/tex&quot;&gt;C(G)&lt;/script&gt; expression above,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
C(G) &amp;= \mathbb{E}_{x\sim p_{data}} \left[log \frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}\right] + \mathbb{E}_{x\sim p_g} \left[log\frac{p_{g }(x)}{p_{data}(x)+p_{g}(x)}\right] + log\,2 \cdot 2 - log\,4 \\
&amp;= - log\,4 + \mathbb{E}_{x\sim p_{data}} \left[log \frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)}\right] + log\,2 +  \mathbb{E}_{x\sim p_g} \left[log\frac{p_{g }(x)}{p_{data}(x)+p_{g}(x)}\right] + log\,2 \\ 
&amp;=- log\,4 + \mathbb{E}_{x\sim p_{data}} \left[log \frac{p_{data}(x)}{p_{data}(x)+p_{g}(x)} + log\,2\right] +  \mathbb{E}_{x\sim p_g} \left[log\frac{p_{g }(x)}{p_{data}(x)+p_{g}(x)} + log\,2\right] \\
&amp;=- log\,4 + \mathbb{E}_{x\sim p_{data}} \left[log \frac{p_{data}(x)}{\frac{p_{data}(x)+p_{g}(x)}{2}}\right] +  \mathbb{E}_{x\sim p_g} \left[log\frac{p_{g }(x)}{\frac{p_{data}(x)+p_{g}(x)}{2}}\right] \\
&amp;= -log\,4 + KL\left(p_{data}||\frac{p_{data}+p_g}{2}\right) + KL\left(p_{g}||\frac{p_{data}+p_g}{2}\right)\\
&amp;= -log\,4 + 2\cdot JSD(p_{data}||p_g)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The last term is the Jensen-Shannon divergence between two distributions which is always non-negative and zero only when the two distributions are equal. So &lt;script type=&quot;math/tex&quot;&gt;C^* = -log\,4&lt;/script&gt; is the global minimum of &lt;script type=&quot;math/tex&quot;&gt;C(G)&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;p_g=p_{data}&lt;/script&gt;, i.e. generative model perfectly replicating the data distribution.&lt;/p&gt;

&lt;h3 id=&quot;complexity-comparison-of-generative-models&quot;&gt;Complexity Comparison of Generative Models&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-29-generative-adversarial-networks/fig-3-comparison-of-generative models.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;disadvantages&quot;&gt;Disadvantages&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;There is no explicit representation of &lt;script type=&quot;math/tex&quot;&gt;p_g(x)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; must be synchronized well with &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; during training. There are possibilities of &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; being too strong leading to zero gradient for &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; being too weak which causes &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; to collapse to many values of &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; to the same value of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; which would not have enough diversity to model &lt;script type=&quot;math/tex&quot;&gt;p_{data}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;follow-up-citations&quot;&gt;Follow-up Citations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;RBMs and DBMs
    &lt;ul&gt;
      &lt;li&gt;A fast learning algorithm for deep belief nets by Hinton et al.&lt;/li&gt;
      &lt;li&gt;Deep boltzman machines by Salakhutdinov et al.&lt;/li&gt;
      &lt;li&gt;Information processing in dynamical systems: Foundations of harmony theory by Smolensky&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MCMC
    &lt;ul&gt;
      &lt;li&gt;Better mixing via deep representations by Bengio et al.&lt;/li&gt;
      &lt;li&gt;Deep generative stochastic networks trainable by backprop by Bengio et al.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Encodings
    &lt;ul&gt;
      &lt;li&gt;What is the best multi-stage architecture for object recognition? by Jarett et al.&lt;/li&gt;
      &lt;li&gt;Generalized denoising auto-encoders as generative models by Bengio et al.&lt;/li&gt;
      &lt;li&gt;Deep sparse rectifier neural networks by Glorot et al.&lt;/li&gt;
      &lt;li&gt;Maxout networks by Goodfellow et al.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimizations
    &lt;ul&gt;
      &lt;li&gt;Auto-encoding variational bayes by Kingma et al.&lt;/li&gt;
      &lt;li&gt;Stochastic backpropagation and approximate inference in deep generative models by Rezende et al.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Learning deep architectures for AI by Bengio Y.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&quot; target=&quot;_blank&quot;&gt;Generative Adversarial Nets by Goodfellow et al.&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2019/10/29/generative-adversarial-networks/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2019/10/29/generative-adversarial-networks/</guid>
        
        <category>gan</category>
        
        <category>machine-learning</category>
        
        <category>papers</category>
        
        <category>privacy-gans</category>
        
        
      </item>
    
      <item>
        <title>Privacy Preserving Predictive Modeling GANs</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In the current era of increasing data sharing and ubiquity of machine learning there is a very little focus on privacy of the subjects whose data is being shared at such large scale. While most data released promises anonymization of the PIIs, there is much work in literature to point of that simple anonymization techniques fail to mask users if there is an access to auxilliary dataset in which such features are not masked.&lt;/p&gt;

&lt;p&gt;In order to fix this issue there has been study in the field of privacy preservation for the datasets in public domain to ensure public trust in sharing their information which eventually is the root to building amazing machine learning models and drawing insights from big data.&lt;/p&gt;

&lt;p&gt;This paper discusses a new architecture of GAN which tries to achieve this very objective of anonymization of the private data but in a deep learning setting where the work in overseen by an objective function for the encoder which embeds in itself the notion of anonymizing sensitive columns while trying to maximize the predivity of non-sensitive data columns.&lt;/p&gt;

&lt;h3 id=&quot;about-the-paper&quot;&gt;About the paper&lt;/h3&gt;

&lt;p&gt;The GAN framework presented consists of three components (explained in detail in later sections):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Encoder&lt;/li&gt;
  &lt;li&gt;Ally that predicts the desired variables,&lt;/li&gt;
  &lt;li&gt;Adversary that predicts the sensitive data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The objective of the GAN framework is two-fold:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;learn low-dimensional representation of data points (users in this case) that excels at  classifying a desired task (whether a user will answer quiz question correctly here)&lt;/li&gt;
  &lt;li&gt;prevent an adversary from recovering sensitive data (each users identity in this case)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;Netflix dataset one of the famous datasets which is used for various starter tutorials on recommendation system. The history of privacy breach in this particular case is related to this seemly harmless data. While the company had ensured data anonymization to prevent breach of privacy. It was later discovered that it is easy to locate the users with good accuracy using auxilliary data from other related datasets such as IMDB which does not anonymize its dataset. Similarly there are cases in the domain of insurance companies where it has been proven that reverse engineering is a viable effort despite the anonymization.&lt;/p&gt;

&lt;p&gt;It has similarly been shown that one can identify anonymous users in online social networks with upto 90% accuracy. These metrics point to the fact that it is possible for an attacker to uncover sensitive attributes from user data.&lt;/p&gt;

&lt;p&gt;One of the popular work that has caught traction in this field is called Differential Privacy (DP), which proposes to add random noise to raw data, where the noise (generally from Laplacian Distribution) level controls the trade off between predictive quality and user privacy. But it has been found that this mechanism also reduces the utility of the data for predictive modeling and increases the sample complexity&lt;/p&gt;

&lt;p&gt;The GAN model presented in this particular work is an effort to achieve the privacy (to prevent de-anonymization) while preserving the predictive aspects of the dataset (to overcome the drawbacks of techniques like DP).&lt;/p&gt;

&lt;h3 id=&quot;contributions&quot;&gt;Contributions&lt;/h3&gt;

&lt;p&gt;The authors apply this GAN architecture in a online MOOC setting. The objectives of the work include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use the student data to predict whether or not they will answer a quiz correctly.&lt;/li&gt;
  &lt;li&gt;Ensure that the encoded data does not achieve a good convergence on sensitive data such as user identity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-14-learning-informative-and-private-representations/fig-1-gan-architecture.png?raw=true&quot; alt=&quot;GAN Architecure&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The work is different from DP in two key aspects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is data-dependent, i.e., it learns representations from user data&lt;/li&gt;
  &lt;li&gt;Directly uses raw user data without relying on feature engineering&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The objective of the GAN is to generate representations that minimize the loss function of the ally while maximizing the loss function of adversary.&lt;/p&gt;

&lt;p&gt;One key advantage mentioned about the architecture is that it is model agnostic, i.e. each module can instantiate a specific differential function (e.g. neural networks) based on the needs of the particular application&lt;/p&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-10-14-learning-informative-and-private-representations/fig-2-algorithm.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;datasets-and-objectives&quot;&gt;Datasets and Objectives&lt;/h3&gt;

&lt;p&gt;This particular paper presents the empirical results on dataset from the course Networks: Friends, Money and Bytes on Coursera MOOC platform. This has a total of 92 in-video quiz questions among 20 lectures. Each lecture has 4-5 videos. A total of 314,632 clickstreams were recorded for 3976 unique students.&lt;/p&gt;

&lt;p&gt;Two types of data are collected about students:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Video-watching clickstream: behavior is recorded as a sequence of clickstreams based on actions available in the scrub bar.&lt;/li&gt;
  &lt;li&gt;Question submissions: answers submitted by a student to an in-video question&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The final &lt;strong&gt;objective&lt;/strong&gt; is defined as a mapping from student’s interaction (clickstream) on a video to their performance on questions (data acquired regarding question submissions)&lt;/p&gt;

&lt;p&gt;The data collected can have both time-varying as well as static attributes. Time varying attributes include the series of clickstream before a question is answered, while the static attribute will included metrics like fraction of course completed, amount of time spent etc.&lt;/p&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Accuracy on binary prediction of questions answered&lt;/li&gt;
  &lt;li&gt;AUC-ROC curve to assess the tradeoff between true and false positive rates&lt;/li&gt;
  &lt;li&gt;K Ranks and Mean average precision at K (MAP@K) to measure performance of privacy preservation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Only one baseline benchmark is included in the work which is Laplace Mechanism in DP (Differential Privacy) which simply adds Laplace noise to the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;findings-and-conclusions&quot;&gt;Findings and Conclusions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The new architecture outperforms DP in terms of prediction task on question answers. It actually performs slightly better than the original features themselves.&lt;/li&gt;
  &lt;li&gt;With parameter &lt;script type=&quot;math/tex&quot;&gt;\alpha \to 1&lt;/script&gt; in the GAN architecture, encoder is biased towards prediction than sensitive data obfuscation which is theoretically correct.&lt;/li&gt;
  &lt;li&gt;Larger &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; in DP means adding smaller noise component to the actual data, and it can be seen that models are better at predictive performance under such a setting.&lt;/li&gt;
  &lt;li&gt;Larger sizes of encoding dimension ensures more preserved information towards both prediction and sensitive data with identical &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; values. This confirms the fact that the size of representations controls the amount of information contained in data representation.&lt;/li&gt;
  &lt;li&gt;Raw clickstream data with LSTM performs better than the hand-crafted features in terms of the tradeoff between prediction quality vs user privacy.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;follow-up-citations&quot;&gt;Follow-up Citations&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;J. Bennett, S. Lanning et al., “The netflix prize,” in Proceedings of KDD
cup and workshop, vol. 2007. New York, NY, USA, 2007, p. 35.&lt;/li&gt;
  &lt;li&gt;A. Narayanan and V. Shmatikov, “Robust de-anonymization of large sparse datasets,” in Security and Privacy, 2008. SP 2008. IEEE Symposium on. IEEE, 2008, pp. 111–125.&lt;/li&gt;
  &lt;li&gt;“De-anonymizing social networks,” in Security and Privacy, 2009
30th IEEE Symposium on. IEEE, 2009, pp. 173–187.&lt;/li&gt;
  &lt;li&gt;Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise
to sensitivity in private data analysis,” in Proc. Theory of Cryptography
Conference, Mar. 2006, pp. 265–284.&lt;/li&gt;
  &lt;li&gt;“Calibrating noise to sensitivity in private data analysis,” in Theory of Cryptography Conference. Springer, 2006, pp. 265–284.&lt;/li&gt;
  &lt;li&gt;C. Huang, P. Kairouzyz, X. Chen, S. L., and R. Rajagopal, “Context-aware generative adversarial privacy,” arXiv preprint arXiv:1710.09549, Dec. 2017.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;references&quot;&gt;REFERENCES&lt;/h1&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=8622089&amp;amp;tag=1&quot; target=&quot;_blank&quot;&gt;Learning Informative and Private Representations via Generative Adversarial Networks&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Oct 2019 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2019/10/14/learning-informative-and-private-representations/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2019/10/14/learning-informative-and-private-representations/</guid>
        
        <category>GAN</category>
        
        <category>machine-learning</category>
        
        <category>papers</category>
        
        <category>privacy-gans</category>
        
        
      </item>
    
      <item>
        <title>Social Learning Networks</title>
        <description>&lt;h3 id=&quot;active-recall&quot;&gt;Active Recall&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;What is a SLN?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#characteristics-of-sln&quot;&gt;What do the nodes, connections and weights represent in a SLN graph?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#types-of-sln-graphs&quot;&gt;What are the types of graphs used to represent information in SLN?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;An SLN is a type of &lt;strong&gt;social network between student, instructors and modules of learning&lt;/strong&gt;. The network consists of dynamics of learning behaviour over a variety of &lt;strong&gt;graphs that represent the relationships&lt;/strong&gt; among people and processes involved in learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;characteristics-of-sln&quot;&gt;Characteristics of SLN&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;People in network &lt;strong&gt;learn through interaction&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Consists of functionality models and &lt;strong&gt;graph-theoretic models&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Nodes&lt;/strong&gt; represent learners, learning concepts, or both learners and concepts.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Links&lt;/strong&gt; represent the connection between the nodes.&lt;/li&gt;
  &lt;li&gt;Links can be &lt;strong&gt;undirected to denote similarity&lt;/strong&gt; or &lt;strong&gt;directed to denote flow of information&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Link weights&lt;/strong&gt; gives magnitude to the connections extracted through some network measurements.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;applications-for-sln&quot;&gt;Applications for SLN&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Massive increase in popularity of &lt;strong&gt;MOOCs&lt;/strong&gt; over the past decades has led to access of unprecedented data that can be easily used in the context of SLNs. Among these data, the discussion forums are of utmost importance as they are the primary means for interaction among students or between students and instructors and help extract SLNs.&lt;/li&gt;
  &lt;li&gt;The blended learning offered by &lt;strong&gt;FLIP&lt;/strong&gt; that combines the elements of online and traditional instruction is also gaining popularity. In this system, watching the lectures online becomes homework and class time is used for discussions instead.&lt;/li&gt;
  &lt;li&gt;In the last decade, there has also been a steady rise in the popularity of Q&amp;amp;A Sites which have emerged with complementary search engines that allows users to enter questions in NLP format.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;1. MOOCs Discussion Forums&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Similar to Q&amp;amp;A sites but have a different set of terminologies.&lt;/li&gt;
  &lt;li&gt;Each course has a &lt;strong&gt;discussion forum&lt;/strong&gt; (hierarchy 1). Discussion forum comprises of &lt;strong&gt;threads&lt;/strong&gt; (hierarchy 2): the first &lt;strong&gt;post&lt;/strong&gt; is by the creator of thread followed by &lt;strong&gt;comments&lt;/strong&gt; (hierarchy 3), hence leading to a 3-level hierarchy.&lt;/li&gt;
  &lt;li&gt;A user interacting with these forums has one of following options:
    &lt;ul&gt;
      &lt;li&gt;Create a thread&lt;/li&gt;
      &lt;li&gt;Create a post&lt;/li&gt;
      &lt;li&gt;Comment on an existing post&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;When threads are created, each forum allows annotation of these threads with a number categories. These categories are often inconsistent as there is no effective mechanism to force learners to abide by them consistently.&lt;/li&gt;
  &lt;li&gt;Posts and comments can also &lt;strong&gt;receive up votes and down votes&lt;/strong&gt; from users and staff.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. FLIP&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The interaction in these classes can be used to extract the SLN as the structure of posts and comments here would resemble that of MOOCs but would be different on the following accounts:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Size and sparsity&lt;/strong&gt;: smaller number of learners and denser connectivity among them.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Informational vs conversational discussions&lt;/strong&gt;: While MOOC discussions are more conversational in nature, SLN from FLIP would typically not include conversational discussions because students can talk informally outside the class and also because the discussions are presided over by an instructor.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Q&amp;amp;A Sites&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Q&amp;amp;A sites allow the following functions to it’s users:
    &lt;ul&gt;
      &lt;li&gt;post question&lt;/li&gt;
      &lt;li&gt;answer question&lt;/li&gt;
      &lt;li&gt;comment on answer&lt;/li&gt;
      &lt;li&gt;up/down vote post&lt;/li&gt;
      &lt;li&gt;allow asker to choose the best or acceptable answer&lt;/li&gt;
      &lt;li&gt;for quality assurance there are points associated with receiving up or down votes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Major differences can be seen between SLNs from such sites and sites from educational settings:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Incentive structure&lt;/strong&gt;: well-defined automated set of incentives to encourage participation. These may or may not be a part of conversation in MOOCs or FLIP, more so because it is not a scalable concept for these forums as participation in MOOCs and FLIP can not be pushed for assessment of student academically.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Broader concept list&lt;/strong&gt;: information propagated in an SLN for a course will be limited to materials associated with the subject. Also each course has it’s own forum where only the students enrolled can participate. Q&amp;amp;A sites have some focal specificity but typically one can expect the number of concepts emerging in these SLNs to be much more broader.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Single learning modality&lt;/strong&gt;: In a Q&amp;amp;A site, SLN is the only means of learning, whereas in an educational setting it is one of the four modalities: lecture videos, assessment and text resources.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;types-of-sln-graphs&quot;&gt;Types of SLN Graphs&lt;/h3&gt;

&lt;p&gt;These are the most commonly used graphs but are not comprehensive.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Undirected graph among learners&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nodes represent learners&lt;/li&gt;
  &lt;li&gt;Undirected links indicate the presence or absence of some characteristics between them. Properties could be age, geographic location, education level, whether or not they have interacted etc.&lt;/li&gt;
  &lt;li&gt;For nodes \(i\) and \(j\), one can say for example,
    &lt;ul&gt;
      &lt;li&gt;\(prop_k(i,j)\) is a binary variable that is 1 iff \(i\) and \(j\) satisfy property \(k\) in set of properties \(K\),&lt;/li&gt;
      &lt;li&gt;\(P \leq |K|\) is a threshold constant i.e. node \(i\) and \(j\) are connected if and only if they both satisfy at least \(P\) criiteria specified in \(K\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(i, j) \in G \Leftrightarrow \sum_{k \in K} prop_k(i, j) \geq P \tag{1} \label{1}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;2. Directed graph among learners&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Used to note flow of information in the SLN.&lt;/li&gt;
  &lt;li&gt;A directional link frok \(i\) to \(j\) represents an answer by \(j\) to a question posted by \(i\). Several restrictions can be added to this directional flow e.g. include only “best answers” given.&lt;/li&gt;
  &lt;li&gt;It could be a multi-graph where there is more than one link from \(i\) to \(j\) since learners can ask and answer more than one question each.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Undirected graph among learners and concepts&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Nodes are used to represent both the learners as well as the concepts.&lt;/li&gt;
  &lt;li&gt;Key concepts are extracted in a number of ways:
    &lt;ul&gt;
      &lt;li&gt;running textual analysis to find keywords in discussions&lt;/li&gt;
      &lt;li&gt;using syllabus specified by the instructors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Such graphs are generally bipartite graphs between learners and concepts.&lt;/li&gt;
  &lt;li&gt;A setting similar to \eqref{1} represents this graph, but each property \(k\) represent a condition on the participation of user \(i\) in concept \(j\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. Directed graph among learners and concepts&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To depict structure of interactions in more details.&lt;/li&gt;
  &lt;li&gt;Concept nodes are used and each question or post by a learner is handled separately, which allows tow sets of links for each post: \((i_0, j) \in G\) for learner \(i_0\) who makes the initial post, and \((j, i_l) \in G\) for each learner \(i_l\) who commented on \(j\).&lt;/li&gt;
  &lt;li&gt;In a forum where up/down votes are allowed, these links can be weighted to match the net votes obtained.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;research-objectives&quot;&gt;Research Objectives&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Predictions&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: ability to predict performance on assessments - homework, quiz, or exam questions - a student has not taken.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Drop-off rate&lt;/strong&gt;: predict drop-off rates of a course. This could be for an individual student or for the volume of participation in the course as a whole. Metrics on interest in such a case would be the completion rate of assignments, lecture videos, or rate of involvement in discussion forums.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Recommendations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Courses and topics&lt;/strong&gt;: to help students locate courses of interests based on their interactions with the enrolled courses. This presents an opportunity to improve the learning experience by recommending new courses and redirecting to relevant discussion forums.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Study buddies&lt;/strong&gt;: to help students locate study partners over MOOCs where the number of students participating can vary in terms of engagement, interests, demographics, geography etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recommendation algorithms could focus on similarities or, more importantly dissimilarities between users. For example, a learner who actively engages on discussion forums can be paired with one who is struggling in those topics.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Peer-grading&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In MOOCs, generally the teacher-to-student ratio is very small. As a result, it would be infeasible for a teaching staff to manually grade each submission.&lt;/li&gt;
  &lt;li&gt;This is generally tackled by only giving away machine gradable homework or exams, such as MCQs. But this limits the variety and quality of questions that can be posed.&lt;/li&gt;
  &lt;li&gt;A different approach would be where students score each others work. This method lacks efficacy so far, because:
    &lt;ul&gt;
      &lt;li&gt;different students have different grading quality&lt;/li&gt;
      &lt;li&gt;time commitment is required for grading&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Structure of SLN might help in locating quality-graders for each assignment related to a specific topic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. Personalization&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Online education poses the question of trade-off between efficacy and scale in learning. It is statistically seen that only 10% of students enrolling in a MOOC ever complete the courses.&lt;/li&gt;
  &lt;li&gt;The ineffectiveness of MOOCs can be because of the following reasons:
    &lt;ul&gt;
      &lt;li&gt;teacher-to-student ratio is very low&lt;/li&gt;
      &lt;li&gt;learning is asynchronous&lt;/li&gt;
      &lt;li&gt;student population is very diverse and hard to personalize&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Advance technology is required for course individualization, to lift tradeoff curve and enable effective learning environment at massive scales rather than having a one-size-fits-all online course.&lt;/li&gt;
  &lt;li&gt;The information stored  in SLNs can play a key role in such adaptation to become a part of learning experience.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-06-02-social-learning-networks/fig-1-flowchart-of-individualization.png?raw=true&quot; alt=&quot;Fig-1: Flowchart of individualizaiton&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The key components of such an indivualization effort can be seen in the flowchart in Fig-1.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Behavioural measurements&lt;/strong&gt;: measurement of user behaviour while engaging with course material, e.g. video watching trajectory (pauses and jumps) can be captured, information that user enters in a discussion forum can also be collected.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Data analytics&lt;/strong&gt;: use machine learning techniques to generate a low-dimensional model of the high-dimensional process of learning. The latent space can be:
        &lt;ul&gt;
          &lt;li&gt;discovered throught data mining&lt;/li&gt;
          &lt;li&gt;defined in advance in terms of author-specified learning features&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Content/presentation adaptation&lt;/strong&gt;: based on the analysis, user’s updated profile dictates decisions on what content will be presented next and how it will be presented, e.g. different versions of text and video may be presented.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methodologies&quot;&gt;Methodologies&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Data Collection&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are two basic modes of data collections, both with pros and cons:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Use existing data&lt;/strong&gt;: various open online course offerings over the past years remain open even after the sessions end and gives access to the discussion forums etc. Similary the SLN data from Q&amp;amp;A portals is accessible through the respective websites. This data can easily be crawled and scraped by writing scripts to extract the information from the pages. Major drawbacks of this methodology are:
        &lt;ul&gt;
          &lt;li&gt;no opportunity to excite the state of SLN formation for subsequent data analysis&lt;/li&gt;
          &lt;li&gt;only data on open courses is available&lt;/li&gt;
          &lt;li&gt;public data is only accessible upto a certain measurement granularity&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Generate new data&lt;/strong&gt;: To overcome the cons of earlier method, one can collaborate with the educators. Or alternatively, a team could invest resources in creating  a brand new online education platform to host courses for a number of instructors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Analysis&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This approach varies widely based on the research objective being tackled and generally involves large-scale machine learning methods.&lt;/li&gt;
  &lt;li&gt;Linear regression models can be used to determine which course properties are correlated with learner participation in the forums, which can be quantified using the number of posts that appeared each day for each course in the dataset.&lt;/li&gt;
  &lt;li&gt;Using user-quiz pair matrix, algorithms can be trained:
    &lt;ul&gt;
      &lt;li&gt;baseline predictor for solving least square optimization to minimize error in terms of student and quiz biases.&lt;/li&gt;
      &lt;li&gt;neighbourhood predictorthat extends the baseline to leverage student-student and quiz-quiz similarities&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;SLN data from the online interactions can lead to tracking of information in the network that can assist various objectives that lead to better learning outcomes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/6814139&quot; target=&quot;_blank&quot;&gt;Social Learning Networks: A brief survey&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 02 Jun 2019 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2019/06/02/social-learning-networks/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2019/06/02/social-learning-networks/</guid>
        
        <category>machine-learning</category>
        
        <category>papers</category>
        
        <category>sln</category>
        
        
      </item>
    
      <item>
        <title>Breaking down Tesseract OCR</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;It was originally an HP research project between 1984 and 1994, which was presented at 1995 UNLV Annual Test of OCR Accuracy where it performed beyond expectations.&lt;/li&gt;
  &lt;li&gt;Purpose of tesseract was integration with the flatbed HP scanners with objectives such as compression which was not possible with the then existing commercial OCR solutions which were struggling with accuracy.&lt;/li&gt;
  &lt;li&gt;During a phase of development, work concentrated on improving rejection efficiency than on base-level accuracy.&lt;/li&gt;
  &lt;li&gt;Finally in 2005, Tesseract was released as an open-source project by HP available at Google Code until it was finally moved to &lt;a href=&quot;https://github.com/tesseract-ocr/tesseract&quot; target=&quot;\_blank&quot;&gt;Github&lt;/a&gt; for open-source contribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Because of HP’s proprietary layout analysis technology, Tesseract did not have it’s own dedicated layout analyser. As a result, Tesseract assumes the inputs to be &lt;strong&gt;binary image with optional polygonal text regions defined&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Connected Component Analysis&lt;/strong&gt; is the first step in which the outlines of the components are stored. Outlines are gathered together, purely by nesting, into &lt;strong&gt;Blobs&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Blobs are organized into text lines, and the the lines and regions are analyzed for &lt;strong&gt;fixed pitch&lt;/strong&gt; or &lt;strong&gt;proportional text&lt;/strong&gt;. The lines are broken into words differently based on the kind of character spacing. Fixed pitch text is chopped immidiately by character cells. Proportional text is broken into words using &lt;strong&gt;definite spaces or fuzzy spaces&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Recognition&lt;/strong&gt; proceeds as a &lt;strong&gt;two-pass process&lt;/strong&gt;. During the &lt;strong&gt;first pass&lt;/strong&gt;, attempt is made to recognize each word. The words that are satifactorily identified are passed to an &lt;strong&gt;adaptive classifier&lt;/strong&gt; as training data. As a result the adaptive classifier gets a chance at improving results among text lower down on the page. In order to utilize the training of adaptive classifier on the text near the top of the page as &lt;strong&gt;second pass&lt;/strong&gt; is performed, during which words that were not recognized well enough are classified again.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Final Phase&lt;/strong&gt; resolves the &lt;strong&gt;fuzzy spaces&lt;/strong&gt;, and checks &lt;strong&gt;alternative hypotheses&lt;/strong&gt; for the x-height to locate small-cap text.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;line-and-word-finding&quot;&gt;Line and Word Finding&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Line Finding&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Algorithm is designed so that skewed page can be recognized without having to deskew, thus preventing any loss of image quality.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Blob filtering&lt;/strong&gt; and &lt;strong&gt;line construction&lt;/strong&gt; are key parts of this process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Under the assumption that most blobs have uniform text size, a simple &lt;strong&gt;percentile height filter&lt;/strong&gt; removes drop-caps and vertically touching characters and &lt;strong&gt;median height&lt;/strong&gt; approximates the text size in the region.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Blobs smaller than a certain fraction of the median height are filtered out, being most likely punctuation, diacritical marks and noise.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The filtered blobs are more likely to fit a model of &lt;strong&gt;non-overlapping, parallel, but sloping lines&lt;/strong&gt;. Sorting and processing the blobs by x-coordinates makes it possible to assign blobs to a unique text line, while tracking the slope across the page.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Once the lines are assigned, a &lt;strong&gt;least median of squares fit&lt;/strong&gt; is used to estimate the baselines, and filtered-out blobs are fitted back into appropriate lines.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Final step merges blobs that overlap by at least half horizontally, putting diacritical marks together with the correct base and correctly associating parts of some broken characters.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Baseline Fitting&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using the text lines, baselines are fitted precisely using a &lt;strong&gt;quadratic spline&lt;/strong&gt;, which allows Tesseract to handle pages with curved baselines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-15-breaking-down-tesseract-ocr/fig-1-curved-fitted-baseline.png?raw=true&quot; alt=&quot;Fig-1: Curved Fitted Baseline&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Baseline fitting is done by partitioning the blobs into groups of reasonable continuous displacement for the original straight baseline. A quadratic spline is fitted to the most populous partition by a least square fit.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Fixed Pitch Detection and Chopping&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lines are tested to determine whether they are fixed pitch. Where it finds fixed pitch text, Tesseract chops the words into characters using pitch, and disables the &lt;strong&gt;chopper&lt;/strong&gt; and &lt;strong&gt;associator&lt;/strong&gt; on these words for the &lt;strong&gt;word recognition step&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-15-breaking-down-tesseract-ocr/fig-2-fixed-pitch-chopped-word.png?raw=true&quot; alt=&quot;Fig-2: Fixed Pitch Chopped Word&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Proportional Word Finding&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Detecting word boundaries in a not-fixed-pitch or proportional text spacing is highly non-trivial task.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-15-breaking-down-tesseract-ocr/fig-3-difficult-word-spacing.png?raw=true&quot; alt=&quot;Fig-3: Difficult Word Spacing&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For example, the gap between the tens and units of ‘11.9%’ is similar size to general space, but is certainly larger the kerned space between ‘erated’ and ‘junk’. Another case can be noticed that there is no horizontal gap between the bounding box of ‘of’ and ‘financial’.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tesseract solves most of these problems by measuring &lt;strong&gt;gaps in a limited vertical range between baseline and mean line&lt;/strong&gt;. Spaces close to a threshold are made &lt;strong&gt;fuzzy&lt;/strong&gt;, where the decisions are made after &lt;strong&gt;word recognition&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;word-recognition&quot;&gt;Word Recognition&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A major part of any word recognition algorithm is to identify how a word should be segmented into characters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The initial segmented outputs from line finding is classified first. The non-fixed pitch text in the remaining text is classified using other word recognition steps.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;1. Chopping Joined Characters&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tesseract attempts to improve the result by chopping the blob with worst confidence from the character classifier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Chop points&lt;/strong&gt; are found from concave vertices of a poligonal approximation of the outline, which may have a concave vertex opposite or a line segment. It may take upto 3 pairs of chop points to successfully separate joined characters from ASCII set.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-15-breaking-down-tesseract-ocr/fig-4-candidate-chop-points-and-chop.png?raw=true&quot; alt=&quot;Fig-4: Candidate Chop Points and Chop&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Chops are executed in priority order. Any chop that fails to improve the confidence of the result is undone, but not completely discarded so that it can be re-used by the &lt;strong&gt;associator&lt;/strong&gt; if needed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Associating Broken Characters&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;After the potential chops have been exhausted, if the word is still not good enough, it is given to the &lt;strong&gt;associator&lt;/strong&gt;, which makes a &lt;strong&gt;best first search&lt;/strong&gt; of the segmentation graph of the possible combinations of the maximally chopped blobs into candidate characters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The search pulls candidate new states from a priority queue and evaluates them by classifying unclassified combinations of fragments.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The chop-then-associate method is inefficient but it gives a benefit of simpler data structures that would be required to maintain the full segmentation graph.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-15-breaking-down-tesseract-ocr/fig-5-broken-characters.png?raw=true&quot; alt=&quot;Fig-5: Broken Characters recognized by Tesseract&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This ability of Tesseract to successfully classify broken characters gave it an edge over the contemporaries.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;static-character-classifier&quot;&gt;Static Character Classifier&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Features&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Early version of Tesseract used &lt;strong&gt;topological features&lt;/strong&gt;, which are independent of font and size but are not robust to issues found in real-life images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another idea for classification involved use of segments of the polygonal approximation as features, but this method is also not robust to damaged characters.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-15-breaking-down-tesseract-ocr/fig-6-poligonal-approximation-features.png?raw=true&quot; alt=&quot;Fig-6: Differences in Polygonal Approximation for same character&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Solution to these problems lie in the fact that the features in the unknown need not be the same as the features in the training data. During training, &lt;strong&gt;segments of a polygonal approximation&lt;/strong&gt; are used for features, but during recognition, features of a small, fixed length are extracted from the outline and matched many-to-one against the clustered prototype features of the training data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The process of small features matching large prototypes is easily able to cope with recognition of damaged words.&lt;/strong&gt; It’s main problem is that the computational cost of computing the distance between an unknown and a prototype is very high.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Classfication&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This stage proceeds as a &lt;strong&gt;two-step process&lt;/strong&gt;. First step involves a &lt;strong&gt;class pruner&lt;/strong&gt; that creates a shortlist of character classes that the unknown might match.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The classes shortlisted in step one are taken further to the next step, where the actual similarity is calculated from the feature bit vectors. &lt;strong&gt;Each prototype character class is represented by a logical sum-of-product expression with each term called a configuration&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Training Data&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The classifier is trained on a mere 20 samples of 94 characters from 8 fonts in a single size, but with 4 attributes (normal, bold, italic, bold italic), making the total of 60160 training samples.&lt;/p&gt;

&lt;h3 id=&quot;linguistic-analysis&quot;&gt;Linguistic Analysis&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Whenver the word recognition module is considering a new segmentation, the linguistic model (called &lt;strong&gt;permuter&lt;/strong&gt;) choses the best available word string in the categories: &lt;strong&gt;Top frequent word, Top dictionary word, Top numeric word, Top UPPER case word, Top lower case word (with optional initial upper)&lt;/strong&gt;, where the final decision for segmentation is simply the word with the lowest total distance rating.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since words from different segmentations may have different number of characters in them, it would be hard to compare these words directly (even if a classifier claims to produce probabilities, which Tesseract does not).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tesseract instead produces two numbers to solve this issue, namely,&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Confidence&lt;/strong&gt;, is minus the normalized distance from the prototype. It is confidence in the sense that greater the number better a metric it is.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Rating&lt;/strong&gt; is product of normalized distance from the prototype and total outline length in the unknown character.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adaptive-classifier&quot;&gt;Adaptive Classifier&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;OCR engines are benefitted from use of an adaptive classifier because the static classifier has to be good at generalizing to any kind of font, its ability to discriminate between different characters or between characters and non-characters is weakened.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tesseract has a font-sensitive adaptive classifier that is trained using the output of the static classifiers which is commonly used to obtain greater discrimination within each document, where the number of fonts is limited.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It uses the same features and classifier as the static classifier to train the adaptive classifier. &lt;strong&gt;The only significant difference between the two classifiers apart from the training data is that the adaptive classifier uses the isotropic baseline/x-height normalization, whereas the static classifier normalizes the characters by the centroid (first moment) for position, and second moments for anisotropic size normalization.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The baseline normalization helps distinguish the upper and lower case characters and also improves immunity to noise specks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-01-15-breaking-down-tesseract-ocr/fig-7-baseline-and-moment-normalized.png?raw=true&quot; alt=&quot;Fig-7: Baseline and Moment Normalized letters&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The main benefit of character moment normalization is &lt;strong&gt;removal of font aspect ratio&lt;/strong&gt; and some degree of &lt;strong&gt;font stroke width&lt;/strong&gt;. It also makes recognition of subscripts and superscripts easier, but requires an additional feature to distinguish the uppercase and lowercase characters.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;references&quot;&gt;REFERENCES&lt;/h1&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/33418.pdf&quot; target=&quot;\_blank&quot;&gt;An Overview of the Tesseract OCR Engine&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2019/01/15/breaking-down-tesseract-ocr/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2019/01/15/breaking-down-tesseract-ocr/</guid>
        
        <category>vision</category>
        
        <category>ocr</category>
        
        <category>machine-learning</category>
        
        <category>papers</category>
        
        
      </item>
    
      <item>
        <title>Social Bias in Machine Learning</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Discrimination, injustice, oppression are some of the dark words that have been an integral part of human history. While there is a active effort to make world a fair place in every sphere of life, it is almost impossible to make the data that has been recorded over the years fair to all the caste, creeds, races and religions because the history is written in ink. Since the world was far more biased as we age backwards in time, it would not be incorrect to say that a historical record of data would often reflect these biases in terms of minority and majority classes. These are the very same data that is continuously used in training most of our machine learning models without actually giving a conscious thought to the fairness of the algorithm i.e. whether or not the algorithm reflects the biases that prevailed back then. Recently machine learning has seen its utilitization in a lot of important decision making pipelines such as predicting time of recidivism, college acceptance, loan approvals etc., and hence it becomes increasingly important to question the machine learning models being developed in terms of implicit bias that they might be inheriting from the data that they train on. In order to do away with such biases in a machine learning algorithm one needs to understand how exactly does bias creep in, what are the various metrics through which it can be measured and what are the methods through which one can remove such unfairness. This post is an attempt to summarize such issues and possible remedies.&lt;/p&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;Since machine learning is now being used to make a lot policy decisions that affect the life of people on an everyday basis, it should be made sure that unfairness is not a part of such decision making. It is found that training machine learning algorithms with the standard utility maximization and loss minimization objectives sometimes result in algorithms that behave in a way that a fair human observer would deem biased. A very recent example of such a case was &lt;a href=&quot;https://www.ml.cmu.edu/news/news-archive/2018/october/amazon-scraps-secret-artificial-intelligence-recruiting-engine-that-showed-biases-against-women.html&quot; target=&quot;\_blank&quot;&gt;cited&lt;/a&gt; by Amazon which notices a gender bias in its recruiting engine algorithms.&lt;/p&gt;

&lt;h3 id=&quot;its-all-in-the-data&quot;&gt;It’s all in the Data&lt;/h3&gt;

&lt;p&gt;One of the potential reasons for such biases in these algorithms can be attributed to the training data itself. Since the algorithms are big numerical puzzles that are trained to recognize and mimic the statistical patterns over the history, it is only natural for such a trained system to display biased characteristics. Even some of the state of the art solutions in the field of NLP and Machine Learning are not free from biases and unfairness. For example, it has been shown that word2vec embeddings learnt from huge corpuses of text often show gender bias as the euclidean distance between words that signifies correlation between words, suggests strong correlation between words like homemaker, nanny with she and maestro, boss with he. Any system built on top of such a word embedding is very likely to propagate this bias on a daily basis at some level.&lt;/p&gt;

&lt;p&gt;One of the contested ways of dealing with this issue is to retrain the models continuously with new data, which relies on the assumption that historical bias is on a process of correcting itself.&lt;/p&gt;

&lt;p&gt;Another major question that continuously arise is based on the fact the these machine learning algorithms work well when the amount of data they train on is huge. While this is true in an overall sense, if we break down the number of data points one has for minority class it becomes more apparent that the algorithms does not have enough supporting instances to learn as good a representation about minority classes as it would about the majority and hence could lead in unfair judgements because of lack of data.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There is general tendency for automated decisions to favor those who belong to statistically dominant groups.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Statistical patterns that apply to majority population might be invalid for the the minority group. It can also happen that a variable that is positively correlated with target in general population maybe negatively correlated with target in the minority group. For example, a real name might be a short common name in one culture and a long unique name in another. Hence same rules for detecting fake names would not work across such groups.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-10-09-algorithmic-fairness/fig-2-survival-distribution.png?raw=true&quot; alt=&quot;Fig-1: Survival Distribution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider a very simple dataset from Kaggle called &lt;a href=&quot;https://www.kaggle.com/c/titanic&quot; target=&quot;\_blank&quot;&gt;titanic&lt;/a&gt;. This is a basic dataset where based on a bunch of features given one has to &lt;strong&gt;predict the survival probability of an individual who was on titanic&lt;/strong&gt;. The survival distribution on the training data shows that in past &lt;strong&gt;during the titanic incident a female candidate had much higher chances of surviving than a male candidate&lt;/strong&gt;. It would be rather obvious &lt;strong&gt;for an algorithm trained on this data that being female is a strong indicator of survival&lt;/strong&gt;. If the same algorithm was used to predict survival on an impending sinking incident where candidates who have higher survival probability would be boarded on rescue boats first, it is bound to make biased decisions.&lt;/p&gt;

&lt;p&gt;Also it can be seen that being male is negatively correlated to surviving while being a female is positively correlated, because graph 2 in fig-1 shows that more males died than survived and by contrast,  more females survived than died. So &lt;strong&gt;if the algorithm was to learn only from majority of the data belonging to males, it would predict badly for the female population&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;undeniable-complexities&quot;&gt;Undeniable Complexities&lt;/h3&gt;

&lt;p&gt;One way to counter the sample size disparity might be to learn different classifiers for different sub-groups. But it is not as simple as it sounds because of the reason that learning and testing for individual sub-group might require acting on the protected attributes which might in itself be objectionable. Also the definition of minority is fuzzy as there could be many different overlapping minorities and no straightforward way of determining group membership.&lt;/p&gt;

&lt;h3 id=&quot;noise-vs-modeling-error&quot;&gt;Noise vs Modeling Error&lt;/h3&gt;

&lt;p&gt;Say a classifier achieves 95 percent accuracy. In the real world scenario this 5 percent error rate would point to a really well trained classifier. But what is often overlooked is that there might be two different kinds of underlying reasons behind the error rate. One could be the general case of noise that the classifier was not able to model and hence was not able to predict and account for. Other possible reason could be that while the model is 100 percent accurate on majority class, it is only 50 percent accurate on minority class. This systematic error in the minority class would be a clear case of algorithmic unfairness.&lt;/p&gt;

&lt;p&gt;The bigger issue of the matter here is that there is no principled or book methodology for distinguishing noise from the modeling errors. Such questions can only be answered by great deal of domain knowledge and experience.&lt;/p&gt;

&lt;h3 id=&quot;edge-cases-always-exist&quot;&gt;Edge Cases always exist&lt;/h3&gt;

&lt;p&gt;It is also true to assume that in a very unexpected way it is possible for bias to creep into the algorithms even if the training data is labelled correctly and is free of any issues that could be pointed out as unbiased. A recent &lt;a href=&quot;https://www.theverge.com/2015/7/1/8880363/google-apologizes-photos-app-tags-two-black-people-gorillas&quot; target=&quot;\_blank&quot;&gt;example&lt;/a&gt; of this is when google photos by mistake labeled two black people as gorillas. Obviously, the machine was never trained with any training data that should lead to such inferences, but because the number of trained parameters are so high, it often becomes intractable and unimaginably hard to understand why a system behaves haphazardly in certain conditions. This uncertainty of outcomes can also be a cause of bias in situations that could not be predicted in advance.&lt;/p&gt;

&lt;h3 id=&quot;what-is-fairness&quot;&gt;What is Fairness?&lt;/h3&gt;

&lt;p&gt;Fairness in classification involves studying algorithms not only from a perspective of accuracy, but also from a perspective of fairness.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The most difficult part of this is to define what is fairness.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Consideration for fairness often leads to compromise on accuracy but it’s a necessary evil that is not going anywhere in the near future. What if often more surprising is that many of these metrics have a trade off among themselves.&lt;/p&gt;

&lt;h3 id=&quot;fairness-of-process-vs-fairness-of-outcome&quot;&gt;Fairness of Process vs Fairness of Outcome&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;An &lt;strong&gt;aware&lt;/strong&gt; algorithm is one that uses the information regarding the protected attribute (such as gender, ethnicity etc.) in the process of learning. An &lt;strong&gt;unaware&lt;/strong&gt; algorithm will not.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;While the motivation regarding unaware algorithm is that being fair means disregarding the protected attribute, it often does not work just by removing the protected attribute. Sometimes there is a strong correlation between protected attribute and some other feature. So in order to train a truly unaware algorithm, one needs to remove the correlated feature group as well.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This process of manually engineering a feature list that conveys no information about the protected attribute can also be automated using machine learning techniques discussed in following sections.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;are-unaware-algorithms-the-solution&quot;&gt;Are Unaware Algorithms the Solution&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There could be inherent differences between the populations defined by these masked protected attributes, which would only render this process undesirable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The aware approaches use these proctected attributes and have a better chance of understanding depence of outcome on them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This can be seen as a distinction between &lt;strong&gt;fairness of process&lt;/strong&gt; vs &lt;strong&gt;fairness of outcomes&lt;/strong&gt;. The unaware algorithms ensure a fairness of process, because under such a scheme the algorithm does not use any of the protected attributes for decision making. However, such fairness in process does not guarantee a fair outcome towards the protected and un-protected sub-groups.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The aware approaches on the contrary use these protected attributes and hence not a fair process, but it can reach an outcome that is more fair towards the minorities.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mathematical-fairness-statistical-parity&quot;&gt;Mathematical Fairness: Statistical Parity&lt;/h3&gt;

&lt;p&gt;A mathematical version of absolute fairness can be a statistical condition where the chances of success or failure is same for both the majority and minority classes (or more classes in case of multi-class scenarios). This can be written as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Pr[h(x) = 1 \vert x \in P^C] = Pr[h(x) = 1 \vert x \in P] \tag{1} \label{1}&lt;/script&gt;

&lt;p&gt;The main drawback of such models is given by the argument that is that &lt;strong&gt;does one really want to equalize the outcomes across all sub-groups?&lt;/strong&gt;. For example, predicting the success chances of a basketball player irrespective of his height is not really a very strong model, because the discrimination in various domains do not really fall in a black or white region but may lie in the gray region somewhere in between. Another example might be predicting the chances of child birth without using the features such as gender and age would be a really poor algorithm. So, &lt;strong&gt;enforcing the statistical parity is not always the solution&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;cross-group-calibration&quot;&gt;Cross-group Calibration&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Instead of equalizing the outcomes themselves, one can look to equalize some other statistics of the algorithm’s performance, for example &lt;strong&gt;error rates across groups&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;A fair algorithm would make as many mistakes on a minority group as it does on the majority group.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A useful tool for such an analysis is the confusion matrix as shown below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-10-09-algorithmic-fairness/fig-1-confusion-matrix.png?raw=true&quot; alt=&quot;Fig-2: Confusion Matrix&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Some of the metrics based on the confusion matrix are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Treatment equality&lt;/strong&gt; is achieved by a classifier that yields a ratio of false negatives and false positives (in table, c/b or b/c) that is same for both protected group categories.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Conditional procedure accuracy equality&lt;/strong&gt; is achieved when conditioning on the known outcome, the classifier is equally accurate across protected group categories. This is equivalent to the false negative rate and false positive rate being same for all protected categories.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since all the columns and rows of a confusion matrix should add up to the total number of observations, many of these fainess metrics have a trade-off relationship. This basically means &lt;strong&gt;zero-sum game&lt;/strong&gt;, one increases at the cost of the other and there is no win-win situation. Based on the use-case one has to decide which metrics should be optimized for as there is no blanket solution to the group.&lt;/p&gt;

&lt;h3 id=&quot;example-titanic&quot;&gt;Example: Titanic&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/shamssam/algorithmic-fairness-in-ml&quot; target=&quot;\_blank&quot;&gt;&lt;strong&gt;Kaggle Notebook&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;### libraries
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;xgboost&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBClassifier&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'train.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index_col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PassengerId'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'female'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Ticket'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Cabin'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Embarked'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stratify&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# aware classification
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Survived'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OVERALL&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_valid_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Survived'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;FEMALE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_valid_hat_female&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Survived'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat_female&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat_female&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;MALE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_valid_hat_male&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Survived'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat_male&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat_male&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# output
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ========================================
# OVERALL
# ========================================
#              precision    recall  f1-score   support
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#           0       0.85      0.90      0.87       165
#           1       0.82      0.75      0.78       103
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# avg / total       0.84      0.84      0.84       268
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Accuracy: 0.8395522388059702
# ========================================
# FEMALE
# ========================================
#              precision    recall  f1-score   support
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#           0       0.45      0.41      0.43        22
#           1       0.83      0.86      0.84        76
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# avg / total       0.75      0.76      0.75        98
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Accuracy: 0.7551020408163265
# ========================================
# MALE
# ========================================
#              precision    recall  f1-score   support
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#           0       0.90      0.97      0.94       143
#           1       0.75      0.44      0.56        27
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# avg / total       0.88      0.89      0.88       170
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Accuracy: 0.888235294117647
&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# unaware classification
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Survived'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Sex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OVERALL&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_valid_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Survived'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Sex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;FEMALE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_valid_hat_female&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Survived'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Sex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat_female&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat_female&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;MALE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;=&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_valid_hat_male&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Survived'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Sex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classification_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat_male&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_valid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_valid_hat_male&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# output
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ========================================
# OVERALL
# ========================================
#              precision    recall  f1-score   support
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#           0       0.73      0.84      0.78       165
#           1       0.66      0.51      0.58       103
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# avg / total       0.71      0.71      0.70       268
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Accuracy: 0.7126865671641791
# ========================================
# FEMALE
# ========================================
#              precision    recall  f1-score   support
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#           0       0.32      0.82      0.46        22
#           1       0.90      0.50      0.64        76
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# avg / total       0.77      0.57      0.60        98
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Accuracy: 0.5714285714285714
# ========================================
# MALE
# ========================================
#              precision    recall  f1-score   support
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#           0       0.91      0.84      0.87       143
#           1       0.39      0.56      0.46        27
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# avg / total       0.83      0.79      0.81       170
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Accuracy: 0.7941176470588235
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Confusion matrices for the cases using awareness and without awareness of protected attribute (sex in this case) is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-10-09-algorithmic-fairness/fig-3-aware-confusion-matrix.png?raw=true&quot; alt=&quot;Fig-3: Aware Confusion Matrix&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-10-09-algorithmic-fairness/fig-4-unaware-confusion-matrix.png?raw=true&quot; alt=&quot;Fig-4: Unaware Confusion Matrix&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Conditional accuracy in the code output shows that the system is very biased both in aware in unaware scenarios.&lt;/li&gt;
  &lt;li&gt;treatment equality is more divergent in aware case than in unaware case.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://towardsdatascience.com/a-gentle-introduction-to-the-discussion-on-algorithmic-fairness-740bbb469b6&quot; target=&quot;_blank&quot;&gt;A Gentle Introduction to the Discussion on Algorithmic Fairness
&lt;/a&gt;&lt;/small&gt;&lt;br /&gt; 
&lt;small&gt;&lt;a href=&quot;https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de&quot; target=&quot;\_blank&quot;&gt;How big data is unfair&lt;/a&gt;&lt;/small&gt;&lt;br /&gt; 
&lt;small&gt;&lt;a href=&quot;http://fairness-measures.org/&quot; target=&quot;\_blank&quot;&gt;Fairness Measures&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 09 Oct 2018 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2018/10/09/algorithmic-fairness/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2018/10/09/algorithmic-fairness/</guid>
        
        <category>machine-learning</category>
        
        <category>algorithmic-fairness</category>
        
        
      </item>
    
      <item>
        <title>Introduction to Computer Architecture</title>
        <description>&lt;h3 id=&quot;what-is-a-computer&quot;&gt;What is a Computer?&lt;/h3&gt;

&lt;p&gt;A computer is a general purpose device that can be programmed process information, yield meaningful results.&lt;/p&gt;

&lt;p&gt;The three important take-aways being:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;programmable device&lt;/li&gt;
  &lt;li&gt;process information&lt;/li&gt;
  &lt;li&gt;yield meaningful results&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So the important parts for the working of a computer are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Program: a list of instructions given to computer&lt;/li&gt;
  &lt;li&gt;Information Store: the data it has to process&lt;/li&gt;
  &lt;li&gt;Computer: processes information into meaningful results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A fully functional computer includes at the very least:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Processing Unit (CPU)&lt;/li&gt;
  &lt;li&gt;Memory&lt;/li&gt;
  &lt;li&gt;Hard disk&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other than these some input output (I/O) devices can also be a part of the system, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Keyboard: Input&lt;/li&gt;
  &lt;li&gt;Mouse: Input&lt;/li&gt;
  &lt;li&gt;Monitor: Output&lt;/li&gt;
  &lt;li&gt;Printer: Output&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;memory-vs-hard-disk&quot;&gt;Memory vs Hard Disk&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Storage Capacity: more on hard disk, less on memory&lt;/li&gt;
  &lt;li&gt;Volatile: data on hard disk is non-volatile, while on memory is volatile&lt;/li&gt;
  &lt;li&gt;Speed: speed of access and other operations are slower on hard disk when compared to memory.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;brain-vs-computer&quot;&gt;Brain vs Computer&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Brain is capable of doing a lot of abstract work that computers cannot be programmed to do.&lt;/li&gt;
  &lt;li&gt;Speed of basic calculations is much higher in a computer which is its primary advantage.&lt;/li&gt;
  &lt;li&gt;Computers do not get tired or bored or disinterested.&lt;/li&gt;
  &lt;li&gt;Humans can understand complicated instructions in a variety of semantics and languages.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;program&quot;&gt;Program&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Write a instruction in a high level language like C, C++, Java etc. (done by human interface)&lt;/li&gt;
  &lt;li&gt;Compile it into an executable (binary) that converts it into byte-code, i.e. the language computers understand. (done by compilers)&lt;/li&gt;
  &lt;li&gt;Execute the binary. (done by processor)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;instruction-set-architecture-isa&quot;&gt;Instruction Set Architecture (ISA)&lt;/h3&gt;

&lt;p&gt;The semantics of all the instructions supported by a processor is known as instruction set architecture (ISA). This includes the semantics of the instructions themselves along with their operands and interfaces with the peripherals.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;ISA is an interface between software and hardware.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Examples of ISA:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;arithmetic instructions&lt;/li&gt;
  &lt;li&gt;logical instructions&lt;/li&gt;
  &lt;li&gt;data transfer/movement instructions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Features of ISA:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Complete: it should be able to execute the programs a user wants to write&lt;/li&gt;
  &lt;li&gt;Concise: smaller set of instructions, currently they fall in the range 32-1000&lt;/li&gt;
  &lt;li&gt;Generic: instructions should not be too specialized for a given user or a given system.&lt;/li&gt;
  &lt;li&gt;Simple: instructions should not be complicated&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are two different paradigms of designing an ISA:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RISC: Reduced Instruction Set Computer has fewer set of simple and regular instructions in the range 64 to 128. eg. ARM, IBM PowerPC. Found in mobiles and tablets etc.&lt;/li&gt;
  &lt;li&gt;CISC: Complex Instruction Set Computer implements complex instructions which are highly irregular, take multiple operands. Also the number of instructions are large, typically 500+. eg. Intel x86, VAX. Used in desktops and bigger computers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;completeness-of-isa&quot;&gt;Completeness of ISA&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;How do we ensure the completeness of an ISA?&lt;/strong&gt; Say, there are two instructions addition and subtraction, while it is possible to implement addition using substraction (a + b = a - (0 - b)), the same cannot be said otherwise. This basically means that &lt;strong&gt;in order to complete an ISA one needs a set of instructions such that no other instruction is more powerful than the set&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we ensure that one has a complete instruction set such that one can write any program?&lt;/strong&gt; The answer to this lies in finding a &lt;strong&gt;Universal ISA&lt;/strong&gt; which would inturn constitute a &lt;strong&gt;Universal Machine&lt;/strong&gt; which can be used to write any program known to mankind (Universal Machine has a set of basic actions where each such action can be interpretted as an instruction).&lt;/p&gt;

&lt;h3 id=&quot;turing-machine&quot;&gt;Turing Machine&lt;/h3&gt;

&lt;p&gt;Alan Turing, the father of computer science discovered a the theoretical device called &lt;strong&gt;turing machine&lt;/strong&gt; which is the most powerful machine known because theoretically it can compute the results of all the programs one can be interested in.&lt;/p&gt;

&lt;p&gt;A turing machine is a hypothetical machine which consists of an &lt;strong&gt;infinite tape consisting of cells&lt;/strong&gt; extending in either directions, a &lt;strong&gt;tape head to maintain pointer on the tape that can move left or right&lt;/strong&gt;, a &lt;strong&gt;state cell the saves the current state&lt;/strong&gt; of the machine, and an &lt;strong&gt;action table to write down the set of instructions&lt;/strong&gt;. It is posed as an thesis ( &lt;strong&gt;Church-Turing Thesis&lt;/strong&gt; and not a theorem) that has not been counter in the past 60 years that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Any real-world computation can be translated into an equivalent computation involving Turing machine.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Also,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Any computer that is equivalent to a Turing machine is said to be Turing Complete.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So the answer to &lt;strong&gt;Can we build a complete ISA&lt;/strong&gt; lies in the question &lt;strong&gt;can we design a Universal Turing Machine (UTM) that an simulate turing machine&lt;/strong&gt;, i.e. the all one needs to do is to build a turing machine (seemingly simple architecture) that can implement other turing machines (manage tape, tape-head, cell and action table).&lt;/p&gt;

&lt;p&gt;So analogously speaking, the current computers are an attempt to implement this universal turing machine (UTM), where the &lt;strong&gt;generic action table of the UTM is implemented as CPU&lt;/strong&gt;, the &lt;strong&gt;the simulated action table of turing machine to be implemented is the Instruction memory&lt;/strong&gt;, the &lt;strong&gt;working area or the UTM on the tape is the data memory&lt;/strong&gt;, and the &lt;strong&gt;simulated state register of the implemented turing machine is the program counter (PC)&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;elements-of-computers&quot;&gt;Elements of Computers&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Memory (array of bytes), contains
    &lt;ul&gt;
      &lt;li&gt;program, which is a set of instructions&lt;/li&gt;
      &lt;li&gt;program data, i.e. variables, constants etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Program Counter (PC)
    &lt;ul&gt;
      &lt;li&gt;points to an instruction the program&lt;/li&gt;
      &lt;li&gt;after execution of one instruction it points to the next one&lt;/li&gt;
      &lt;li&gt;branch instructions make PC jump to another instruction (not in sequence)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CPU contains
    &lt;ul&gt;
      &lt;li&gt;program counter&lt;/li&gt;
      &lt;li&gt;instruction execution unit&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;single-instruction-isa&quot;&gt;Single Instruction ISA&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;sbn - subtract and branch if negative&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This basically leads to the following psuedocode&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sbn(a, b, line_no):
    a = a-b
    if (a&amp;lt;0):
        goto line_no
    else:
        goto next_statement
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Addition using SBN&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;intialize
    temp = 0
1: sbn temp, b, 2
exit: exit
2: sbn a, temp, exit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Add 1-10 using SBN&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;initialize
    one = 1
    index = 10
    sum = 0

1: sbn temp, temp, 2    \\ sets temp = 0
2: sbn temp, index, 3   \\ sets temp = -index
3: sbn sum, temp, 4     \\ sets sum += index
4: sbn index, one, exit \\ sets index -= 1
5: sbn temp, temp, 6    \\ sets temp = 0
6: sbn temp, one, 1     \\ the for loop, since 0 - 1 &amp;lt; 0
exit: exit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is similar to writing &lt;strong&gt;assembly level programs&lt;/strong&gt;, which are low level programs.&lt;/p&gt;

&lt;h3 id=&quot;mutliple-instruction-isas&quot;&gt;Mutliple Instruction ISAs&lt;/h3&gt;

&lt;p&gt;They typicall have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Arithmetic Instructions: Add, Subtract, Multiply, Divide&lt;/li&gt;
  &lt;li&gt;Logical Instructions: And, Or, Not&lt;/li&gt;
  &lt;li&gt;Move Instructions: Transfer between memory locations&lt;/li&gt;
  &lt;li&gt;Branch Instructions: Jump to new memory locations based on program instructions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;design-of-practical-machines&quot;&gt;Design of Practical Machines&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;While Harvard Machine has seperate data and instruction memories, Von-Neumann Machine has a single memory to serve both the purposes.&lt;/li&gt;
  &lt;li&gt;The problems with these machines is that they assume memory to be one large array of bytes. In practice these are slower because as the size of the structure increases the speed of processing decreases. The possible solution of this lies in having several smaller array of name locations called &lt;strong&gt;registers&lt;/strong&gt; that can be used by instructions. Hence these smaller arrays are faster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CPU contains a set of registers which are named storage locations.&lt;/li&gt;
  &lt;li&gt;values are loaded from memory to registers.&lt;/li&gt;
  &lt;li&gt;arithmetic an logical instructions use registers for input&lt;/li&gt;
  &lt;li&gt;finally, data is stored back in the memory.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Example program in machine language,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;r1 = mem[b] \\ load b
r2 = mem[c] \\ load c
r3 = r1 + r2 
mem[a] = r3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;r1, r2, r3 are registers&lt;/li&gt;
  &lt;li&gt;mem is the array of bytes representing memory&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result the modern day computers are similar to Von-Neumann Machines with the addition of register in the CPU.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://onlinecourses.nptel.ac.in/noc18_cs29/unit?unit=6&amp;amp;lesson=8&quot; target=&quot;_blank&quot;&gt;NPTEL: Introduction to Computer Architecture&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2018/07/25/introduction-to-computer-architecture/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2018/07/25/introduction-to-computer-architecture/</guid>
        
        <category>computer-science</category>
        
        <category>nptel</category>
        
        
        <category>nptel-computer-architecture</category>
        
      </item>
    
      <item>
        <title>Introduction to Survival Analysis</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Survival analysis refers to the set of statistical analyses that are used to analyze the length of time until an event of interest occurs. These methods have been traditionally used in analysing the survival times of patients and hence the name. But they also have a utility in a lot of different application including but not limited to analysis of the time of recidivism, failure of equipments, survival time of patients etc. Hence, simply put the phrase &lt;strong&gt;survival time&lt;/strong&gt; is used to refer to the type of variable of interest. It is often also referred by names such as &lt;strong&gt;failure time&lt;/strong&gt; and &lt;strong&gt;waiting time&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Such studies generally work with &lt;strong&gt;data leading upto an event of interest&lt;/strong&gt; along with several other characteristics of individual data points that may be used to explain the survival times statistically.&lt;/p&gt;

&lt;p&gt;The statistical problem (survival analysis) is to construct and estimate an appropriate model of the time of event occurance. A survival model fulfills the following expectations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;yield predictions of number of individuals who will fail (undergo the event of interest) at any length of time since the beginning of observation (or other decided point in time).&lt;/li&gt;
  &lt;li&gt;estimate the effect of observable individual characteristics on the survival time (to check the relevance of one variable holding constant all others).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is often observed that the survival models such as proportional hazard model are capable of &lt;strong&gt;explaining the survival times in terms of observed characteristics&lt;/strong&gt; which is better than straight-forward statistical inferences such as &lt;strong&gt;rates of event occurence without considering characteristic features&lt;/strong&gt; of data.&lt;/p&gt;

&lt;h3 id=&quot;basics&quot;&gt;Basics&lt;/h3&gt;

&lt;p&gt;Assume &lt;strong&gt;survival time T is a random variable&lt;/strong&gt; following some distribution &lt;strong&gt;characterized by cumulative distribution function \(F(t, \theta)\)&lt;/strong&gt;, where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\theta\) is the set of &lt;strong&gt;parameters to be estimated&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;\(F(t, \theta) = P(T \leq t) = \) &lt;strong&gt;probability that there is a failure&lt;/strong&gt; at or before time \(t\), for any \(t \geq 0\)&lt;/li&gt;
  &lt;li&gt;\(F(t, \theta) \to 1\) as \(t \to \infty\), since \(F(t, \theta)\) is a &lt;strong&gt;cumulative distribution function&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Above tendency leads to an &lt;strong&gt;implicit assumption that all candidates would eventually fail&lt;/strong&gt;. While this assumptions works selectively based on settings (true for patient survival times, not true for time of repayment of loans) and hence needs to be relaxed where it does not hold true.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Survival times are non-negative&lt;/strong&gt; by definition and hence the distributions (like exponential, Weibull, gamma, lognormal etc.) characterising it are defined for value of time \(t\) from \(0\) to \(\infty\).&lt;/p&gt;

&lt;p&gt;Let \(f(t, \theta)\) be the &lt;strong&gt;density function&lt;/strong&gt; correponding to the distribution function \(F(t, \theta)\), then the &lt;strong&gt;survival function&lt;/strong&gt; is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t, \theta) = 1 - F(t, \theta) = P(T \gt t) \tag{1} \label{1}&lt;/script&gt;

&lt;p&gt;which gives the &lt;strong&gt;probability of survival&lt;/strong&gt; until time \(t\) (\(S(t, \theta) \to 0\) as \(t \to \infty\) because, \(F(t, \theta) \to 1\) as \(t \to \infty\)).&lt;/p&gt;

&lt;p&gt;Another useful concept in survival analysis is called &lt;strong&gt;hazard rate&lt;/strong&gt;, defined by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(t, \theta) = \frac{f(t, \theta)} {1-F(t, \theta)} = \frac{f(t, \theta)} {S(t, \theta)} \tag{2} \label{2}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hazard rate represents the density of a failure at time \(t\), conditional on no failure prior to time \(t\), i.e., it indicates the probability of failure in the next unit of time, given that no failure has occured yet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;While \(f(t, \theta)\) roughly represents the proportion of original cohort that should be expected to fail between time \(t\) and \(t+1\), hazard rate \(h(t, \theta)\) represents the proportion of survivors until time \(t\) that should be expected to fail in the same time window, \(t\) to \(t+1\).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The relationship betwee the cumulative distribution function and the hazard rate is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(t, \theta) = 1 - exp \left[ - \int_0^t h(x, \theta) dx \right] \tag{3} \label{3}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(t, \theta) = - \frac {d\,ln\,[1 - F(t, \theta)]} {dt} \tag{4} \label{4}&lt;/script&gt;

&lt;p&gt;The fact that \(F(t, \theta)\) is a cdf puts some restrictions on the hazard rate,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hazard rate is non-negative function&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(t, \theta) = \int_0^t h(x, \theta) dx \tag{5} \label{5}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;the integrated hazard in \eqref{5} is finite for finite \(t\) and tends to \(\infty\) as \(t\) approaches \(\infty\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;state-dependence&quot;&gt;State Dependence&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Positive state dependence or an increasing hazard rate \(dh(t)/dt \gt 0 \) indicates that the &lt;strong&gt;probability of failure during the next time unit increases&lt;/strong&gt; as the length of time at risk increases.&lt;/li&gt;
  &lt;li&gt;Negative state dependence or a decreasing hazard rate \(dh(t)/dt \lt 0 \) indicates that the &lt;strong&gt;probability of failure in the next time unit decreases&lt;/strong&gt; as the length of time at risk decreases.&lt;/li&gt;
  &lt;li&gt;No state dependence indicates a &lt;strong&gt;constant hazard rate&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Only exponential distribution displays no state dependence.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;censoring-and-truncation&quot;&gt;Censoring and Truncation&lt;/h3&gt;

&lt;p&gt;A common feature of data on survival times is that they are censored or truncated. Censoring and truncation are statistical terms that refer to the &lt;strong&gt;inability to observe the variable of interest for the entire population&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A standard example to understand this can be understood in the form of a case of an individual shooting at a round target with a rifle and the variable of interest is the distance by which the bullet misses the center of the target.&lt;/li&gt;
  &lt;li&gt;If all shots hit the target, this distance can be measure for all the shots and there is no problem of censoring or truncation.&lt;/li&gt;
  &lt;li&gt;If some shots miss the target, but we know the number of shots fired, &lt;strong&gt;the sample is censored&lt;/strong&gt;. In this case either the distance of shot from center is known or it is known that it was atleast as large as the radius of the target.&lt;/li&gt;
  &lt;li&gt;Similarly if one does not know how many shots were fired but only have information about distance for shots that hit the target, &lt;strong&gt;the sample is truncated&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Censored sample has more information than a truncated sample.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Survival times are often censored because not all candidates would fail by the end of time during which the data was collected. This &lt;strong&gt;censoring of data must be taken into account&lt;/strong&gt; while making the estimations because it is &lt;strong&gt;not legitimate to drop such observations&lt;/strong&gt; with unobserved survival times &lt;strong&gt;r to set survival times for these observations equal to the length of the follow-up period&lt;/strong&gt; (when the data was collected).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Infrequently so, but there is also a chance of getting information about a candidate during a follow-up collection who was not a part of the original population. In such cases the &lt;strong&gt;survival time is truncated&lt;/strong&gt; because there is no information of the candidate or his survival time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problem-of-estimation&quot;&gt;Problem of Estimation&lt;/h3&gt;

&lt;p&gt;The initial assumption specifies a cumulative distribution function \(F(t, \theta)\), or equivalently a density \(f(t, \theta)\) or hazard \(h(t, \theta)\) that is of a known form except that it depends on a unknown parameter \(\theta\). Estimation of this parameter is first step for the model to make any meaningful prediction about the survival time of new candidate&lt;/p&gt;

&lt;p&gt;Consider a case of estimation of parameter for a censored sample which is defined as follows,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sample has \(N\) individuals with follow-up periods \(T_1, T_2, \cdots, T_N\). These follow-ups may be all equal, but they usually are not.&lt;/li&gt;
  &lt;li&gt;\(n\) is number of individuals who fail, numbered \(1, 2, \cdots, n\) and individuals numbered \(n+1, n+2, \cdots, N\) are the non-failures.&lt;/li&gt;
  &lt;li&gt;for the candidates who fail, there exists a survival time \(t_i \leq T_i, \, i \in [1, n]\)&lt;/li&gt;
  &lt;li&gt;for the non-failures, survival time \(t_i\) is not observed but it is known that it is greater than the length of the follow-up period \(T_i\), \(i \in [n+1, N]\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If it is assumed that &lt;strong&gt;all the outcomes are independent&lt;/strong&gt; of each other the likelyhood function of the sample is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \prod_{i=1}^n f(t_i, \theta) \prod_{i=n+1}^N S(T_i, \theta) \tag{6} \label{6}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Likelyhood function is a general statistical tool that expresses the probability of outcomes observed in terms of unknown parameters that are to be estimated, i.e., it is function of the parameters to be estimated, which serves as a measure of how likely it is that the statistical model, with a given parameter value, would generate the given data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A common used estimator of \(\theta\) is the &lt;strong&gt;Maximum Likelyhood Estimator (MLE)&lt;/strong&gt; which is defined as the value of \(\theta\) that maximizes the likelyhood function.&lt;/p&gt;

&lt;p&gt;The MLE have been shown to display the following desirable properties over a large sample (&lt;strong&gt;as the sample size approaches infinity&lt;/strong&gt;),&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Unbiased&lt;/li&gt;
  &lt;li&gt;Efficient&lt;/li&gt;
  &lt;li&gt;Normally Distributed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As mentioned, the properties of MLE are only &lt;strong&gt;relevant when the sample size is large&lt;/strong&gt;. It is often &lt;strong&gt;observed that the sample sizes in these studies are much smaller&lt;/strong&gt; and hence reliance on large sample properties of estimator is more tenuous.&lt;/p&gt;

&lt;p&gt;The above survival model uses observed survival time \(t_i\) while &lt;strong&gt;ignoring the specific timing of the observed returns&lt;/strong&gt;. So the analysis of the fact of failure or non-failure, ignoring the timing of observed failures, would properly be based on the likelyhood function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \prod_{i=1}^n F(T_i, \theta) \prod_{i=n+1}^N S(T_i, \theta) \tag{7} \label{7}&lt;/script&gt;

&lt;p&gt;Estimation using \eqref{7} is a legitimate procedure and does not cause any bias or inconsistency, but the estimates are inefficient relative to MLEs from \eqref{6}.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The estimates of \(\theta\) gotten by maximizing \eqref{7} will be less efficient (have larger variance) than the estimates of \(\theta\) gotten by maximizing \eqref{6}, atleast for large sample sizes. Hence if the information on time of return is available, it should be used.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If the &lt;strong&gt;truncated&lt;/strong&gt; case of sample is considered, then there is &lt;strong&gt;no information on all the individuals who do not fail&lt;/strong&gt;. Formally, one starts with a cohort of \(N\) candidates, where &lt;strong&gt;\(N\) is unknown&lt;/strong&gt;, and the only &lt;strong&gt;observations available are the survival times \(t_i\) for the \(n\) individuals who fail&lt;/strong&gt; before the end of follow-up period. The \(n\) individuals appear in sample because \(t_i \leq T_i\), and the appropriate density is therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(t_i, \theta \mid t_i \leq T_i) = \frac{f(t_i, \theta)}{P(t_i \leq T_i)} =  \frac{f(t_i, \theta)}{F(T_i, \theta)} \tag{8} \label{8}&lt;/script&gt;

&lt;p&gt;And the corresponding &lt;strong&gt;likelyhood function&lt;/strong&gt; which can be maximized to obtain the MLEs is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \prod_{i=1}^n f(t_i, \theta \mid t_i \leq T_i) = \prod_{i=1}^n \frac{f(t_i, \theta)}{F(T_i, \theta)} \tag{9} \label{9}&lt;/script&gt;

&lt;h3 id=&quot;explanatory-variables&quot;&gt;Explanatory Variables&lt;/h3&gt;

&lt;p&gt;Information on &lt;strong&gt;explanatory variables may or may not be used&lt;/strong&gt; in estimating survival time models. Some models that are based on the &lt;strong&gt;implicit assumption that distribution of survival time is the same for all individuals&lt;/strong&gt;, do not use explanatory variables.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But practically it is observed that some individuals are more prone to failing than others and hence if information on individual charestistics and environmental variables is available, it should be used.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This information can be incorporated is survival models by letting the parameter \(\theta\) depend on these individual characteristics and a new set of parameters. E.g. exponential model depends on a single parameter, say \(\theta\), and &lt;strong&gt;\(\theta\) can be assumed to depend on the individual characteristics&lt;/strong&gt; as in linear regression.&lt;/p&gt;

&lt;h3 id=&quot;non-parametric-hazard-rate-and-kaplan-meier&quot;&gt;Non-Parametric Hazard Rate and Kaplan Meier&lt;/h3&gt;

&lt;p&gt;Before beginning any formal analyses of the data, it is often instructive to check the hazard rate. For this purpose, the &lt;strong&gt;time until failure are rounded to the nearest quantized time unit&lt;/strong&gt; (month, week, day etc.). Following this it is easy to count the &lt;strong&gt;number of candidates at risk at the beginning of the said time period&lt;/strong&gt; (i.e. the number of individuals who have not yet failed or been censored at the beginining of the time unit) and the &lt;strong&gt;number of individuals who fail during the time period&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Then, the &lt;strong&gt;non-parametric hazard rate&lt;/strong&gt; can be estimated as the ratio of number of failures during the time period to the number of individuals at risk at the beginning of time period, i.e., if the number of individuals at risk at the beginning of time \(t \, (t = 1, 2, \cdots)\) is denoted by \(r\), and the number of individuals who fail during this time \(t\) is denoted by \(n_t\), then the estimated hazard for time \(t\), \(\hat{h}(t)\) is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{h}(t) = \frac{n_t}{r} \tag{10} \label{10}&lt;/script&gt;

&lt;p&gt;Such estimated hazard rates are prone to high variability. Also this high variability makes the purely non parametric estimates unattractive as they are less likely to give an accurate prediction on a new dataset. The parametric models such as exponential, Weibull or lognormal take care of this high variability and makes the model more tractable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But the plots of non parametric estimates of hazard rate provides a good initial guide as to which probability distribution may work well for a given usecase.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As noted earlier, the hazard function, density function, and distribution function are alternative but equivalent ways of characterizing the distribution of the time until failure. Hence, once the hazard rate is estimated, then implicitly so is the density and the distribution function. It is possible to solve explicitly for the estimated density of distribution function in terms of the estimated hazard function. The resulting estimator (called &lt;strong&gt;Kaplan Meier&lt;/strong&gt; or &lt;strong&gt;product limit&lt;/strong&gt; estimator in statistical literature which is nothing but the non-parametric estimate) of the distribution function is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{F}(t) = 1 - \prod_{j=1}^t [1 - \hat{h}(j)] \tag{11} \label{11}&lt;/script&gt;

&lt;h3 id=&quot;models-without-explanatory-variables&quot;&gt;Models without Explanatory Variables&lt;/h3&gt;

&lt;p&gt;There are various models that do not consider the explanatory variables, and instead &lt;strong&gt;assume some specific distribution&lt;/strong&gt; such as exponential, Weibull, or lognormal for the length of time until failure. Essentially, the distribution of time until failure is known, except for some &lt;strong&gt;unknown parameters that have to be estimated&lt;/strong&gt;. Hence, models of this type are called parametric models, which are different from the models discussed before as the later have no associated parameters or distribution.&lt;/p&gt;

&lt;p&gt;The unknown parameters are &lt;strong&gt;estimated by maximizing the likelyhood function&lt;/strong&gt; of the form \eqref{6}.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In case of exponential distribution, MLEs cannot be written in closed form (i.e. expressed algebraically), and so the maximization of likelyhood function is done numerically.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Once the characteristic parameters have been estimated, one can determine the following (which cannot be determined in case of non-parametric estimates like Kaplan Meier):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;mean time&lt;/strong&gt; until failures&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;proportion of population that should be expected to fail&lt;/strong&gt; within any arbitrary period of time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the &lt;strong&gt;advantage of such models lies in the smoothness of predictions&lt;/strong&gt;, the &lt;strong&gt;disadvatage is the fact that it can be wrong and inturn lead to statements that are systematically misleading&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exponential Distribution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The exponential distribution has density,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(t) = \theta \, e^{-\theta t} \tag{12} \label{12}&lt;/script&gt;

&lt;p&gt;and &lt;strong&gt;survivor function&lt;/strong&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = e^{-\theta t} \tag{13} \label{13}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the parameter is constrained, \(\theta \gt 0\)&lt;/li&gt;
  &lt;li&gt;mean: \(1 / \theta\) and variance: \(1 / \theta^2\)&lt;/li&gt;
  &lt;li&gt;only distribution with a &lt;strong&gt;constant hazard rate&lt;/strong&gt;, specifically \(h(t) = \theta\) for all \(t \geq 0\)&lt;/li&gt;
  &lt;li&gt;such hazard rates are generally seen in some physical processes such as radioactive decay.&lt;/li&gt;
  &lt;li&gt;it is often not the most reasonable distribution for survival models.&lt;/li&gt;
  &lt;li&gt;exponential distribution requires estimation of single parameter \(\theta\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Consider a sample of \(N\) individuals, of which \(n\) have failed before the end of the follow-up period. The observed failure times be denoted by \(t_i\, (i=1, 2, \cdots, n)\) and the censoring times (length of follow up) for the non-failures de denoted by \(T_i\, (i = n+1, \cdots, N)\). Then the likelyhood function \eqref{6} can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \prod_{i=1}^n \theta\,e^{-\theta t_i} \prod_{i=n+1}^N e^{-\theta T_i} \tag{14} \label{14}&lt;/script&gt;

&lt;p&gt;Maximizing \eqref{14} w.r.t. \(\theta\) yields MLE in closed form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta} = \frac {n} {\sum_{i=1}^n t_i + \sum_{i=n+1}^N T_i} \tag{15} \label{15}&lt;/script&gt;

&lt;p&gt;For large samples \(\hat{\theta}\) is normal with mean \(\theta\) and variance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\theta^2}{\sum_{i=1}^N [1 - exp(-\theta T_i)]} \tag{16} \label{16}&lt;/script&gt;

&lt;p&gt;which for large \(N\) is adequately approximated by \(\theta^2/n\).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Exponential distribution is highly skewed.&lt;/li&gt;
  &lt;li&gt;Mean may not be a good measure of central tendency for exponential distribution.&lt;/li&gt;
  &lt;li&gt;Median may be more preferrable indicator in most cases.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Logarithm of likelyhood or log-likelyhood is used as a value to measure the goodness of fit. A higher value (more positive or less negative) for this variable indicates that the model fits the data better.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Weibull Distribution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In statistical literature, a very common alternative to the exponential distribution is the Weibull distribution. It is a generalization of the exponential distribution. By using Weibull distribution one can test to check if a simpler exponential model is more appropriate.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A variable \(T\) has Weibull distribution if \(T^{\tau}\) has an exponential distribution for some value of \(\tau\).&lt;/li&gt;
  &lt;li&gt;increasing hazard rate if \(\tau \gt 1\) and decreasing hazard rate if \(\tau \lt 1\). Also, if \(\tau = 1\) the hazard rate is constant and the Weibull distribution reduces to the exponential.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Weibull distribution has a monotonic hazard rate&lt;/strong&gt;, i.e it can be increasing, constant or decreasing but it cannot be increasing at first and then decreasing after some point.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The density of Weibull distribution is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(t) = \tau \theta^{\tau} \, t^{\tau -1} e^{-(\theta t)^\tau} \tag{17} \label{17}&lt;/script&gt;

&lt;p&gt;and the survivor function is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S(t) = e^{-(\theta t)^\tau} \tag{18} \label{18}&lt;/script&gt;

&lt;p&gt;The likelyhood function for Weibull distribution can be derived by substituting \eqref{17} and \eqref{18} in \eqref{6}.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lognormal Distribution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If \(z\) is distributed as \(N(\mu, \sigma^2)\), then \(y = e^z\) has a lognormal distribution with mean&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi = exp(\mu + {1 \over 2} \sigma^2) \tag{19} \label{19}&lt;/script&gt;

&lt;p&gt;and variance,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tau^2 = exp(2 \mu + \sigma^2) [exp(\sigma^2) -1] = \phi^2 \psi^2 \tag{20} \label{20}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi^2 = exp(\sigma^2) - 1 \tag{21} \label{21}&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;density&lt;/strong&gt; of \(z = ln \, y\) is the density of \(N(\mu, \sigma^2)\) given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(ln \, y) = (1 / \sqrt{2\pi} \sigma) exp [-(1/2 \sigma^2) (ln\, y - \mu)^2] \tag{22} \label{22}&lt;/script&gt;

&lt;p&gt;Generally there is &lt;strong&gt;no advantage to working with the density of \(y\) itself, rather than \(ln \, y\)&lt;/strong&gt;. Thus, one can simply assume that log of survival time is distributed normally, and hence the likelyhood function \eqref{6} becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
L = &amp;- {n \over 2} ln(2\pi) - {n \over 2} ln(\sigma^2) - {1 \over 2\sigma^2} \sum_{i=1}^n (ln\, t_i - \mu)^2 \\
&amp;+ \sum_{i=n+1}^N ln \, F \left[ \frac {\mu - ln\, T_i} {\sigma} \right]
\end{align}
\tag{23} \label{23} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;where &lt;strong&gt;\(F\) is the cumulative distribution function&lt;/strong&gt; for \(N(0, 1)\) distribution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No analytical solution&lt;/strong&gt; exists for the maximization of \eqref{23} w.r.t. \(\mu\), and \(\sigma^2\), so it &lt;strong&gt;must be maximized numerically&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;the hazard function for lognormal distribution is complicated; it &lt;strong&gt;increases first and then decreases&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Other distributions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Although exponential, Weibull and lognormal are among the three most used distributions, there are various other well-known probability distributions possible, such as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;log-logistic&lt;/li&gt;
  &lt;li&gt;LaGuerre&lt;/li&gt;
  &lt;li&gt;distributions based on Box-Cox power transformation of the normal&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are various ways of measuring how well models fit the data:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;value of likelyhood (or log-likelyhood) function&lt;/li&gt;
  &lt;li&gt;maximum difference between the fitted value and actual cumulative distribution function&lt;/li&gt;
  &lt;li&gt;standard Kolmogorov-Smirnov test of goodness of fit&lt;/li&gt;
  &lt;li&gt;chi-square goodness-of-fit statistic based on predicted and actual failure times.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Over time it has been observed that even though some of these parametric distributions &lt;strong&gt;might fit the data&lt;/strong&gt; better than others and excel on various metrics of good fit of data, these &lt;strong&gt;do not give any explaination about the reasons governing the distribution&lt;/strong&gt; or any &lt;strong&gt;insight into the affecting parameters&lt;/strong&gt; that lead to the different survival times in a population. Hence, these parametric models without the explanatory variables are not considered to be an effective tool for analysis.&lt;/p&gt;

&lt;h3 id=&quot;models-with-explanatory-variables&quot;&gt;Models with Explanatory Variables&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Explanatory variables are in general added to survival models in an attempt to make more accurate predictions: the practical experiments over time corroborate the fact that individual characteristics, previous experiences and environmental setup helps predict whether or not a person will fail.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;An analysis of survival time without using the explanatory variables amounts to an analysis of its &lt;strong&gt;marginal distribution&lt;/strong&gt;, whereas an analysis using explanatory variable amounts to an analysis of the &lt;strong&gt;distribution of survival time conditional on these variables&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Variance of the conditional distribution is less than the variance of the marginal distribution, i.e. expect more precise distribution from former.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Another more fundamental reason may include the interest of understanding the effect of explanatory variables on the survival time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More generally, these variables might be the demographics or environmental characteristics.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proportional-hazards-model&quot;&gt;Proportional Hazards Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;allows one to estimate the effects of individual characteristics on survival time without having to assume a specific parametric form of distribution of time until failure.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For an individual with the vector of characteristics, \(x\), the proportional hazards model assumes a hazard rate of the form,&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(t \mid x) = h_0(t) e^{x_i^\prime \beta} \tag{24} \label{24}&lt;/script&gt;

&lt;p&gt;where \(h_0(t)\) is completely arbitrary and unspecified baseline hazard function. &lt;strong&gt;Thus, the model assumes that the hazard functions of all individuals differ only by a factor of proportionality,&lt;/strong&gt; i.e. if an individuals hazard rate is 10 times higher than another’s at a given point of time, then it must be 10 times higher at all points in time. &lt;strong&gt;Each hazard function follows same pattern over time.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;However, there is no restriction on what this pattern can be, i.e. it puts no restriction on the \(h_0(t)\) curve, which determines the shape of \(h(t \vert x)\) curve. &lt;strong&gt;\(\beta\) can be estimated without specifying \(h_0(t)\), and \(h_0(t)\) can be estimated non-parametrically and thus with flexibility.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Consider a sample of \(N\) individuals, \(n\) of whom fail before the end of their follow-up period. Let the observations be ordered such that individual 1 has the shortest failure time, individual 2 has the second shortest failure time, and so forth. Thus, for individual \(i\), failure time \(t_i\) is observed, with,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;t_1 \lt t_2 \lt \cdots \lt t_n \tag{25} \label{25}&lt;/script&gt;

&lt;p&gt;A vector \(x_i\) represents individual characteristics for each individual \(i = 1, 2, \cdots, N\), irrespective of whether they failed.&lt;/p&gt;

&lt;p&gt;For each observed failure times, \(t_i\), \(R(t_i)\) is defined as set of all individuals who were at risk just prior to time \(t_i\), i.e., it includes the individuals with failure times greater than or equal to \(t_i\), as well as the individuals whose follow-up is at least of length \(t_i\).&lt;/p&gt;

&lt;p&gt;Using these definitions, the &lt;strong&gt;partial-likelihood&lt;/strong&gt; function proposed by Cox can be defined for any failure time \(t_i\), as the probability that it is individual \(i\) who fails, given that exactly one individual from set \(R(t_i\)) fails, is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac {h(t_i \vert x_i)} {\sum_{j \in R(t_i)} h(t_i \vert x_j)} = \frac {exp(x_i^\prime \beta)} {\sum_{j \in R(t_i)} exp(x_j^\prime \beta)} \tag{26} \label{26}&lt;/script&gt;

&lt;p&gt;The partial-likelyhood function is formed by multiplying \eqref{26} over all \(n\) failure times,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \prod_{i=1}^n \frac {exp(x_i^\prime \beta)} {\sum_{j \in R(t_i)} exp(x_j^\prime \beta)} \tag{27} \label{27}&lt;/script&gt;

&lt;p&gt;The estimate of \(\beta\) by maximizing \eqref{27} numerically w.r.t \(\beta\) is the &lt;strong&gt;partial maximum-likelyhood estimate&lt;/strong&gt;. The word &lt;strong&gt;partial&lt;/strong&gt; in partial likelyhood refers to the fact that not all available information is used in estimating \(\beta\), i.e., it only depends on knowing which individuals were at risk when each observed failure occured. The exact numerical values of the failure times \(t_i\) or of the censoring times for the non recedivists are not needed; only their &lt;strong&gt;order matters&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Once \(\beta\) is estimated, \(h_0(t)\), the baseline hazard function can be estimated non-parametrically. The estimated baseline hazard function is constant over the intervals between failure times. One can also calculate &lt;strong&gt;survivor function&lt;/strong&gt; \(S_0(t)\) or equivalently the baseline cumulative distribution function \(F_0(t)\), that corresponds to the estimated baseline hazard function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The estimated survivor function is a step function that falls at each time at which there is a failure.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The point of proportional hazard model is that the survivor function is estimated non-parametrically (i.e. not imposing any structure on its pattern over time, except that it must decrease as \(t\) increases) and estimation of \(\beta\) can proceed seperately from estimation of survivor function.&lt;/p&gt;

&lt;h3 id=&quot;split-population-models&quot;&gt;Split Population Models&lt;/h3&gt;

&lt;p&gt;The models considered so far assume some cumulative distribution function, \(F(t)\) for the survival time, that gives the probability of a failure upto and including time \(t\), and it approaches one as \(t\) approaches infinity. This basically means that every individual must eventually fail, if they were observed for long enough time. This assumption is not true in all cases.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Split Population Models&lt;/strong&gt; (or split models) do not imply that every individual would eventually fail. Rather the population is divided into two groups, one of which would never fail.&lt;/p&gt;

&lt;p&gt;Mathematically, let \(Y\) be an observable indicator with two values, one implying ultimate failure and zero implying perpetual success. Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
P(Y=1) &amp;= \delta \\
P(Y=0) &amp;= 1 - \delta
\end{align}
\tag{28} \label{28} %]]&gt;&lt;/script&gt;

&lt;p&gt;where \(\delta\) is the proportion of the population that would eventually fail, and \(1 - \delta\) is the proportion that would never fail.&lt;/p&gt;

&lt;p&gt;Let \(g(t \vert Y=1)\) be density of survival times for the ultimate failures, and \(G(t \vert Y=1)\) be the corresponding cumulative distribution function. If one considers exponential model to represent them, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
g(t \vert Y=1) = \theta e^{-\theta t} \\
G(t \vert Y=1) = 1 - e^{-\theta t}
\end{align}
\tag{29} \label{29}&lt;/script&gt;

&lt;p&gt;It can also be noted that \(g (t \vert Y = 0)\) and \(G(t \vert Y=0)\) are not defined.&lt;/p&gt;

&lt;p&gt;Let \(T\) be the length of the follow up period and let \(R\) be an observable indicator equal to one if there is failure by time \(T\) and zero if there is not. The probability for individuals who do not fail during the follow up period, i.e, the event of \(R = 0\) is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
P(R=0) &amp;= P(Y=0) + P(Y=1)P(t \gt T \vert Y=1) \\
&amp;= 1 - \delta + \delta e^{-\theta T}
\end{align}

\tag{30} \label{30} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similarly, probability density for people who fail with survival time \(t\) is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y=1)P(t \lt T \vert Y=1) g(t \vert t \lt T, Y=1) = P(Y=1) g(t \vert Y=1) = \delta \theta e^{-\theta t} \tag{31} \label{31}&lt;/script&gt;

&lt;p&gt;So the likelyhood function is made up of \eqref{29} for those who do not fail and \eqref{30} for those who do. It is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \prod_{i=1}^n \delta \theta exp(-\theta t_i) \prod_{i = n+1}^N (1 - \delta + \delta exp(-\theta T_i)) \tag{32} \label{32}&lt;/script&gt;

&lt;p&gt;The maximum likelyhood estimate of both \(\theta\) and \(\delta\) can be obtained by maximizing \eqref{32} numerically. It can be noted that when \(\delta = 1\), \eqref{32} reduces to \eqref{14}, the original exponential survival time model.&lt;/p&gt;

&lt;p&gt;The split population model can be seen as a model of two seperate subpopulations, one with hazard rate \(\theta\) and other with zero. A more generalized model exists where the subpopulations exist with two non-zero hazard rates namely, \(\theta_1\) and \(\theta_2\). Such models help to account for population that is heterogenous in nature.&lt;/p&gt;

&lt;p&gt;Split models can also be based on other distributions such as lognormal etc. Also, it is possible to include explanatory variables into a split model. In such cases, the explanatory variables maybe taken to affect the probabiliy of failure, \(\delta\) or distribution of time until failure.&lt;/p&gt;

&lt;p&gt;For example, for a given feature vector \(x_i\) of explanatory variables, using &lt;strong&gt;logit/individual lognormal model&lt;/strong&gt;, \(\delta\) is modeled using,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_i = 1/(1+exp(x_i^\prime \alpha)) \tag{33} \label{33}&lt;/script&gt;

&lt;p&gt;and parameter \(\mu\) of the lognormal distribution is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_i = x_i^\prime \beta \tag{34} \label{34}&lt;/script&gt;

&lt;p&gt;Here, the parameter \(\alpha\) gives the effect of \(x_i\) on the probablity of failure, and \(\beta\) gives the effect of \(x_i\) on the time until failure.&lt;/p&gt;

&lt;p&gt;Such models are of importance because they let one distinguish between effects of explanatory variable on probability of eventual failure from effects on time until failure who eventually do fail.&lt;/p&gt;

&lt;h3 id=&quot;heterogeneity-and-state-dependence&quot;&gt;Heterogeneity and State Dependence&lt;/h3&gt;

&lt;p&gt;The two major causes of observed declining hazard rates are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;state dependence&lt;/li&gt;
  &lt;li&gt;heterogeneity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The phenomenon of an actually decreasing hazard rate over time due to an actual change in behavior over time at individual level is referred to as &lt;strong&gt;state dependence&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The second possible reason is &lt;strong&gt;heterogeneity&lt;/strong&gt;. This basically means that the hazard rates are different across individuals, i.e., some individuals are more prone to failure than others. Naturally, individuals with higher hazard rates tend to fail earlier, on average, than individuals with lower hazard rates. As a result the average hazard rate of the surviving group will decrease with length of time simply because the most failure prone individuals have been removed already. This is true even without state dependence, i.e, each individual has a constant hazard rate but hazard rate varies across individuals. Even such a group would display decreasing hazard rate.&lt;/p&gt;

&lt;p&gt;It is important to understand the difference because a decrease in a hazard rate due to state dependance means a success of the underlying program, while decrease due to heterogeneity does not imply that the program is effective in preventing failure, because it is happening by the virtue of the data at hand.&lt;/p&gt;

&lt;h3 id=&quot;time-varying-covariates&quot;&gt;Time Varying Covariates&lt;/h3&gt;

&lt;p&gt;Until now explanatory variables affecting the time until failure do not potray changing values over time, but is a possibility that can not be denied.&lt;/p&gt;

&lt;p&gt;The types of explanatory variables can be categorizaed as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;variables that do not change over time, e.g race, sex etc.&lt;/li&gt;
  &lt;li&gt;variables that change over time but not within a single follow-up period, e.g. number of times followed up etc.&lt;/li&gt;
  &lt;li&gt;variables that change continuously over time, such as age, education etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The last type of variables make it reasonable to use a statistical model that allows covariates to vary over time. Such incorporation is relatively straightforward in hazard-based models such as proportional hazard models. At each point in time, hazard rate is determined by the values of explanatory variables at that time.&lt;/p&gt;

&lt;p&gt;However, it is much more difficult to introduce time-varying components into parametric models because these models are parameterized in terms of density and cumulative distribution function, and the density of distribution function at time \(t\) depends on the whole history of the explanatory variables up to time \(t\). &lt;strong&gt;In the presence of time varying covariates, a parameterization of the hazard rate would be much more convenient.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Panel or Longitudinal Data:&lt;/strong&gt; data on individuals over time without reference to just a single follow-up. Such datasets include a large number of time-varying explanatory variables.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://link.springer.com/article/10.1007/BF01083132#&quot; target=&quot;_blank&quot;&gt;Survival Analysis: A Survey&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Jul 2018 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2018/07/23/survival-analysis/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2018/07/23/survival-analysis/</guid>
        
        <category>machine-learning</category>
        
        <category>papers</category>
        
        <category>featured</category>
        
        
      </item>
    
      <item>
        <title>Google Smart Reply</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Smart reply is an end to end method for automatically generating &lt;strong&gt;short yet semantically diverse&lt;/strong&gt; email repsonses. The feature also depends on some novel methods for &lt;strong&gt;semantic clustering of user-generated content&lt;/strong&gt; that requires minimal amount of explicitly labeled data.&lt;/p&gt;

&lt;p&gt;Google reveals that around 25% of the email responses are 20 tokens or less in length. The high frequency of short replies was the major motivation behind developing an automated reply assist feature. The system exploits concepts of machine learning such as fully-connected neural networks, LSTMs etch.&lt;/p&gt;

&lt;p&gt;Major challenges that have been addressed in building this features includes the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;High &lt;strong&gt;repsonse quality&lt;/strong&gt; in terms of language and content.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Utility&lt;/strong&gt; maintained by presenting a variety of responses.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalable architecture&lt;/strong&gt; to serve millions of emails google handles without significant latencies.&lt;/li&gt;
  &lt;li&gt;Maintaining &lt;strong&gt;privacy&lt;/strong&gt; by ensuring that no personal data is leaked while generating training data. &lt;strong&gt;Only aggregate statistics are inspected.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;smart-reply&quot;&gt;Smart Reply&lt;/h3&gt;

&lt;p&gt;Smart reply consists of the following components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Response Selection: An LSTM network processes the incoming messages and produces the most likely responses. &lt;strong&gt;To improve scalability and increase speed of processing, only approximate best responses are found.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Response Set Generation: In order to maintain high quality, the responses are selected from a response state &lt;strong&gt;generated offline using semi-supervised graph learning approach&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-07-22-google-smart-reply/fig-1-smart-reply.png?raw=true&quot; alt=&quot;Fig-1: Lifecycle of a message&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Diversity: After generating the most likely responses, a smaller set of responses are chosen among them to &lt;strong&gt;maximize the utility which requires enforcing diverse semantic intents&lt;/strong&gt; among the presented options.&lt;/li&gt;
  &lt;li&gt;Triggering Model: A feedforward neural network decides whether or not to suggest responses, which further improves the utility by not showing suggestions when they are unlikely to be used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;The entire application of smart reply can be basically broken down into two core tasks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;predicting responses&lt;/li&gt;
  &lt;li&gt;identifying a target response space&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the task of finding the apt response has be attempted before, it has never been applied to a production environment at such a scale. It is this widespread use of the application that requires it to deliver high quality responses at all the instances. This is achieved by choosing the responses from a set of pre-identified response space.&lt;/p&gt;

&lt;p&gt;Which leads to the second core task of identifying the target response space. This is achieved by using an algorithm called &lt;strong&gt;Expander Graph Learning Approach&lt;/strong&gt;. It is used because it scales well to really large datasets and large output sizes. Generally used for knowledge expansion and classification tasks, smart reply is the first attempt to use it for semantic intent clustering.&lt;/p&gt;

&lt;h3 id=&quot;selecting-responses&quot;&gt;Selecting Responses&lt;/h3&gt;

&lt;p&gt;The fundamental aim of smart reply is to find the most likely response given an original message text. i.e. given an original message \(o\) and the set of all possible responses \(R\), find,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r^* = argmax_{r \in R} P(r|o) \tag{1} \label{1}&lt;/script&gt;

&lt;p&gt;In order to acheive this a model is built to score the responses and then response with the highest score is picked.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LSTM Model&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Since a sequence of tokens \(r\) is being scored conditional on another sequence of characters \(o\), the task is a natural fit for &lt;strong&gt;sequence to sequence learning&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Input to the model is the original message \(\{o_1, o_2, \cdots o_n\}\)&lt;/li&gt;
  &lt;li&gt;The output is the conditional probability distribution of sequence of response tokens given the input:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(r_1, r_2, \cdots, r_m | o_1, o_2, \cdots, o_n) \tag{2} \label{2}&lt;/script&gt;

&lt;p&gt;The distribution in \eqref{2} can be further factorized as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(r_1, \cdots, r_m | o_1, \cdots, o_n) = \prod_{i=1}^m P(r_i|o_1, \cdots, o_n, r_1, \cdots, r_{i-1}) \tag{3} \label{3}&lt;/script&gt;

&lt;p&gt;In practice, the sequence of original message is fed to the LSTM, which then encodes the entire message in a vector representation. Then given this state, a softmax output is computed, which is interpretted as \(P(r_1|o_1, \cdots, o_n)\)(probability distribution of the first response token).&lt;/p&gt;

&lt;p&gt;Similarly, as the response tokens are fed in, softmax at each timestep \(t\) is interpretted as \(P(r_t|o_1, \cdots, o_n, r_1, \cdots, r_{t-1})\)&lt;/p&gt;

&lt;p&gt;Using the factorization in \eqref{3}, these softmax scores can be used to compute \(P(r_1, r_2, \cdots, r_m | o_1, o_2, \cdots, o_n)\).&lt;/p&gt;

&lt;p&gt;Training involves the following points:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;maximize the log probability of observed responses, given their respective original messages, i.e.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{(o, r)} log \, P(r_1, \cdots, r_m | o_1, \cdots, o_n) \tag{4} \label{4}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;train using stochastic gradient descent using AdaGrad.&lt;/li&gt;
  &lt;li&gt;training is done on a distributed system because of the size of the dataset.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;recurrenct projection layer&lt;/strong&gt; helped improve quality and time of convergence.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;gradient clipping&lt;/strong&gt; helps stabalize training.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;: At the time of inference one can feed in the original message and then use the output of the softmaxes to get a probability distribution over the vocabulary at each timestep. These can be used in a variety of ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;to draw a random sample from the response distribution. This is done by sampling one token at each timestep to feed it back into the model.&lt;/li&gt;
  &lt;li&gt;to approximate the most likely response given the original message. This can be done greedily by taking most likely token at each timestep and feeding it back in. A less greedy strategy is to use &lt;strong&gt;beam search&lt;/strong&gt;, i.e. take the top \(b\) tokens and feed them in, then retain the best \(b\) response prefixes and repeat.&lt;/li&gt;
  &lt;li&gt;to determine the likelyhood of a specific response candidate. Done by feeding each token of the candidate and using softmax output to get the likelyhood of next candidate token.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;challenges&quot;&gt;Challenges&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Response Quality&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In order to surface responses to the users, responses must be always high quality in terms of style, tone, diction, and content. Since the models are trained on real-world data, one has to account for the possibility where the most response is not necessarily a high quality response. Even the most frequent responses might not be appropriate to suggest to users because it could contain poor grammar, spelling or machanics (like &lt;em&gt;you’re the best!&lt;/em&gt;) or it could also convey a sense of familiarity that is likely to be offensive (like &lt;em&gt;thanks hon!&lt;/em&gt;) etc.&lt;/li&gt;
  &lt;li&gt;While restricting the vocabulary can take care of issues such as profanity or spell errors, it would not be sufficient in averting a politically incorrect statement that can be formed in a wide variety of ways.&lt;/li&gt;
  &lt;li&gt;Hence, smart reply uses a semi-supervised learning to build the target repsonse space \(R\) comprising of only high quality responses.&lt;/li&gt;
  &lt;li&gt;Hence the model described is used to choose the best response among \(R\), instead of best response from any sequence of words in the vocabulary.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Utility&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Suggestions are most useful when they are highly specific to the original message and express a diverse intent.&lt;/li&gt;
  &lt;li&gt;Generally the outputs from LSTM observed tend to (1) favor common but unspecific responses and (2) have little diversity.&lt;/li&gt;
  &lt;li&gt;Specificity of the responses is increased by penalizing the responses that are applicable to a broad range of incoming messages.&lt;/li&gt;
  &lt;li&gt;In order to increase the breadth of options presented to users, diversity is enforced by exploiting the semantic structure of \(R\).&lt;/li&gt;
  &lt;li&gt;Utility of responses is also boosted by passing the incoming message first through a triggering model which decides whether or not it is appropriate for suggestions to pop up.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Scoring every candidate \(r \in R\) would require \(O(|R | l)\) LSTM steps where \(l\) is the length of the longest response.&lt;/li&gt;
  &lt;li&gt;This would mean a growing response time as the number of responses in \(R\) increases over time.&lt;/li&gt;
  &lt;li&gt;In general, an efficient algorithm for this purpose should not be a function of \(|R|\)&lt;/li&gt;
  &lt;li&gt;In order to achieve this, the responses among \(R\) are organized as a trie, followed by a left-to-right beam-search but retain only the hypotheses that appear in the trie.&lt;/li&gt;
  &lt;li&gt;This search process has a complexity of \(O(bl)\) where both \(b\) and \(l\) are in a range of 10-30, which greatly reduces the time it would take to generate the responses.&lt;/li&gt;
  &lt;li&gt;Although the search only approximates the best responses in \(R\), its results are very similar to what one would get by scoring and ranking all \(r \in R\), even for a small \(b\).&lt;/li&gt;
  &lt;li&gt;Also first pass through the triggering model, reduces the average time a message has to spend in LSTM computations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;response-set-generation&quot;&gt;Response Set Generation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The goal of this step is to generate a structured response set that effectively captures various intents conveyed by people in natural language conversations.&lt;/li&gt;
  &lt;li&gt;The target response space is required to capture both variablity in language and intents.&lt;/li&gt;
  &lt;li&gt;The results are used in two ways - (1) define a response space and (2) promote diversity among chosen suggestions.&lt;/li&gt;
  &lt;li&gt;Response set is constructed by aggregating the most frequently used sentences among the preprocessed data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Canonicalizing Email Responses&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Involves generating a set of canonicalized responses that capture the variability in language.&lt;/li&gt;
  &lt;li&gt;This is done by performing a dependency parse on all the sentences and then using the syntactic structure to generate a canonicalized representation.&lt;/li&gt;
  &lt;li&gt;Words, phrases that are modifiers or not attached to the head words are ignored.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Semantic Intent Clustering&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;partition the responses into semantic clusters where each cluster represents a meaningful response intent.&lt;/li&gt;
  &lt;li&gt;all the messages within a cluster share the same semantic meaning but may appear different in structure.&lt;/li&gt;
  &lt;li&gt;this helps digest the entire information present in frequent responses into a coherent set of semantic cluster&lt;/li&gt;
  &lt;li&gt;because of the lack of data available to train a classifier, a supervised model cannot be trained to predict the semantic cluster of a candidate response.&lt;/li&gt;
  &lt;li&gt;another hindrance in performing supervised learning is that the semantic space classes cannot be all defined a priori.&lt;/li&gt;
  &lt;li&gt;hence the semi-supervised technique is used for achieving this.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Graph Construction&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start by manually defining the clusters sampled from top frequent responses.&lt;/li&gt;
  &lt;li&gt;A small number of responses are added as seed for the clustering.&lt;/li&gt;
  &lt;li&gt;This leads to a base graph, where &lt;strong&gt;frequent responses are represented by nodes, \(V_R\)&lt;/strong&gt;. Lexical features (n-grams and skip grams upto a length of 3) are extracted for the responses and populated in graph as the &lt;strong&gt;feature nodes, \(V_F\)&lt;/strong&gt;. Edges are created between the pair of nodes, \((u,v)\) where \(u \in V_R\) and \(v \in V_F\). Similarly, nodes are created for manually labelled examples, \(V_L\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Unsupervised Learning&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The constructed graph captures the relationship between the canonicalized responses via feature nodes.&lt;/li&gt;
  &lt;li&gt;Semantic intent for each repsonse node is learnt by propagating intent information from manually labelled examples through the graph.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The algorithm works to minimize the following objective function for the response nodes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s_i \lVert \hat{C_i} - C_i \rVert^2 + \mu_{pp} \lVert \hat{C_i} - U \rVert^2 + \mu_{np} \left( \sum_{j \in \mathcal{N}_{\mathcal{F}} (i)} w_{ij} \lVert \hat{C_i} - \hat{C_j} \rVert^2 + \sum_{k \in \mathcal{N}_{\mathcal{R}} (i)} w_{ik} \lVert \hat{C_i} - \hat{C_k} \rVert^2\right) \tag{5} \label{5}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(s_i\) is an &lt;strong&gt;indicator function&lt;/strong&gt; equal to 1 if node \(i\) is a seed else 0.&lt;/li&gt;
  &lt;li&gt;\(\hat{C_i}\) is the &lt;strong&gt;learnt semantic cluster distribution&lt;/strong&gt; for response node \(i\).&lt;/li&gt;
  &lt;li&gt;\(C_i\) is the &lt;strong&gt;true label distribution&lt;/strong&gt; (i.e. for the manually provided examples)&lt;/li&gt;
  &lt;li&gt;\(\mathcal{N}_{\mathcal{F}} (i)\) and \(\mathcal{N}_{\mathcal{R}} (i)\) represent the feature and response neighbourhood of node \(i\).&lt;/li&gt;
  &lt;li&gt;\(\mu_{np}\) is the predefined penalty for neighbouring nodes with divergent label distributions.&lt;/li&gt;
  &lt;li&gt;\(\hat{C_j}\) is the learnt label distribution for feature neighbour \(j\).&lt;/li&gt;
  &lt;li&gt;\(w_{ij}\) is the weight of feature \(j\) in response \(i\).&lt;/li&gt;
  &lt;li&gt;\(\mu_{pp}\) is the penalty for label distribution deviating from prior, Uniform Distribution \(U\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similarly, the objective is to reduce the following objective function for the feature nodes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_{pp} \lVert \hat{C_i} - U \rVert^2 + \mu_{np} \left( \sum_{j \in \mathcal{N}_{\mathcal{F}} (i)} w_{ij} \lVert \hat{C_i} - \hat{C_j} \rVert^2 + \sum_{k \in \mathcal{N}_{\mathcal{R}} (i)} w_{ik} \lVert \hat{C_i} - \hat{C_k} \rVert^2\right) \tag{6} \label{6}&lt;/script&gt;

&lt;p&gt;\eqref{5} and \eqref{6} are alike except that \eqref{6} does not have the first term as there are no seed labels for the feature nodes.&lt;/p&gt;

&lt;p&gt;The objective functions \eqref{5} and \eqref{6} are jointly optimized for all the nodes. In order to discover the new clusters the algorithm is run in phases, in which randomly 100 new responses are sampled among the unlabeled nodes. These are treated as the potential new clusters and labeled with there canonicalized representations after which the algorithm is rerun and the process is repeated for the unlabeled nodes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cluster Validation&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Finally, the top \(k\) members from each semantic cluster are extracted and sorted by their label scores.&lt;/li&gt;
  &lt;li&gt;The set of (response, cluster label) pairs are then validated by human raters.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;suggestion-diversity&quot;&gt;Suggestion Diversity&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The LSTM model is trained to returned the approximate best response among the target response set.&lt;/li&gt;
  &lt;li&gt;The responses are &lt;strong&gt;penalized if they are too general&lt;/strong&gt; to be valuable to any user.&lt;/li&gt;
  &lt;li&gt;The next &lt;strong&gt;challenge lies in choosing a small number of responses&lt;/strong&gt; to display to the user which maximizes the utility.&lt;/li&gt;
  &lt;li&gt;A straight-forward way of doing this can be to &lt;strong&gt;choose the top \(N\) responses&lt;/strong&gt; and present them to the user. But in practice it is observed that such responses tend to be very similar. It is obvious to anyone that the likelihood of one of the repsonses being useful is greatest when none of the responses presented to the users are redundant, i.e. it would be wasteful to present a user with three responses that are a variation of same sentence.&lt;/li&gt;
  &lt;li&gt;The second and more optimal approach to suggest responses to users would &lt;strong&gt;include enforcing diversity&lt;/strong&gt;. This is achieved by:
    &lt;ul&gt;
      &lt;li&gt;omitting redundant responses.&lt;/li&gt;
      &lt;li&gt;enforcing negative or positive responses.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Omitting Redundant Responses&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The strategy states that a user should &lt;strong&gt;never see two responses with the same intent&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Intent can be thought of as a cluster of responses that have a common communication purpose.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;In smart reply, every suggested responses is associated with a exactly one intent. These intents are learnt using the semi-supervised learning algorithm explained &lt;a href=&quot;#response-set-generation&quot;&gt;above&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The actual diversity strategy simple: the top responses are iterated over in order of decreasing score. Each response is added to suggestion list unless its intent is already covered by a response in the suggestion list.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Enforcing Negatives and Positives&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is observed that the LSTM trained has a strong tendency towards positive responses, whereas negative responses generally get a low score.&lt;/li&gt;
  &lt;li&gt;It might be reflective of the style of email conversations: positive replies are more common and when the replies are negative people prefer more indirect wording.&lt;/li&gt;
  &lt;li&gt;Since, it is important to give out and option of repsonding negatively, the following strategy is followed:&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;If the top two responses (chosen from different intents) contain atleast one positive and none of the three responses are negative, the third response is replaced with a negative one.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A positive response is the one that is clearly affirmative. In order to find the negative response to be included as the third option, a second LSTM pass is performed, in which the search is restricted to only to the negative responses in the target set.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It might also be the case that an incoming message triggers exclusively negative responses. In which case, an analogous strategy for enforcing positives is employed.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;triggering&quot;&gt;Triggering&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;This is a second model (in this case a fully-connected feed-forward neural network which produced probability score) that is responsible for filtering messages that are bad candidates for suggesting responses. These might include emails that require longer responses, or emails that do not require a response at all.&lt;/li&gt;
  &lt;li&gt;On an average this system only decides that 11% of the incoming messages should get processed for smart reply. This selectivity further helps to speed up the process of analyzing the incoming emails, and decrease the time spent on LSTM and hence inturn reduce the infrastructure costs.&lt;/li&gt;
  &lt;li&gt;The two main objectives that this system should fulful are:
    &lt;ul&gt;
      &lt;li&gt;it should be accurate enough to decide when a smart reply should not be generated&lt;/li&gt;
      &lt;li&gt;it should be fast.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The choice of model is because it has been repeatedly observed that these ANN outperform linear models such as SVMs or linear regression on NLP tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Data and Features&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data includes the set of emails in the pair \((o, y)\), where \(o\) is an incoming message and \(y\) is a boolean true or false based on whether or not a email was replied to. For the positive class, only the messages that were replied to from a mobile device are considered.&lt;/li&gt;
  &lt;li&gt;Since the number of emails that are not replied to are found to be higher, the negative class examples are downsampled to match the number of positive class examples.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt; (unigrams and bigrams) are extracted from message body, subject and headers. Other &lt;strong&gt;social signals&lt;/strong&gt; such as whether or not the sender is in receipent’s address book etc is also used.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Network Architecture and Training&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Feed forward neural network with embedding layer and three fully connected hidden layers&lt;/li&gt;
  &lt;li&gt;Feature hashing is used to bucket rare words that are not present in the vocabulary.&lt;/li&gt;
  &lt;li&gt;Embeddings are aggregated by summation within a features (like bigram etc.)&lt;/li&gt;
  &lt;li&gt;Activation function: ReLu and Dropout layers are used.&lt;/li&gt;
  &lt;li&gt;Trained using AdaGrad optimization technique.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;evaluation-and-results&quot;&gt;Evaluation and Results&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For the LSTM model data consists of incoming messages and its responses by a user.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the triggering model, messages are used with the label describing whether or not they were replied to from a mobile device.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The following &lt;strong&gt;preprocessing&lt;/strong&gt; techniques are used:
    &lt;ul&gt;
      &lt;li&gt;Language detection: non-english messages are discarded.&lt;/li&gt;
      &lt;li&gt;Tokenization: messages and subjects are broken down into words and punctuations&lt;/li&gt;
      &lt;li&gt;Sentence segmentation: sentence boundaries are detected in the message body&lt;/li&gt;
      &lt;li&gt;Normalization: infrequent words and entities like personal informations are replaced by special tokens.&lt;/li&gt;
      &lt;li&gt;Quotation removal: Quoted original messages and forwarded messages are removed.&lt;/li&gt;
      &lt;li&gt;Salutation/close removal: salutations and closing notes are removed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;After preprocessing the size of the training data is &lt;strong&gt;238 million&lt;/strong&gt; messages, which includes 153 million messages that have no response.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Standard binary performance metrics are observed for triggering model: Precision, recall and area under the ROC curve.&lt;/li&gt;
  &lt;li&gt;AUC of triggering model is 0.854&lt;/li&gt;
  &lt;li&gt;For the LSTM model Precision, Mean Reciprocal Rank and Precision@K is observed.&lt;/li&gt;
  &lt;li&gt;A model with lower perplexity assigns a higher likelyhood to the test responses, and hence should be better at predicting responses. Perplexity of smart reply is 17.0 (by comparison, and n-gram model with katz backoff and maximum order of 5 has a perplexity of 31.4)&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;A perplexity equal to \(k\) means that when the model predicts the next word, there are on average \(k\) likely candidates.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;In an ideal scenario the perplexity of the system would be 1, i.e. one knows exactly what should be the next word. The perplexity on a set of \(N\) test samples is computed using the following formula:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_r = exp\left( - {1 \over W} \sum_{i=1}^N ln (\hat{P} (r_1^i, \cdots, r_m^i| o_1^i, \cdots, o_n^i)) \right) \tag{7} \label{7}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(W\) is the total number of words in the \(N\) samples.&lt;/li&gt;
  &lt;li&gt;\(\hat{P}\) is the learnt distribution&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(r^i\) and \(o^i\) are the \(i-th\) repsonse and original message.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The model is also evaluated on the response ranking. Simply put, the rank of the actual response with respect to other responses in R is evaluated. Using this, the &lt;strong&gt;mean reciprocal rank&lt;/strong&gt; (MRR) is calculated using:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;MRR = {1 \over N} \sum_{i=1}^N {1 \over rank_i} \tag{8} \label{8}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Additionally, Precision@K (for a given value of K, the number of cases for which target response \(r\) is within the topK responses that were ranked by the model) is also computed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;On a daily basis, the smart reply system generates 12.9k unique suggestions that belong to 376 unique semantic clusters, out of which the users utilized, 31.9% of the suggestions and 83.2% of the unique clusters.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Among the selected responses, 45% are the 1st responses, 35% 2nd responses, and 20% 3rd responses.&lt;/li&gt;
  &lt;li&gt;If using only the straight-forward approach instead of enforcing diversity, the click through rates drop by roughly 7.5%.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://ai.google/research/pubs/pub45189&quot; target=&quot;_blank&quot;&gt;Smart Reply: Automated Response Suggestion for Email&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;
&lt;small&gt;&lt;a href=&quot;https://www.blog.google/products/gmail/save-time-with-smart-reply-in-gmail/&quot; target=&quot;_blank&quot;&gt;Save time with Smart Reply in Gmail&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2018/07/22/google-smart-reply/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2018/07/22/google-smart-reply/</guid>
        
        <category>NLP</category>
        
        <category>machine-learning</category>
        
        <category>papers</category>
        
        
      </item>
    
      <item>
        <title>Large Scale Learning</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;The popularity of machine learning techniques have increased in the recent past. One of the reasons leading to this trend is the exponential growth in data available to learn from. Large datasets coupled with a high variance model has the potential to perform well. But as the size of datasets increase, it poses various problems in terms of space and time complexities of the algorithms.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It’s not who has the best algorithm that wins. It’s who has the most data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For example, consider the update rule for parameter optimization using gradient descent from (3) and (4) in the &lt;a href=&quot;/2017/08/23/multivariate-linear-regression/&quot; target=&quot;\_blank&quot;&gt;multivariate linear regression post&lt;/a&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j := \theta_j - \alpha {1 \over m} \sum_{i=1}^m \left( h_{\theta}(x^{(i)}) - y^{(i)} \right) x_j^{(i)} \tag{1} \label{1}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/shamssam/gradient-descent-for-regression&quot; target=&quot;\_blank&quot;&gt;Kaggle Kernel Implementation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;batch_update_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_add_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;batch_update_iterative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;update_theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_add_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DoubleTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;update_theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;update_theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update_theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;
    

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;batch_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;converged&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prev_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;init_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev_cost&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;converged&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_update_vectorized&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;converged&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;prev_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From \eqref{1} above, it can be seen that for each step of gradient descent, summation has to be performed over entire dataset of \(m\) examples. While for small datasets it might seem inconsequential, but as the size of datasets increases this would have very high impact on the training time.&lt;/p&gt;

&lt;p&gt;In such cases, it would also be helpful to plot &lt;a href=&quot;/2018/04/02/evaluation-of-learning-algorithm/#learning-curves&quot;&gt;learning curves&lt;/a&gt;, to check if actually training the model with such high number data samples is really helpful, because if the model has high bias then similar result could be acheived by using a smaller dataset. It would be more helpful to incrase variance of the model in such cases.&lt;/p&gt;

&lt;p&gt;On the other hand, if the learning curves show that using the larger dataset is indeed helpful, it would be more productive to use more computationally efficient algorithms to train the model such as the ones mentioned in the following sections.&lt;/p&gt;

&lt;h3 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/h3&gt;

&lt;p&gt;The gradient descent rule presented in \eqref{1}, also known as &lt;strong&gt;batch gradient descent&lt;/strong&gt;, has the disadvantage that for each update the summation of update term has to be performed over all the training data.&lt;/p&gt;

&lt;p&gt;Stochastic gradient descent is an approximation of the batch gradient descent. Each epoch in this algorithm is begun with a random shuffle of the data followed by the following update rule,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j := \theta_j - \alpha \left( h_{\theta}(x^{(i)}) - y^{(i)} \right) x_j^{(i)} \tag{2} \label{2}&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;stochastic_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;converged&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_add_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;init_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;converged&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;prev_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev_cost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;converged&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;i.e. for each training data in the sample dataset, as soon as the cost correponding to that instance is calculated it is used to make an approximate update to the parameters instead of waiting for the summation to finish. While this is not as accurate as the batch gradient descent in reaching the global minimum, it always converges within its close proximity.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In practice, stochastic gradient descent speeds up the process of convergence over the traditional batch gradient descent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While learning rate is kept constant in most implementations of stochastic gradient descent, it is observed in practice that it helps to taper off the value of learning rate as the iteration proceeds. It can be done as follows,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha = \frac {constant_1} {iteration\_number + constant_2} \tag{3} \label{3}&lt;/script&gt;

&lt;h3 id=&quot;mini-batch-gradient-descent&quot;&gt;Mini-Batch Gradient Descent&lt;/h3&gt;

&lt;p&gt;While batch gradient descent sums over all the data for a single update iteration of the parameters, the stochastic gradient descent does it by considering individual training examples as and when they are encountered. The &lt;strong&gt;mini-batch gradient descent&lt;/strong&gt; takes the mid-way and uses the summation from only &lt;strong&gt;b training examples (i.e. batch size)&lt;/strong&gt; for every update iteration. Mathematically it can be presented as follows,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j := \theta_j - \alpha {1 \over b} \sum_{i=1}^{i+b} \left( h_{\theta}(x^{(i)}) - y^{(i)} \right) x_j^{(i)} \tag{4} \label{4}&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mini_batch_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;converged&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_add_bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;init_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;converged&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;prev_cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev_cost&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tolerance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;converged&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Compared to stochastic gradient descent, the mini-batch gradient descent will be faster only if vectorized implementation is used for the updates.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compared to batch gradient descent, the mini-batch gradient descent is faster due to the obvious reason of lesser number of summations that are to be performed for a single update iteration. Also, if both the implementations are vectorized, mini-batch gradient descent will have lower memory usage. The speed of operations depends on the trade-off between the matrix operation complexities and memory usage.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generally it is observed that mini-batch gradient descent converges faster than both stochastic and batch gradient descent.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;online-learning&quot;&gt;Online Learning&lt;/h3&gt;

&lt;p&gt;Online learning is a form of learning when the system has a continuous stream of training data. It implements the stochastic gradient descent forever using the input stream of data and discarding it once the parameter updates have been done using it.&lt;/p&gt;

&lt;p&gt;It is observed that such an online learning setting is &lt;strong&gt;capable of learning the changing trends&lt;/strong&gt; of data streams.&lt;/p&gt;

&lt;p&gt;Typical domains where online learning can be successfully implemented include, search engines (predict click through rate i.e. CTR), recommendation websites etc.&lt;/p&gt;

&lt;p&gt;Many of the listed problems can be modeled as a standard learning problem with fixed dataset, but often such data streams are available in such abundance that there is little utility of storing the data in place of implementing an online training system.&lt;/p&gt;

&lt;h3 id=&quot;map-reduce-and-parallelism&quot;&gt;Map Reduce and Parallelism&lt;/h3&gt;

&lt;p&gt;Map-Reduce is a technique used in large scale learning when a single system is not enough to train the models required. Under this training paradigm, all the &lt;strong&gt;summation operations are parallelized over a set of slave systems by spliting the training data&lt;/strong&gt; (batch or entire set) across the systems which compute on smaller datasets and feed the results to the &lt;strong&gt;master system that aggregates the results&lt;/strong&gt; from all the slaves and combines them together. This parallelized implementation boosts the speed of algorithm.&lt;/p&gt;

&lt;p&gt;If the network latencies are not high, then one can expect a boost in speed by upto \(n\) times by using a pool of \(n\) systems. So, in practice when the systems are on a network speed boost is slightly less than \(n\) times.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Algorithms that can be expressed as a summation over the training sets can be parallelized using map-reduce.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Besides a pool of computers, parallelization also works on multi-core machines with the added benifit of near-zero network latencies and hence faster.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/CipHf/learning-with-large-datasets&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Learning with Large Dataset&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;
&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/DoRHJ/stochastic-gradient-descent&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Stochastic Gradient Descent&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;
&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/9zJUs/mini-batch-gradient-descent&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Mini-Batch Gradient Descent&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;
&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/fKi0M/stochastic-gradient-descent-convergence&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Convergence of Stochastic Gradient Descent&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;
&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/ABO2q/online-learning&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Online Learning&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;
&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/10sqI/map-reduce-and-data-parallelism&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Map Reduce and Data Parallelism&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate>
        <link>https://machinelearningmedium.com/2018/06/22/large-scale-learning/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2018/06/22/large-scale-learning/</guid>
        
        <category>machine-learning</category>
        
        <category>andrew-ng</category>
        
        
        <category>basics-of-machine-learning</category>
        
      </item>
    
  </channel>
</rss>
