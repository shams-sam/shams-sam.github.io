<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning Medium</title>
    <description>Mathematics, Machine Learning, Natural Language Processing Etc.</description>
    <link>https://machinelearningmedium.com/</link>
    <atom:link href="https://machinelearningmedium.com/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 16 Feb 2018 19:20:40 +0530</pubDate>
    <lastBuildDate>Fri, 16 Feb 2018 19:20:40 +0530</lastBuildDate>
    <generator>Jekyll v3.5.2</generator>
    
      <item>
        <title>Understanding TensorFlow</title>
        <description>&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Python Programming&lt;/li&gt;
  &lt;li&gt;Basics of Arrays&lt;/li&gt;
  &lt;li&gt;Basics of Machine Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tensforflow-apis&quot;&gt;TensforFlow APIs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The lowest level TensorFlow API, &lt;strong&gt;TensorFlow Core&lt;/strong&gt;, provides the complete programming control, recommended for machine learning researchers who require fine levels of control over their model.&lt;/li&gt;
  &lt;li&gt;The higher level TensorFlow APIs are built on top of TensorFlow Core. These APIs are easier to learn and use. Higher level APIs also provide convinient wrappers for repetitive tasks, e.g. &lt;strong&gt;tf.estimator&lt;/strong&gt; helps to manage datasets, estimators, training, inference etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;terminologies&quot;&gt;Terminologies&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Computational Graph&lt;/strong&gt; defines the series of TensorFlow operations arranged in the form of graph nodes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: Similar to placeholders, but Dataset represents a potentially large set of elements that can be accessed using iterators. These are the preferred method of streaming data into a model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Node&lt;/strong&gt; in a TensorFlow graph takes zero or more tensors as inputs and produces a tensor as an output, e.g. &lt;strong&gt;Constant&lt;/strong&gt; is a type of node in TensorFlow that takes no inputs and outputs the value that it stores internally (as defined in the defination of Tensorflow nodes earlier).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Operations&lt;/strong&gt; are another kind of node in TensorFlow used to build the computational graphs, that consume and produce tensors.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Placeholder&lt;/strong&gt; is a promise to provide value later. These serve the purpose or parameters or arguments to a graph which represents a dynamic function based on inputs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Rank:&lt;/strong&gt; The number of dimensions of a tensor (similar to rank of a matrix).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Session&lt;/strong&gt;, used for evaluation of TensorFlow graphs, encapsulates the control and state of the TensorFlow runtime.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shape:&lt;/strong&gt; Often confused with rank, shape refers to the tuple of integers specifying the length of tensor along each dimension.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tensor:&lt;/strong&gt; A set of primitive types shaped into an array of any number of dimensions. It can be considered a higher-dimensional vector. Tensorflow &lt;strong&gt;used numpy arrays&lt;/strong&gt; to represent tensor values.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;TensorBoard&lt;/strong&gt; is a TensorFlow utility used to visualize the computational graphs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tensorflow-imports&quot;&gt;TensorFlow Imports&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;__future__&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;absolute_import&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;__future__&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;division&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;__future__&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_function&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tf&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;absolute_import&lt;/strong&gt;: The distinction between absolute and relative imports can be considered to be analogous to the concept of absolute or relative file paths or even URLs, i.e. absolute imports specify the exact path of the imports while the relative imports work w.r.t. the working directory. Therefore for the code that is to be shared among peers, it is recommended to use absolute imports.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;division&lt;/strong&gt;: The import belongs to era when the debate on &lt;strong&gt;true division vs floor division&lt;/strong&gt; was on in the python community i.e. for python 2.*. The import in python 3.* is not required as the regular division operator itself is the true division while the floor division is denoted by \\.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;print_function&lt;/strong&gt;: This import is again not necessary in python 3.*. It is used to invalidate the print as a statement in python 2.*. Post call to this function, print only has valid represetnation as a function, which has some apparent advantages over the print as a statement, e.g. print function can be used inside lambda function or list and dictionary comprehensions.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Generally all the &lt;strong&gt;__future__ imports&lt;/strong&gt; are recommended to be kept at the top of the file because it changes the way the compiler behaves and the set of rules it follows.&lt;/p&gt;

&lt;h3 id=&quot;initialization&quot;&gt;Initialization&lt;/h3&gt;

&lt;p&gt;Basically, a TensorFlow Core program can be split into two sections:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Building&lt;/strong&gt; the computational graph&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Running&lt;/strong&gt; the computational graph&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tensor initialization is done as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The output shows that the &lt;strong&gt;default type of a tensor is float32&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Also, the print statement does not print value assigned to the nodes. The actual values will be displayed only on evaluation of the nodes. In TensorFlow the evaluation of a node can only be done within a session as shown below.&lt;/p&gt;

&lt;h3 id=&quot;session&quot;&gt;Session&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;More complex TensorFlow graphs are built using operation nodes. For example,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Mutliple tensors can be passed to a &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Session.run&lt;/code&gt;, i.e., the &lt;code class=&quot;highlighter-rouge&quot;&gt;run&lt;/code&gt; method handles any combination of tuples dictionaries,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ab'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'total'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Some tensorflow functions return &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Operations&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Tensors&lt;/code&gt;. Also, the result of calling run on an Operation is &lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt; because Operations are run to cause a side-effect and not to retrieve a value.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;During a call to &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Session.run&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Tensor&lt;/code&gt; holds a single value throughout that run. This is consistent with the notion that state of graph is saved in a session making sure once initialized a tensor will not have updated values unless operated upon.&lt;/p&gt;

&lt;h3 id=&quot;tensorboard&quot;&gt;TensorBoard&lt;/h3&gt;

&lt;p&gt;In order to visualize the TensorFlow graph, following command can be followed:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FileWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/path/to/save/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FileWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/path/to/save'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_default_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now run the following command on the terminal, ensure &lt;strong&gt;TensorBoard&lt;/strong&gt; is installed.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensorboard --logdir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/path/to/save
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-01-02-understanding-tensorflow/fig-1-tensorflow-graph-visualization.png?raw=true&quot; alt=&quot;Fig.1 TensorFlow Graph Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It leads to the above graph displayed, which is an apt representation of the minimal graph that has been built so far. But it is a constant graph as the input nodes are constants. In order to build a parameterized graph, placeholders are used as shown below.&lt;/p&gt;

&lt;h3 id=&quot;feeding-data-using-placeholders&quot;&gt;Feeding Data using Placeholders&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FileWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/path/to/save'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/2018-01-02-understanding-tensorflow/fig-2-placeholder-graph.png?raw=true&quot; alt=&quot;Fig.2 TensorFlow Placeholder Graph&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;feed_dict&lt;/code&gt; argument can be used to overwrite any tensor in the graph.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The only &lt;strong&gt;difference between placeholders and other &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Tensors&lt;/code&gt;&lt;/strong&gt; is that placeholders throw and error if no value is fed to them.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fetches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;run_metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Runs operations and &lt;strong&gt;evaluates tensors listed in fetches&lt;/strong&gt; argument. The method will run &lt;strong&gt;one step of TensorFlow computation&lt;/strong&gt;, by running necessary graph consisting of tensors and operations to &lt;strong&gt;evaluate the dependencies of Tensors listed in fetches&lt;/strong&gt;, substituting the values listed in the feed_dict for the corresponding values.&lt;/p&gt;

&lt;p&gt;The fetches argument can be a &lt;strong&gt;single graph element, nested list, tuple, namedtuple, dict, or OrderedDict&lt;/strong&gt; that consists of graph elements as its leaves.&lt;/p&gt;

&lt;p&gt;The graph element may belong to one of the following classes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;tf.Operation: fetched value is None.&lt;/li&gt;
  &lt;li&gt;tf.Tensor: fetched value is a numpy ndarray containing the value of the tensor.&lt;/li&gt;
  &lt;li&gt;tf.SparseTensor: fetched value is a tf.SparseTensorValue.&lt;/li&gt;
  &lt;li&gt;get_tensor_handle op: fetched value is a numpy ndarray containing the handle of the tensor.&lt;/li&gt;
  &lt;li&gt;string: name of a tensor or operation in the graph.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;The value returned by &lt;code class=&quot;highlighter-rouge&quot;&gt;run()&lt;/code&gt; has the same shape as the fetches argument, where leaves are replaced by corresponding values returned by TensorFlow.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Example code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;namedtuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'k2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It can be observed on executing the code that the output of run saved has the same structure as the input to the fetches argument, i.e. a dictionary of namedtuple of lists and list of lists.&lt;/p&gt;

&lt;p&gt;The keys in feed_dict can belong to one of the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If key is &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Tensor&lt;/code&gt;: value maybe a scalar, string, list, or numpy array.&lt;/li&gt;
  &lt;li&gt;If key is &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Placeholder&lt;/code&gt;: shape of the value will be checked with shape of the placeholder for compatibility.&lt;/li&gt;
  &lt;li&gt;If key is &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.SparseTensor&lt;/code&gt;: value should be &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.SparseTensorValue&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;If key is nested tuple of Tensors or SparseTensors, then the value should be follow the same nested structure that maps to corresponding values in the keyâ€™s structure.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Each value in the feed_dict must be convertible to a numpy array of the dtype of corresponding key.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The following errors are raised by run function:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RuntimeError&lt;/code&gt;: If the Session is in invalid state or closed.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TypeError&lt;/code&gt;: If fetches or feed_dict keys are of inappropriate types.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ValueError&lt;/code&gt;: If fetches or feed_dict keys are invalid or refer to a tensor that does not exist.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;importing-data&quot;&gt;Importing Data&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.data&lt;/code&gt; api helps to build complex input pipelines. It basically helps deal with large amounts of data, maybe belonging to different formats and apply complicated transformations to the data such as image augmentation or handling text sequences of different lengths for preprocessing and batch processing use cases. The pipelines help abstract processes and also modularize the code for easier debugging and managing of the code.&lt;/p&gt;

&lt;p&gt;Some of the abstractions dataset introduces in TensorFlow are summarized below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.data.Dataset&lt;/code&gt; is used to represent a sequence of elements where each element contains one or more Tensor objects.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Creating a source (&lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset.from_tensor_slices()&lt;/code&gt;) contructs a dataset from one or more &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Tensor&lt;/code&gt; objects, while applying a transformation (&lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset.batch()&lt;/code&gt;) contructs a dataset from one or more &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.data.Dataset&lt;/code&gt; objects.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.data.Iterator&lt;/code&gt; is used to extract elements from a dataset, and acts as an interface between the input pipeline and the models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;get_next&lt;/code&gt; method of iterator is called for streaming the data.&lt;/p&gt;

&lt;p&gt;Simplest iterator is made using &lt;code class=&quot;highlighter-rouge&quot;&gt;make_one_shot_iterator&lt;/code&gt; method as shown below. It is associated with a particular dataset and iterates through it once.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;slices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_tensor_slices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;next_item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_one_shot_iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OutOfRangeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Reaching the end of dataset, if &lt;code class=&quot;highlighter-rouge&quot;&gt;get_next&lt;/code&gt; is called, &lt;code class=&quot;highlighter-rouge&quot;&gt;OutOfRangeError&lt;/code&gt; is thrown by the Dataset.&lt;/p&gt;

&lt;p&gt;For more sophisticated uses, &lt;code class=&quot;highlighter-rouge&quot;&gt;Iterator.initializer&lt;/code&gt; is used that helps reinitialize and parameterize an iterator with different datasets, including running over a single or a set of datasets multiple number of times in the same program.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Placeholders work for simple experiments, but Datasets are the preferred method of streaming data into a model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A dataset consists of &lt;strong&gt;elements&lt;/strong&gt; that each have exactly the same structure, i.e. an element contains one or more &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Tensor&lt;/code&gt; objects, called &lt;strong&gt;components&lt;/strong&gt;. A component has &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.DType&lt;/code&gt; representing the type of elements in the tensor, and a &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.TensorShape&lt;/code&gt; (maybe partially specified, for example batch size might be missing but dimension of elements in batch may be present) representing the static shape of each element.&lt;/p&gt;

&lt;p&gt;Similarly the  &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset.output_types&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset.output_shapes&lt;/code&gt; inspect the inferred types and shapes of each component of the dataset element.&lt;/p&gt;

&lt;p&gt;It is optional but often helps to &lt;strong&gt;name the components&lt;/strong&gt; in a dataset element. To summarize, one can use tuples, &lt;code class=&quot;highlighter-rouge&quot;&gt;collections.namedtuples&lt;/code&gt; or a dictionary mapping strings to tensors to represent a single element in a &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Sample code to see the above properties:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;(&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;)&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;, &quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;(&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;)&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;, &quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_string&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_tensor_slices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;element1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_initializable_iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dataset1:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_types&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_shapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_tensor_slices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;element2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_initializable_iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dataset2:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_types&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_shapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;element3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_initializable_iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dataset3:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_types&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_shapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Outputs:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dataset1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'float32'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'float32'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'int32'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'float32'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'second'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'first'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'second'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorShape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'first'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorShape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'float32'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'int32'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'float32'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'d1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'d2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'second'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'first'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'d1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorShape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'d2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'second'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorShape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dimension&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'first'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TensorShape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([])}}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So, the &lt;strong&gt;basic mechanics&lt;/strong&gt; of import data can be listed as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Define a source&lt;/strong&gt; is the first step in defining an input pipeline. For example, data can be imported from in-memory tensors using &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.data.Dataset.from_tensors()&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.data.Dataset.from_tensor_slices()&lt;/code&gt;. It can also be imported from disk if the data is in &lt;strong&gt;recommended TFRecord&lt;/strong&gt; format using &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.data.TFRecordDataset&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Transformations&lt;/strong&gt; can be applied on any dataset to obtain subsequent dataset objects. This can be achieved by chaining preprocessing or transformation operations. For example, &lt;strong&gt;per-element transformations&lt;/strong&gt; can be applied using &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset.map()&lt;/code&gt; or &lt;strong&gt;multi-element transformations&lt;/strong&gt; can be applied using &lt;code class=&quot;highlighter-rouge&quot;&gt;Dataset.batch()&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dataset transformations support datasets of any structure. The element structure determines the argument of a function. For example,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dataset1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flat_map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;From the example above it can be seen that the &lt;strong&gt;lambda function can take any structure as the input&lt;/strong&gt; based on the structure of an element in the dataset, however complex it is.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Define an iterator&lt;/strong&gt; to stream data into the model. The iterator can be &lt;strong&gt;one-shot iterator&lt;/strong&gt; as in the example above or one of the types listed below.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;One-shot iterator&lt;/strong&gt; is the simplest iterator that supports iterating only once through the dataset and does not require an explicit initialization. Hence, as a by-product, it does not allow parameterization.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Initializable iterator&lt;/strong&gt; requires an explicit &lt;code class=&quot;highlighter-rouge&quot;&gt;iterator.initializer&lt;/code&gt; operation before using it. At the cost of this inconvinience, it gives the flexibility to parameterize the defination of dataset using placeholders.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;max_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_initializable_iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;next_element&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# parameter passing for the placeholder&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reinitializable iterator&lt;/strong&gt; can be initialized from multiple different dataset objects. Basically, while the datasets may change they have the same structure attributed to each element of the dataset. A reinitializable iterator is defined by its structure and any dataset complying to that structure can used to initialize the iterator.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# two different datasets with same structure&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([],&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# define the iterator using the structure property&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_structure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_types&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                           &lt;span class=&quot;n&quot;&gt;training_dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_shapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;next_element&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;training_init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# initialize for training set&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# reinitialize for validation set&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Feedable iterator&lt;/strong&gt; is used along with &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.placeholder&lt;/code&gt; to select &lt;code class=&quot;highlighter-rouge&quot;&gt;Iterator&lt;/code&gt; to use in each call of &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Session.run&lt;/code&gt;, via the &lt;code class=&quot;highlighter-rouge&quot;&gt;feed_dict&lt;/code&gt; mechanism. It does not require one to initialize the iterator from the start of a dataset when switching between the iterators. The &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.data.Iterator.from_string_handle&lt;/code&gt; can be used to define a feedable iterator.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;string_handle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_string_handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;string_handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;training_dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_types&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;training_dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_shapes&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;next_element&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# get string handles of each iterator&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_handle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validation_handle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validation_iterator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# using training iterator&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# using validation iterator&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_handle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;layers&quot;&gt;Layers&lt;/h3&gt;

&lt;p&gt;A trainable model is implemented in TensorFlow by means of using Layers which adds trainable parameters to a graph. A layer packages the variables and the operations that act on them. For example, &lt;strong&gt;densely-connected layer&lt;/strong&gt; performs weighted sum across all inputs from each output and applies an &lt;strong&gt;optional activation function&lt;/strong&gt;. The connection weights and biases are managed by the layer object.&lt;/p&gt;

&lt;p&gt;In order to apply layer to an input, the layer is called as a function with input as an argument.&lt;/p&gt;

&lt;p&gt;After calling the layer as a function, based on the inputs to the layer it sets up shape of weight matrices compatible with the input. Now, the layers contain variables that must be initialized before they can be used.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;placeholder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dense_layer'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Calling &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.global_variables_initializer&lt;/code&gt; only creates and returns a handle to tensorflow operation which will initialize all global variables on call of &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Session.run&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;For each layer class (like &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.layers.Dense&lt;/code&gt;) there exists a shortcut function in TensorFlow (like &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.layers.dense&lt;/code&gt;) that creates and runs the layer in a single call.&lt;/strong&gt; But this approach allows no access for the &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.layers.Layer&lt;/code&gt; object which might cause difficulties in debugging and introspection or layer reuse possibilities.&lt;/p&gt;

&lt;p&gt;The graph of a linear model looks as shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-01-02-understanding-tensorflow/fig-3-linear-model.png?raw=true&quot; alt=&quot;Fig.3 Linear Model Visualization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where the linear_model block internally has the structure as shown in Fig. 4 which abides by \eqref{1}.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-01-02-understanding-tensorflow/fig-4-dense-layer.png?raw=true&quot; alt=&quot;Fig.4 Linear Model Visualization&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = Wx + b \tag{1} \label{1}&lt;/script&gt;

&lt;p&gt;where W is the weights being fed by the kernel, x is the input vector being fed by the input placeholder and b is the bias.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Feature columns can be experimented with using &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.feature_column.input_layer&lt;/code&gt; function for dense columns as input and &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.feature_column.indicator_column&lt;/code&gt; for categorical indicators as input.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'sales'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'department'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sports'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'sports'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'gardening'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'gardening'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;department_column&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_column&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;categorical_column_with_vocabulary_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'department'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sports'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'gardening'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;department_column&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_column&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indicator_column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;department_column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_column&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numeric_column&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sales'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;department_column&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature_column&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;var_init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;table_init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;var_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;table_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Feature columns can have internal state&lt;/strong&gt;, like layers and so need to be initialized. Similarly, categorical columns use lookup tables internally and hence require &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.table_initializer&lt;/code&gt; additionally.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Categorical input fed using indicator vectors are one-hot encoded.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Define the data and labels.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Define the model&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;At this point, model output can be evaluated&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# initialize session and variable&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# extract model output&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The output would not be correct as the model is not trained to optimize model parameters for accuracy.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To optimize the model, a loss has to be defined.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using mean squared error,&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;After defining a loss, one of the optimizers provided by TensorFlow out of box can be used as optimization algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The optimizers are defined in &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.train.Optimizer&lt;/code&gt;. Using the simplest &lt;strong&gt;gradient descent&lt;/strong&gt; implemented in &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.train.GradientDescentOptimizer&lt;/code&gt;,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Gradient Descent modifies each variable according to the magnitude of the derivative of loss with respect to that variable.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientDescentOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Training the model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At this stage the model graph is built and the only task pending is to &lt;strong&gt;call the evaluate train iteratively to minimize loss by optimizing model trainable parameters&lt;/strong&gt; by updating the corresponding variables.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Evaluate model predictions&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Model Graph&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model graph generated from the above training example can be seen below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2018-01-02-understanding-tensorflow/fig-5-example-model.png?raw=true&quot; alt=&quot;Fig.5 Example Model Graph&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since the model inputs are constants, it can be seen that &lt;strong&gt;the dense layer in the graph has no input&lt;/strong&gt; being fed from outside, rather is a part of the dense block.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://www.tensorflow.org/programmers_guide/low_level_intro&quot; target=&quot;_blank&quot;&gt;TensorFlow Low-level Introduction&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 02 Jan 2018 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2018/01/02/understanding-tensorflow/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2018/01/02/understanding-tensorflow/</guid>
        
        <category>machine learning</category>
        
        <category>tensorflow</category>
        
        <category>featured</category>
        
        
      </item>
    
      <item>
        <title>B-Money</title>
        <description>&lt;h3 id=&quot;cryptocurrency-series&quot;&gt;Cryptocurrency Series&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;/2017/07/02/blockchain/&quot;&gt;Blockchain&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/12/21/bitcoin-satoshi-nakamoto/&quot;&gt;Bitcoin&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/12/28/b-money-wei-dai/&quot;&gt;B-Money&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;horizontal-divider&quot;&gt;Â· Â· Â·&lt;/div&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;B-Money, an early proposal by Wei Dai for an anonymous, distributed electronic cash system was referred by Satoshi Nakamoto for creating Bitcoin, the current cryptocurrency giant.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the referred post, the author identified existing operational problems in an online community named Crypto-Anarchy. Crypto-Anarchy is explained as an online community where participants are identified with psuedonyms which are in no way associated with their true names or physical locations. Because of this seperation of true identity from community identity, the possibility of voilence is rendered impotent. Together these make the role of government permanently forbidden and unnecessary in such a community.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A community is defined by cooperation of its participants, and an efficient cooperation requires a medium of exchange (money) and a way to enforce contracts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While traditionally these services were provided by the government or government sponsored institutions, it was not clear theoretically how to implement a similar system among decentralized nodes without using a trusted central authority.&lt;/p&gt;

&lt;p&gt;Author proposed two different protocols which can solve these issues, first one is very similar to the current proof-of-work protocol and second is similar to proof-of-stake protocol.&lt;/p&gt;

&lt;p&gt;Author poses that the implementation of the first system is impractical, because it makes heavy use of synchronous and unjammable anonymous broadcast channel.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Existence of untraceable network is assumed, where senders and receivers are identified only by their digital psuedonyms (i.e. public keys) and every message is signed by its sender and encrypted to its receiver.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;first-protocol&quot;&gt;First Protocol&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Analogous to &lt;strong&gt;proof-of-work&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;A seperate database is maintained by every participant with details of how much money belongs to which &lt;strong&gt;pseudonym&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The individual databases collectively define the ownership of the money. The accounts are update subject to the rules in this protocol listed below.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The creation of money:&lt;/strong&gt; Money is created by broadcasting the solution to a previously unsolved computational problem. Such solutions must be easy to determine how much computing effort it took to solve the problem and must not otherwise have any practical or intellectual value (similar to nonce in a proof of work). Upon broadcasting the solution and verifying it, everyone credits the amount to solverâ€™s psuedonym, equivalent to the amount of units it would take to buy the electricity utilized by the most economical computer to solve the problem.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The transfer of money:&lt;/strong&gt; If owner of psuedonym pk_A (public key of A) sends X units of money to owner of psuedonym pk_B (public key of B), they have to broadcast a message signed with their public key. On receiving this broadcast, all the participants would debit amount X from psuedonym pk_A and credit it to psuedonym pk_B, unless this creates a negative balance in pk_Aâ€™s account in which case the broadcasted transaction is ignored.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The effecting of contracts:&lt;/strong&gt; A valid contract binds a &lt;strong&gt;maximum reperation&lt;/strong&gt; for each participating party in case of default. It also specifies a third party who will do the arbitration in case any dispute arises. In order for the contract to be effective, all the participating parties and the arbitrator must broadcast their public keys (i.e. psuedonyms). Upon the broadcast of contract and all the associated signatures, every participant debits the account of each party by the amount of his maximum reperation and credits a special account identified by secure has of the contract by the sum of the maximum reperations. The contract is a success if the debits succeed for every party without any negative balances, failing which the contract is ignored and the accounts are rolled back.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The conclusion of contracts:&lt;/strong&gt; If the contract concludes without any dispute, each party must broadcast a message stating the same, following which each participant credits the accounts of each party by the amount of their maximum reperation, removes the contract account, then credits or debits the account of each party according to the reparation schedule if there is one.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;The enforcement of contract:&lt;/strong&gt; In case of dispute, which cannot be sorted by the arbitrator, each party broadcasts a suggested reparation/ fine schedule and arguments or evidence in his favor. Each participant makes a determination as to the actual reparations and/or fines, and modifies his accounts accordingly.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;second-protocol&quot;&gt;Second Protocol&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Similar to &lt;strong&gt;proof-of-stake&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The accounts of every psuedonym is maintained by a &lt;strong&gt;subset of the participants (called servers)&lt;/strong&gt; instead of everyone as in the previous protocol. Servers are connected (using Usenet-style broadcast channel).&lt;/li&gt;
  &lt;li&gt;Format of broadcasted transaction is same as first protocol, but &lt;strong&gt;affected participant of each transaction should verify the changes&lt;/strong&gt; on a randomly selected subset of the servers.&lt;/li&gt;
  &lt;li&gt;In order to bring a degree of trust in the servers, &lt;strong&gt;each server is required to deposit a certain amount of money in a special account&lt;/strong&gt; to be &lt;strong&gt;used as potential fines or rewards&lt;/strong&gt; for proof of misconduct.&lt;/li&gt;
  &lt;li&gt;Each server must &lt;strong&gt;periodically  publish and commit&lt;/strong&gt; to its current money creation and money ownership databases. Also they should verify that his own account balances are correct and that &lt;strong&gt;total sum of account balances in not greater than the total amount of money created&lt;/strong&gt;. This would prevent the servers, even in total collusion, from permanently and costlessly expanding the money supply.&lt;/li&gt;
  &lt;li&gt;New servers synchronize with the existing servers to used the published databases.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;alternative-b-money-creation&quot;&gt;Alternative B-Money Creation&lt;/h3&gt;

&lt;p&gt;One of the major problems faced in decentralized money network protocols is reaching a consensus for the cost of a computing effort. The rapid advances of computing technology, often in a private development makes it difficult to gather accurate information about these metrics while making sure they are not outdated.&lt;/p&gt;

&lt;p&gt;Author proposes a subprotocol, in which the account keepers (i.e. the participants in first protocol or servers in second protocol) decide and agree on the amount of b-money to be create each period, where the &lt;strong&gt;cost of b-money is determined by an auction&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Each money creation period is divided up into &lt;strong&gt;four phases&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Planning:&lt;/strong&gt; Account keepers compute and negotiate to determine an optimal increase in money supply for a given period. Whether or not  the participants reach a concensus, they broadcast their money creation quota and any macroeconomic calculations done to support the figures.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Bidding:&lt;/strong&gt; Anyone who wants to create the b-money makes a bid of the form &amp;lt;X, Y&amp;gt;, where X is the amount of b-money he wants to create and Y is an unsolved problem from a predetermined problem class (proof-of-work solution in case of bitcoin), where each problem has a nominal cost in MIPS-year publically agreed on.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Computation:&lt;/strong&gt; After bidding, the ones who placed the bids in bidding phase solve the problem in their bid and broadcast the solutions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Money Creation:&lt;/strong&gt; Each participant accepts the highest bids (among all those who broadcast solutions) in terms of nominal cost per unit of b-money created and credits the bidderâ€™s account accordingly.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;http://www.weidai.com/bmoney.txt&quot; target=&quot;_blank&quot;&gt;B-Money by Wei Dai&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Dec 2017 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2017/12/28/b-money-wei-dai/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2017/12/28/b-money-wei-dai/</guid>
        
        <category>concept</category>
        
        <category>references</category>
        
        
      </item>
    
      <item>
        <title>Bitcoin</title>
        <description>&lt;h3 id=&quot;cryptocurrency-series&quot;&gt;Cryptocurrency Series&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;/2017/07/02/blockchain/&quot;&gt;Blockchain&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/12/21/bitcoin-satoshi-nakamoto/&quot;&gt;Bitcoin&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/12/28/b-money-wei-dai/&quot;&gt;B-Money&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;horizontal-divider&quot;&gt;Â· Â· Â·&lt;/div&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Part solution to this task is given by digital signatures, but the other half of the solution posed a major question in developing such systems - the part that requires &lt;strong&gt;a system to prevent double spending&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In the current centralized money tender distributed, this role of preventing the double spending is done by a centralized third-party authority. But in a peer-to-peer network this centralized server would not be available and thus would require a different methodology.&lt;/p&gt;

&lt;p&gt;Bitcoin, proposed by &lt;strong&gt;Satoshi Nakamoto&lt;/strong&gt;, was to solve this very problem on the peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work.&lt;/p&gt;

&lt;h3 id=&quot;current-system&quot;&gt;Current System&lt;/h3&gt;

&lt;p&gt;Current economy is based on trusted third-parties to process electronic payments. These systems suffer from the inherent weaknesses of the trust based model. Completely non-reversible transactions are not possible because banks cannot avoid mediating disputes.&lt;/p&gt;

&lt;p&gt;Because of cost of mediation, the transaction costs are high which inturn limit the minimum practical transaction size thus cutting off the possibility of small casual transaction.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;With the possibility of reversal, the need for trust spreads.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;proposed-system&quot;&gt;Proposed System&lt;/h3&gt;

&lt;p&gt;Bitcoin proposed a system where &lt;strong&gt;trust is replaced by cryptographic proof&lt;/strong&gt;, allowing any two willing parties to transact directly without the need of a supporting trusted third party.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Computationally irreversible transactions prevent frauds.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bitcoin proposed to tackle the issue of double-spending, using a peer-to-peer distributed timestamp server to generate computational proof of the chronological order of transactions.&lt;/p&gt;

&lt;p&gt;The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes.&lt;/p&gt;

&lt;h3 id=&quot;transactions&quot;&gt;Transactions&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Electronic coin is defined as a chain of digital signatures.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A coin is transferred by an owner signing a hash of previous transaction and the public key of the next owner and adding these to then end of the coin.&lt;/p&gt;

&lt;p&gt;The signature can be verified by the payee to check the chain of ownership.&lt;/p&gt;

&lt;p&gt;The payee can still however not check for double-spending.&lt;/p&gt;

&lt;p&gt;Traditional solution of this involves introducing a trusted central authority, or mint, that checks every transaction for double-spending. In such a system, after every transaction the coin must be returned to the mint to issue a new coin, and only coins issued directly from the mint are trusted not to be double-spent. This still does not solve the basic problem of dependency on company running the mint, which is basically serving the same purpose as that of a bank in the current system.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-12-21-bitcoin-satoshi-nakamoto/fig-1-transaction.png&quot; alt=&quot;Flow of Transactions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The only way to confirm the absence of a transaction is to have a copy of all the transactions. To accomplish this in a decentralized system, the transactions must be publically announced and there is a need for a system for participants to agree on a single history of the order in which they were received. So, at the time of the transaction, the payee needs a proof that the majority of the nodes agreed it was the first received one.&lt;/p&gt;

&lt;h3 id=&quot;timestamp-server&quot;&gt;Timestamp Server&lt;/h3&gt;

&lt;p&gt;A timestamp server basically takes a hash of a block of items to be timestamped and widely publishes it, where the timestamp proves that the data must have existed at the time, in order to get the hash.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-12-21-bitcoin-satoshi-nakamoto/fig-2-blockchain.png&quot; alt=&quot;Timestamp Server&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Each timestamp includes previous timestamp in its hash, forming a chain (blockchain), with each timestamp reinforcing the ones before it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;proof-of-work&quot;&gt;Proof-of-Work&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;A system to deter denial of service attacks and other service abuses such as spam on a network by requiring some work from the service requester, usually meaning processing time by a computer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In case of bitcoins, the proof-of-work involves scanning for a value that when hashed along with the contents of the transaction blocks using cryptographic hashes such as SHA-256, gives a hash that begins with a specific number of zero bits. &lt;strong&gt;The average work required is exponential in the number of zero bits.&lt;/strong&gt; Proof-of-Work is generally a problem that is fairly tough to solve but very easy to verify.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;hashlib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;transaction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'A paid B 25 A_public_key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;s&quot;&gt;'A paid C 50 A_public_key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;s&quot;&gt;'C paid B 10 C_public_key'&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transaction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dumps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transaction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;difficulty_bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;time_taken&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_bits&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;difficulty_bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;proved&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;proof&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proved&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transaction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;proof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;current_hash&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hashlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sha256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hexdigest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_hash&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;startswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'0'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_hash&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;proof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;proved&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;time_taken&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;proof&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proof&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difficulty_bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time_taken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
Outputs
-------
cb87ca935b8b44e94f5c670ecd375ba88ab7616aeab4c4c4369fe79e9c153f95
0
0c1d9b1549d14612571040e8352af4175a41b84a805a08880d3acdd5aad998be
5
00a51beea09d1fae73f3e18682445a3eb9bffc2cabf17f87f90f193c9eca1af3
396
00010ff4f55c002484ee13841be9a3c46aba3038a1046d604ac8cee7349a2228
514
0000a3c456eff341c9988b194231b02eb547a36a8134d095a5e1c4efb05dc7e3
22344
000002128ccfc4d1e662a6ceb8053860b887929b691c2f8506b0843e9a57fe34
3355879
000000bfc98081b9aff064455708d2d8fcc2afeac81b5a0b03a6fd926db1ce36
19659271
0000000cbd6302c37b40102ef55040e4315b63b39640110b679e033c44a8b2b0
310723836

Time Taken
----------
[0.0003619194030761719,
 0.00020599365234375,
 0.0012040138244628906,
 0.0016019344329833984,
 0.07679605484008789,
 8.218470811843872,
 47.986721992492676,
 740.9898428916931]
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-12-21-bitcoin-satoshi-nakamoto/fig-8-proof-of-work-plot.png&quot; alt=&quot;Proof-of-work plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Basic needs of such system of proof-of-work are fulfilled by cryptographic hashes, that are often seen as one way functions, i.e. the only way to get the input given an output is to use the bruteforce approach and check all possible values of inputs and arriving at the correct candidate.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A nonce is an arbitrary number that can only be used once&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the bitcoinâ€™s timestamp network, the proof-of-work is implemented by incrementing a nonce in the block until a value is found that gives the block the required number of leading zero bits.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-12-21-bitcoin-satoshi-nakamoto/fig-3-proof-of-work.png&quot; alt=&quot;Proof-of-work Flow&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Once the CPU effort has been expended to satisfy the proof-of-work, the block cannot be changed without redoing the work.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As the later blocks consume the hashes of previous blocks, changing a transaction in earlier block would require redoing the work for all the leading blocks.&lt;/p&gt;

&lt;h3 id=&quot;majority-decision-making&quot;&gt;Majority Decision Making&lt;/h3&gt;

&lt;p&gt;Proof-of-Work also gives a solution to determining representation in majority decision making. The majority decision is represented by the longest chain, i.e. the greatest proof-of-work. So if the majority CPU power is held by honest nodes, then the honest chain would grow faster and outpace any chain being proposed by a pool of attacking nodes.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To compensate hardware power and varying number of nodes over time, the proof-of-work difficulty is determined by a moving average targeting an average number of blocks per hour, i.e. if the blocks are generated too fast, the difficulty increases for subsequent blocks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;network-protocols&quot;&gt;Network Protocols&lt;/h3&gt;

&lt;p&gt;Steps to run the network are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;New transactions are broadcast to all nodes.&lt;/li&gt;
  &lt;li&gt;Each node collects new transaction into a block.&lt;/li&gt;
  &lt;li&gt;Each node works on finding a difficult proof-of-work for its block.&lt;/li&gt;
  &lt;li&gt;Upon finding the solution, it broadcasts the block to all nodes.&lt;/li&gt;
  &lt;li&gt;Nodes accept a new block if all the transactions on it are valid and not already spent.&lt;/li&gt;
  &lt;li&gt;Nodes express their acceptance of the block by working on creating next block in the chain, using the hash of the accepted block as the previous hash.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nodes always consider the longest chain to be the correct one and keep on working to extend it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If a node does not receive a block, it will request it when it receives the next block and realizes that it missed one.&lt;/p&gt;

&lt;h3 id=&quot;mining-incentive&quot;&gt;Mining Incentive&lt;/h3&gt;

&lt;p&gt;By convention of the protocol, the first transaction of a block is a special transaction that starts a new coin owned by the creator of the block.&lt;/p&gt;

&lt;p&gt;It is this, that acts as an incentive for the nodes to support the network, and provides a way to initially distribute coins into circulation.&lt;/p&gt;

&lt;p&gt;Incentive can also be funded in the form of a transaction fees.&lt;/p&gt;

&lt;p&gt;Incentive will also encourage nodes to stay honest. Because an attacker with more compute power would find it more favourable to play by rules and make more transaction fee, instead of competing with rest to fraud the system and invalidating his own wealth in the process by sabotaging the system.&lt;/p&gt;

&lt;h3 id=&quot;memory-optimizations&quot;&gt;Memory Optimizations&lt;/h3&gt;

&lt;p&gt;After the latest transaction is reinforced with several blocks stacking on top of it, the spent transactions can be discarded from the disk.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Transactions are hashed in a Merkle Tree, with only root included in the blockâ€™s hash.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-12-21-bitcoin-satoshi-nakamoto/fig-4-merkle-tree.png&quot; alt=&quot;Memory Optimizations&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A block header with no transaction is about 80 bytes, and blocks are generated every 10 minutes, then every hour about 6 blocks would be generated. So the total space required by blocks generated in a year would be about 4.2MB (i.e 80 bytes * 6 * 24 * 365). With the current capabilities of system memories, storage would not pose a problem to the system.&lt;/p&gt;

&lt;h3 id=&quot;payment-verification&quot;&gt;Payment Verification&lt;/h3&gt;

&lt;p&gt;The system makes it possible to verify payments without running the full network node.&lt;/p&gt;

&lt;p&gt;Verification can be done by keeping a copy of the block headers of the longest proof-of-work chain, and obtaining the Merkle branch linking the transaction to the block itâ€™s timestamped in.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-12-21-bitcoin-satoshi-nakamoto/fig-5-validating-transaction.png&quot; alt=&quot;Payment Verification Process&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By linking the transaction to a place in the chain, one can see if network nodes have accepted it by checking the proof-of-work, which would be further confirmed by blocks built on top of it in the chain.&lt;/p&gt;

&lt;p&gt;Again, the verification is only reliable as long as honest nodes control the network. Because the simplified verification method can be fooled by an attackerâ€™s fabricated transactions for as long as the attacker can overpower the compute resources of the unified honest nodes.&lt;/p&gt;

&lt;h3 id=&quot;combining-and-splitting-value&quot;&gt;Combining and Splitting Value&lt;/h3&gt;

&lt;p&gt;Even though it is possible to handle each coin individually, it would be unweildy to make a seperate transaction for every cent in a transfer.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To allow value to be split and combined, transactions contain multiple inputs and outputs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-12-21-bitcoin-satoshi-nakamoto/fig-6-splitting-and-joining.png&quot; alt=&quot;Value Handling Protocols&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Usually, there is a single input from a larger previous transaction or multiple inputs combining smaller amounts, and at most two outputs: &lt;strong&gt;one for the payment, and one to return the change, if any, back to the sender&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;privacy&quot;&gt;Privacy&lt;/h3&gt;

&lt;p&gt;Traditional banking models achieve privacy through limiting access to information to the parties involved and the trusted third party.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-12-21-bitcoin-satoshi-nakamoto/fig-7-privacy.png&quot; alt=&quot;Privacy Models&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the proposed system, it is achieved by anonymizing the public keys that are used to sign a transaction. So, the public ledger can see someone is sending an amount to someone else, but without information linking the transaction to anyone (similar to tape in a stock exchange).&lt;/p&gt;

&lt;p&gt;Some linking is still unavoidable with multi-input transactions, which reveals that the inputs originated from the same owner. So if one ownerâ€™s key is revealed, linking will allow to discover other transactions associated with that owner.&lt;/p&gt;

&lt;h3 id=&quot;mathematics-of-bitcoin&quot;&gt;Mathematics of Bitcoin&lt;/h3&gt;

&lt;p&gt;If there were a scenario where an attacker is generating a alternate chain faster than the honest chain, it could still not generate value out of thin air or take someone elseâ€™s money because it will be rejected by the corresponding node which owns that token.&lt;/p&gt;

&lt;p&gt;So all it is capable of doing is to double-spend its own money.&lt;/p&gt;

&lt;p&gt;Basically the race between the honest chain and the alternate chain can be characterized as a &lt;strong&gt;Binomial Random Walk&lt;/strong&gt;, where success event is honest chain being extended by a block and failure event is the alternate chain extented by a block gaining a lead on the honest chain.&lt;/p&gt;

&lt;p&gt;In such a case, the probability of an attacker catching up with the honest chain by making up for the deficit can be modeled similar to a &lt;strong&gt;Gamblerâ€™s Ruin Problem&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;probability that the attacker ever reaches a breakeven&lt;/strong&gt; is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
q_z = 
\begin{cases}
1 &amp; \text{ if } p \leq q \\
(q/p)^z &amp; \text{ if } p \gt q
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;where,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(p\) is the probability of an honest chain extending&lt;/li&gt;
  &lt;li&gt;\(q\) is the probability of a attacker chain extending&lt;/li&gt;
  &lt;li&gt;\(q_z\) is the probability of the attacker catching up with the honest chain from \(z\) blocks deficit.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given the assumption that the pool of honest compute power is greater than that of attackers, it is safe to assume that \(p \gt q\). So the probability drops exponentially as the number of blocks of deficit for an attacker increases. So after a fair lead, the probability of attacker to catch up is vanishingly small.&lt;/p&gt;

&lt;h3 id=&quot;time-for-establishing-trust&quot;&gt;Time for Establishing Trust&lt;/h3&gt;

&lt;p&gt;Assume that the sender of a transaction is an attacker who wants to make the receiver believe that he paid him for a while, and switch it back to himself after sometime.&lt;/p&gt;

&lt;p&gt;So initially the receiver will generate a new key pair and give the public key to the attacker for making the transfer. This prevents the attacker from preparing a chain of blocks in advance because of the dynamic nature of the public key generated.&lt;/p&gt;

&lt;p&gt;So once the transaction is made, the attacker starts working on a alternate chain where the transactions are different.&lt;/p&gt;

&lt;p&gt;Meanwhile the receiver waits for the transaction to be added to a block and another \(z\) blocks to be chained on top of that block.&lt;/p&gt;

&lt;p&gt;The receiver does not know the progress of the attacker, but assuming the honest nodes took the average time to build a block, the attackerâ€™s progress will be a Poisson Distribution with the expected value,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda = z {q \over p}&lt;/script&gt;

&lt;p&gt;Probability that the attacker could catch up can be calculated by mutliplying the Poisson density for each amount of progress by the probability that he could catch up from that point, given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\sum_{k=0}^\infty \frac{\lambda^k\,e^{-\lambda}}{k!} \cdot \begin{cases} (q/p)^{(z-k)} &amp; \text{ if } k \leq z \\ 1 &amp; \text{ if } k \gt z \end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Rearranging to avoid infinite summation of the distribution,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;1 - \sum_{k=0}^z \frac{\lambda^k\,e^{-\lambda}}{k!} (1 - (q/p)^{(z-k)})&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;factorial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fac&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fac&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fac&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lamda&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lamda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lamda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;factorial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;=&amp;gt; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calculate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
Output
------
0   =&amp;gt;  1.0
5   =&amp;gt;  0.17735231
10  =&amp;gt;  0.04166048
15  =&amp;gt;  0.01010076
20  =&amp;gt;  0.0024804
25  =&amp;gt;  0.00061323
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Calculating the values, it is observed that the probability drops off exponentially with z.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Bitcoin does not need a third party to enforce trust.&lt;/li&gt;
  &lt;li&gt;Coin framework is based on digital signatures.&lt;/li&gt;
  &lt;li&gt;A peer-to-peer network using proof-of-work records a history of transaction.&lt;/li&gt;
  &lt;li&gt;Nodes can leave or rejoin the system according to their wish.&lt;/li&gt;
  &lt;li&gt;Any needed rules and incentives can be enforced with this consensus mechanism.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://bitcoin.org/bitcoin.pdf&quot; target=&quot;_blank&quot;&gt;Bitcoin: A Peer-to-Peer Electronic Cash System&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Dec 2017 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2017/12/21/bitcoin-satoshi-nakamoto/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2017/12/21/bitcoin-satoshi-nakamoto/</guid>
        
        <category>concept</category>
        
        <category>references</category>
        
        <category>papers</category>
        
        
      </item>
    
      <item>
        <title>Congruence and Modular Arithmetic</title>
        <description>&lt;h3 id=&quot;what-is-mathematics-series&quot;&gt;What is Mathematics Series&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;/2017/10/27/number-system/&quot;&gt;What is Number System ?&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/10/28/mathematical-induction/&quot;&gt;Mathematical Induction&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/11/01/prime-numbers/&quot;&gt;The Mysterious Primes&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/12/20/congruence-and-modulo/&quot;&gt;Congruence and Modular Arithmetic&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;horizontal-divider&quot;&gt;Â· Â· Â·&lt;/div&gt;

&lt;h3 id=&quot;congruence&quot;&gt;Congruence&lt;/h3&gt;

&lt;p&gt;Two integers \(a\) and \(b\) are &lt;strong&gt;congruent modulo \(d\)&lt;/strong&gt;, where \(d\) is a fixed integer, if \(a\) and \(b\) leave same remainder on division by \(d\), i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a-b = nd \tag{1} \label{1}&lt;/script&gt;

&lt;p&gt;It is denoted by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a\equiv b\pmod d \tag{2} \label{2}&lt;/script&gt;

&lt;p&gt;Following defination of congruences are equivalent:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(a\) is congruent to \(b\) modulo \(d\).&lt;/li&gt;
  &lt;li&gt;\(a = b + nd\) for some integer n.&lt;/li&gt;
  &lt;li&gt;\(d\) divides \(a-b\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;properties-and-proof&quot;&gt;Properties and Proof&lt;/h3&gt;

&lt;p&gt;Congruence with respect to a fixed modulus has many of the formal properties of ordinary equality.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(a\equiv a\pmod d\)&lt;/li&gt;
  &lt;li&gt;If \(a\equiv b\pmod d\), then \(b\equiv a\pmod d\)&lt;/li&gt;
  &lt;li&gt;If \(a\equiv b\pmod d\) and \(b\equiv c\pmod d\), then \(a\equiv c\pmod d\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If \(a\equiv aâ€™\pmod d\) and \(b\equiv bâ€™\pmod d\), then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a + b\equiv a' + b'\pmod d \tag{3} \label{3}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a - b\equiv a' - b'\pmod d \tag{4} \label{4}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ab\equiv a'b'\pmod d \tag{5} \label{5}&lt;/script&gt;

&lt;p&gt;Say,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = a' + rd&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b = b' + sd&lt;/script&gt;

&lt;p&gt;then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a + b = a' + b' + (r+s)d&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a - b = a' - b' + (r-s)d&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ab = a'b' + (a's + b'r +rsd)d&lt;/script&gt;

&lt;h3 id=&quot;geometric-interpretation&quot;&gt;Geometric Interpretation&lt;/h3&gt;

&lt;p&gt;Generally, integers are represented geometrically using a number line, where a segment of unit length is chosen and multiplied in either directions to represent negative or positive integers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-12-20-congruence-and-modulo/fig-1-geometric-representation.svg?raw=true&quot; alt=&quot;Geometric Representation of Congruence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But, when an integer modulo \(d\) is considered, the magnitude is insignificant as long as the behavior on division by \(d\) is same (i.e. they leave the same remainder on division by \(d\)). This is geometrically represented using a circle divided into d equal parts. This is because any integer divided by \(d\) leaves as remainder one of the \(d\) numbers \(0, 1, \cdots, d-1\) which are placed at equal distances on the circumference of the circle. Every integer is congruent modulo \(d\) to one of these numbers and hence can be represented by one of these points. (&lt;strong&gt;Two numbers are congruent if they occur at the same point the circle.&lt;/strong&gt;)&lt;/p&gt;

&lt;h3 id=&quot;application-of-congruence-properties&quot;&gt;Application of Congruence Properties&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The test for divisibility, generally taught in elementary school, is a direct result of the properties of congruence operation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For example,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10 \equiv -1 \pmod{11}&lt;/script&gt;

&lt;p&gt;since \(10 = -1 + 11\). Successively multiplying this congruence, using \eqref{5}, we obtain,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10^2 \equiv (-1)(-1) = 1 \pmod{11}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10^3 \equiv = -1 \pmod{11}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10^4 \equiv = 1 \pmod{11}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vdots&lt;/script&gt;

&lt;p&gt;So using \eqref{3} and \eqref{5}, it can be shown that any two number, z and t of the form shown below will leave the same remainder when divided by 11.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z = a_0 + a_1\cdot 10 + a_2 \cdot 10^2 + \cdots + a_n \cdot 10^n \tag{6} \label{6}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;t = a_0 - a_1 + a_2 - \cdots + (-1)^n \cdot a_n \tag{7} \label{7}&lt;/script&gt;

&lt;p&gt;Here, \(z\) is the format of any integer to the base 10. Hence, a number is divisible by 11 (i.e. leaves a remainder 0), if and only if \(t\) is divisible by 11 (which in \eqref{7} basically means that &lt;strong&gt;the difference of the sum of all the odd digits and even digits together should be divisible by 11&lt;/strong&gt;, including 0.)&lt;/p&gt;

&lt;p&gt;It can be observed that while such patterns  are easier for numbers like 3, 9, 11, they are not easy to remember for other numbers like 7 and 11, as shown below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;1 \equiv 1 \pmod{13}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10 \equiv -3 \pmod{13}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10^2 \equiv -4 \pmod{13}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10^3 \equiv -1 \pmod{13}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10^4 \equiv 3 \pmod{13}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10^5 \equiv 4 \pmod{13}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;10^6 \equiv 1 \pmod{13}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;t = a_0 - 3 \cdot a_1 - 4 \cdot a_2 - a_3 + 3 \cdot a_4 + 4 \cdot a_5 + a_6 - \cdots \tag{8} \label{8}&lt;/script&gt;

&lt;p&gt;From above we reach the result that any number \(z\) in \eqref{6} is divisible by 13 if and only if \(t\) of the form \eqref{8} is divisible by 13 (&lt;strong&gt;clearly not an easy one to remember :P&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;Using a similar approach one can deduce the divisibility rule for any other integer.&lt;/p&gt;

&lt;h3 id=&quot;other-properties&quot;&gt;Other Properties&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;\(ab\equiv 0 \pmod d\) only if either \(a\equiv 0 \pmod d\) or \(b\equiv 0 \pmod d\). Property only holds if \(d\) is a prime number. If \(d\) was a composite, there exist numbers \(a \lt d\) and \(b \lt d\), such that,&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d = a\cdot b&lt;/script&gt;

&lt;p&gt;Where,
&lt;script type=&quot;math/tex&quot;&gt;\require{cancel}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a \cancel{\equiv} 0 \pmod d \text{ and } b \cancel{\equiv} 0 \pmod d&lt;/script&gt;

&lt;p&gt;But,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a \cdot b = d \equiv 0 \pmod d&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Law of Cancellation&lt;/strong&gt;: With respect to a prime modulus, if \(ab \equiv ac\) and \(a \cancel{\equiv} 0\), then \(b \equiv c\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://drive.google.com/open?id=0BxedRvE84NXkSy1sdzJKNDlHZGM&quot; target=&quot;_blank&quot;&gt;What is Mathematics? Second Edition - Chapter I: Natural Numbers&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Dec 2017 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2017/12/20/congruence-and-modulo/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2017/12/20/congruence-and-modulo/</guid>
        
        <category>mathematics</category>
        
        <category>number system</category>
        
        <category>what is mathematics</category>
        
        
      </item>
    
      <item>
        <title>The Mysterious Primes</title>
        <description>&lt;h3 id=&quot;what-is-mathematics-series&quot;&gt;What is Mathematics Series&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;/2017/10/27/number-system/&quot;&gt;What is Number System ?&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/10/28/mathematical-induction/&quot;&gt;Mathematical Induction&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/11/01/prime-numbers/&quot;&gt;The Mysterious Primes&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/12/20/congruence-and-modulo/&quot;&gt;Congruence and Modular Arithmetic&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;horizontal-divider&quot;&gt;Â· Â· Â·&lt;/div&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In mathematics, most statements in number theory are concerned not with a single object, but with a whole class of objects that have a common property, such as the class of all even integers etc.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mathematics is the queen of sciences and the theory of numbers is the queen of the mathematics.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One such class of number, called the &lt;strong&gt;prime numbers (or primes)&lt;/strong&gt; are of fundamental importance.&lt;/p&gt;

&lt;h3 id=&quot;the-prime-numbers&quot;&gt;The Prime Numbers&lt;/h3&gt;

&lt;p&gt;Most numbers can be resolved into smaller factors, but the ones that cannot be resolved are known as prime numbers or primes.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A prime is an integer \(p\), greater than one, which has no factors other than itself and one.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A integer \(a\) is a factor or divisor of integer \(b\), if there exists an integer \(c\) such that \(b=ac\). The numbers \(2, 3, 5, 7, \cdots \) are primes.&lt;/p&gt;

&lt;p&gt;So effectively, every integer can be expressed as a product of primes. And a number which not prime (other than 0 and 1) is called &lt;strong&gt;composite&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The set of all primes is a infinite set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The infinitude of primes is proved by contradiction. Say there exists only a finite number of primes, \(n\) and represented by set \({p_1, p_2, \cdots p_n}\). Then according to the assumption any number greater than numbers in the finite set must be composite. But it is possible to come up with a number, \(A\) greater than all these \(n\) primes given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A = p_1p_2\cdots p_n + 1 \tag{1}&lt;/script&gt;

&lt;p&gt;This contracdicts the assumption of finite set of primes and hence its proved that there is a infinite set of primes.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Every integer N greater than 1 can be factored into a product of primes in only one way.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The proof of this can be achieved by contradiction once again. If there exists a positive prime integer capable of decomposition into two essentially different products of primes, there will be a smallest such integer (using the principle of smallest integer), given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m = p_1p_2 \cdots p_r = q_1q_2 \cdots q_s \tag{2} \label{2}&lt;/script&gt;

&lt;p&gt;where \(pâ€™s\) and \(qâ€™s\) are primes. By rearranging if necessary, its possible to get the following order,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_1 \leq p_2 \leq \cdots \leq p_r \text{, and } q_1 \leq q_2 \leq \cdots \leq q_s \tag{3} \label{3}&lt;/script&gt;

&lt;p&gt;Where, \(p_1 \ne q_1\) because then one could cancel them in \eqref{2} and come up with another number smaller than \(m\) with two distinct prime factorization which would contradict the priniciple of smallest integer.&lt;/p&gt;

&lt;p&gt;So, either \(p_1 \lt q_1\) or \(q_1 \lt p_1\).&lt;/p&gt;

&lt;p&gt;Let, \(p_1 \lt q_1\) (if \(q_1 \lt p_1\), interchange the sequences in the analysis presented). Let \(mâ€™\) be an integer given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m' = m - (p_1q_2\cdots q_s) \tag{4} \label{4}&lt;/script&gt;

&lt;p&gt;Substituting, for \(m\) in \eqref{4} using \eqref{2},&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m' = (p_1p_2 \cdots p_r) - (p_1q_2\cdots q_s) = p_1(p_2 \cdots p_r - q_2\cdots q_s) \tag{5} \label{5}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m' = (q_1q_2 \cdots q_s) - (p_1q_2\cdots q_s) = (q_1 - p_1)(q_2 \cdots q_s) \tag{6} \label{6}&lt;/script&gt;

&lt;p&gt;Since \(q_1 \gt p_1\), it follows from \eqref{6} that \(mâ€™\) is a positive integer, while from \eqref{4} it follows that \(mâ€™ \lt m\). Hence the prime factorization of \(mâ€™\) must be unique.&lt;/p&gt;

&lt;p&gt;From \eqref{5} \(p_1\) is a factor of \(mâ€™\), so in \eqref{6}, \(p_1\) must be a factor of either \((q_1 - p_1)\) or \(q_2 \cdots q_s\). Since \(p_1 \lt q_1\) and \eqref{3}, \(p_1\) cannot be a factor of any of the \(qâ€™s\) nor equal to them. So, \(p_1\) must be a factor of \(q_1 - p_1\). So there exists an integer \(h\), such that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_1 - p_1 = p_1 \cdot h \text{ or } q_1 = p_1(h+1) \tag{7} \label{7}&lt;/script&gt;

&lt;p&gt;But, \eqref{7} is a contradicts the fact that \(q_1\) is a prime number. This points to the fact that the initial assumption of a number having two distinct prime factorizations must be wrong.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If a prime \(p\), is a factor of the product \(ab\), then \(p\) must be a factor of either \(a\) or \(b\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Also, if prime factorization of \(a\) can be expressed as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = p_1^{\alpha_1} p_2^{\alpha_2} \cdots p_r^{\alpha_r} \tag{8} \label{8}&lt;/script&gt;

&lt;p&gt;where \(pâ€™s\) are distinct primes, each raised to a certain power. The number of different divisors of \(a\) (including \(a\) and 1) is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\alpha_1 + 1)(\alpha_2 + 1) \cdots (\alpha_r + 1) \tag{9} \label{9}&lt;/script&gt;

&lt;p&gt;and all the divisors of the number \(a\) are given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b = p_1^{\beta_1} p_2^{\beta_2} \cdots p_r^{\beta_r} \tag{10} \label{10}&lt;/script&gt;

&lt;p&gt;where \(\betaâ€™s\) are integers satisfying inequalities,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 \leq \beta_1 \leq \alpha_1,\,0 \leq \beta_2 \leq \alpha_2,\, \cdots 0 \leq \beta_r \leq \alpha_r&lt;/script&gt;

&lt;h3 id=&quot;distribution-of-primes&quot;&gt;Distribution of Primes&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;In mathematics, the sieve of Eratosthenes is a simple, ancient algorithm for finding all prime numbers up to any given limit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Attempts have been made at finding simple arithmetic formulas that yield only primes, even though they may not give all of them.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Fermatâ€™s Conjecture&lt;/strong&gt; : All number of the form \(F(n) = 2^{2^n} +1\) are primes, but was proved incorrect by Euler who discovered that \(2^{2^5} + = 641 \cdot 6700417\) meaning \(F(5)\) is not a prime.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Any of the numbers \(F(n)\) are not prime for \(n&amp;gt;4\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are various other similar expressions which produce primes until a certain limit.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;\(f(n) = n^2 - n + 41\): For limit \(n &amp;lt; 41\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(f(n) = n^2 - 79n + 1601\): For limit \(n &amp;lt; 80\)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Itâ€™s been a futile effort to seek equation of a simple type which produces only primes. Even less promising is the attempt to find an algebraic formula that yields all the primes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;primes-in-arithmetic-progression&quot;&gt;Primes in Arithmetic Progression&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;In each arithmetic progression, \(a,\, a+d,\, a+2d,\, \cdots a+nd,\, \cdots\), where \(a\) and \(d\) have no common factors, there are infinitely many primes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The proof to this theorem could not be cracked for several years until finally it was presented by Lejeune Dirichlet and even after a hundred years itâ€™s not been simplified any further and would be a series of posts in itself if presented here.&lt;/p&gt;

&lt;p&gt;Though a generalized Euclidâ€™s proof of infinitude of primes can be used to cover some special arithmetic progression such as \(4n+3\) and \(6n + 5\).&lt;/p&gt;

&lt;h3 id=&quot;proof-of-infinitude-of-arithmetic-progressions-4n3&quot;&gt;Proof of Infinitude of Arithmetic Progressions: \(4n+3\)&lt;/h3&gt;

&lt;p&gt;Any prime greater than 2 must be a odd prime and hence must be of the form \(4n+1\) or \(4n+3\) for some integer \(n\).&lt;/p&gt;

&lt;p&gt;Also, the product of two numbers of the form \(4n+1\) is again of the same form, since&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(4a+1)(4b+1) = 16ab + 4a + 4b + 1 = 4(4ab + a + b) + 1 \tag{11} \label{11}&lt;/script&gt;

&lt;p&gt;Let there be a finite number of primes in this arithmetic progression given by, \(p_1,\, p_2,\, \cdots p_n\) of the form \(4n+3\).&lt;/p&gt;

&lt;p&gt;Consider the number,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N = 4(p_1p_2 \cdots p_n) - 1 = 4(p_1 \cdots p_n - 1) + 3 \tag{12} \label{12}&lt;/script&gt;

&lt;p&gt;From \eqref{12}, either N has to be a prime or it can be decomposed into a product of primes, none of which can be \(p_1,\, p_2,\, \cdots p_n\) because they leave a remainder \(-1\) on dividing \(N\).&lt;/p&gt;

&lt;p&gt;Also, all of the primes must not be of the form \(4n+1\) because the form of \(N\) is \(4n+3\) and \eqref{11}.&lt;/p&gt;

&lt;p&gt;Hence, one of the factors must of the form \(4n+3\), which is impossible, since none of the \(pâ€™s\), which was assumed to be the finite set of primes of the form \(4n+3\), can be a factor of \(N\) (using \eqref{12}).&lt;/p&gt;

&lt;p&gt;So, the initial assumption that number of primes in arithmetic progression of the form \(4n+3\) is finite is incorrect because of the contradiction encountered.&lt;/p&gt;

&lt;h3 id=&quot;proof-of-infinitude-of-arithmetic-progressions-6n5&quot;&gt;Proof of Infinitude of Arithmetic Progressions: \(6n+5\)&lt;/h3&gt;

&lt;p&gt;Any prime greater than 2 must be a odd prime and hence must be of the form \(6n+1\) or \(6n+3\) or \(6n+5\) for some integer \(n\).&lt;/p&gt;

&lt;p&gt;Let there be a finite number of primes in this arithmetic progression given by, \(p_1,\, p_2,\, \cdots p_n\) of the form \(6n+5\).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(6a+1)(6b+1) = 36ab + 6a + 6b + 1 = 6(6ab + a + b) + 1 \tag{13} \label{13}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(6a+3)(6b+3) = 36ab + 18a + 18b + 9 = 6(6ab + 3a + 3b + 1) + 3 \tag{14} \label{14}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(6a+3)(6b+1) = 36ab + 6a + 18b + 3 = 6(6ab + a + 3b) + 3 \tag{15} \label{15}&lt;/script&gt;

&lt;p&gt;From \eqref{13}, \eqref{14} and \eqref{15}, a number of the form \(6n+5\) cannot be achieved using only primes of the form \(6n+1\) and \(6n+3\).&lt;/p&gt;

&lt;p&gt;Consider the number,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N = 6(p_1p_2 \cdots p_n) - 1 = 6(p_1 \cdots p_n - 1) + 5 \tag{16} \label{16}&lt;/script&gt;

&lt;p&gt;From \eqref{16}, either N has to be a prime or it can be decomposed into a product of primes, none of which can be \(p_1,\, p_2,\, \cdots p_n\) because they leave a remainder \(-1\) on dividing \(N\).&lt;/p&gt;

&lt;p&gt;Also, all of the primes must not be only of the form \(6n+1\) and \(6n+3\) because the form of \(N\) is \(6n+5\) and \eqref{13}, \eqref{14} and \eqref{15} show it cannot be achieved otherwise.&lt;/p&gt;

&lt;p&gt;Hence, one of the factors must of the form \(6n+5\), which is impossible, since none of the \(pâ€™s\), which was assumed to be the finite set of primes of the form \(6n+5\), can be a factor of \(N\) (using \eqref{16}).&lt;/p&gt;

&lt;p&gt;So, the initial assumption that number of primes in arithmetic progression of the form \(6n+5\) is finite is incorrect because of the contradiction encountered.&lt;/p&gt;

&lt;h3 id=&quot;the-prime-number-theorem&quot;&gt;The Prime Number Theorem&lt;/h3&gt;

&lt;p&gt;Let \(A_n\) denote the number of primes among the integers \(1, 2, 3, \cdots , n\).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The density of primes among first n integers is given by the ratio \({A_n \over n} \).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Prime number theorem describes the asymptotic distribution of the primes among positive integers. It states that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{n \to \infty} {A_n/n \over 1 / log\,n} = 1 \tag{17}&lt;/script&gt;

&lt;h3 id=&quot;unsolved-problems-concerning-primes&quot;&gt;Unsolved Problems Concerning Primes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Goldbach Conjecture:&lt;/strong&gt; Goldbachâ€™s conjecture is one of the oldest and best-known unsolved problems in number theory and all of mathematics. It states: Every even integer greater than 2 can be expressed as the sum of two primes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Polignacâ€™s conjecture&lt;/strong&gt;: Twin prime conjecture, also known as Polignacâ€™s conjecture, in number theory, assertion that there are infinitely many twin primes, or pairs of primes that differ by 2.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are simple to experiment with, but mathematically some of the most mysterious problems. These are the properties of the number system that shows that a lot is still left to be discovered about the number system.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://drive.google.com/open?id=0BxedRvE84NXkSy1sdzJKNDlHZGM&quot; target=&quot;_blank&quot;&gt;What is Mathematics? Second Edition - Chapter I: Natural Numbers&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Nov 2017 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2017/11/01/prime-numbers/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2017/11/01/prime-numbers/</guid>
        
        <category>mathematics</category>
        
        <category>number system</category>
        
        <category>what is mathematics</category>
        
        
      </item>
    
      <item>
        <title>Mathematical Induction</title>
        <description>&lt;h3 id=&quot;what-is-mathematics-series&quot;&gt;What is Mathematics Series&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;/2017/10/27/number-system/&quot;&gt;What is Number System ?&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/10/28/mathematical-induction/&quot;&gt;Mathematical Induction&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/11/01/prime-numbers/&quot;&gt;The Mysterious Primes&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/12/20/congruence-and-modulo/&quot;&gt;Congruence and Modular Arithmetic&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;horizontal-divider&quot;&gt;Â· Â· Â·&lt;/div&gt;

&lt;h3 id=&quot;why-mathematical-induction&quot;&gt;Why Mathematical Induction?&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;There are infinitely many integers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The step-by-step procedure of passing from \(n\) to \(n+1\) which generates the infinite sequence of integers forms the basis of one of the most fundamental patterns of mathematical reasoning, the principle of mathematical induction.&lt;/p&gt;

&lt;p&gt;Mathematical Induction is used to establish the truth of a mathematical theorem for an infinite sequence of cases.&lt;/p&gt;

&lt;p&gt;There are two steps in proving a theorem, \(A\) by mathematical induction:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The first statement \(A_1\) must be true.&lt;/li&gt;
  &lt;li&gt;If a statement \(A_r\) is true then the statement \(A_{r+1}\) should be true too.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That these two conditions are sufficient to establish the truth of all the statements, \(A_1, A_2, \cdots \) is a logical principle which is as fundamental to mathematics as are the classical rules of Aristotelian logic.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Suppose that a) by some mathematical argument it is shown that if \(r\) is any integer and if assertion \(A_r\) is known to be true then the truth of assertion \(A_{r+1}\) will follow, and that b) the first proposition \(A_1\) is known to be true. Then all the propositions of the sequence must be true, and A is proved.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The principle of mathematical induction rests on the fact that after any integer \(r\) there is a next, \(r+1\), and that any desired integer \(n\) may be reached by a finite number of such steps, starting from the integer 1.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Although the principle of mathematical induction suffices to prove a theorem or formula once it is expressed, the proof gives no indication of how this formula was arrived at in the first place. So, it should be more fittingly called a verification.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;proof-for-arithmetic-progression&quot;&gt;Proof for Arithmetic Progression&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;For every value of n, the sum \(1 + 2 + \cdots + n\) of the first n integers is equal to \(\frac {n(n+1)} {2} \)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For any \(r\) is given by assertion \(A_r\) is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;1 + 2 + 3 + \cdots + r = \frac {r(r+1)} {2} \label{1} \tag{1}&lt;/script&gt;

&lt;p&gt;Adding \((r+1)\) to both LHS and RHS of \eqref{1},&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
1 + 2 + 3 + \cdots + r + (r+1) &amp;= \frac {r(r+1)} {2} + (r+1) \\
                               &amp;= \frac {r(r+1) + 2(r+1)} {2} \\
                               &amp;= \frac {(r+1)((r+1) + 1)} {2}
                               \label{2} \tag{2}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;\eqref{2} is nothing but assertion, \(A_{r+1}\).&lt;/p&gt;

&lt;p&gt;Also for \(r=1\), assertion \(A_1\) is true because, from \eqref{1},&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;1 = \frac {1(1+1)} {2} \tag{3} \label{3}&lt;/script&gt;

&lt;p&gt;So, using \eqref{2} and \eqref{3}, mathematical induction proves that the assertion in \eqref{1} holds for all positive integers, \(n\).&lt;/p&gt;

&lt;h3 id=&quot;proof-for-geometric-progression&quot;&gt;Proof for Geometric Progression&lt;/h3&gt;

&lt;p&gt;The theorem states that, for every value of \(n\),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_n = a + aq + aq^2 + \cdots + aq^n = a \frac{1 - q^{n+1}} {1-q} \label{4} \tag{4}&lt;/script&gt;

&lt;p&gt;The assertion holds true for \(n=1\), because,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
G_1 = a + aq &amp;= a \frac{1 - q^{2}} {1-q} \\
             &amp;= a \frac{(1-q) (1+q)} {1-q} \\
             &amp;= a (1+q)  \\
    \tag{5} \label{5}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Letâ€™s assume \(G_r\) is true, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_r = a + aq + aq^2 + \cdots + aq^r = a \frac{1 - q^{r+1}} {1-q} \label{6} \tag{6}&lt;/script&gt;

&lt;p&gt;Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
G_{r+1} &amp;= (a + aq + aq^2 + \cdots + aq^r) + aq^{r+1} \\
        &amp;= a \frac{1 - q^{r+1}} {1-q} + aq^{r+1} \\
        &amp;= \frac{a - aq^{r+1} + aq^{r+1} - aq^{r+2}} {1-q} \\
        &amp;= a \frac{1 - q^{(r+1) + 1}} {1-q} \\
        \label{7} \tag{7}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;But \eqref{7} is precisely the assertion \eqref{4} for the case \(n=r+1\). Hence, using \eqref{5} and \eqref{7} the assertion \eqref{4} is proved by mathematical induction.&lt;/p&gt;

&lt;h3 id=&quot;proof-for-sum-of-first-n-squares&quot;&gt;Proof for Sum of First n Squares&lt;/h3&gt;

&lt;p&gt;The theorem states that, for every value of \(n\),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_n = 1^2 + 2^2 + \cdots + n^2 = \frac {n(n+1)(2n+1)} {6} \tag{8} \label{8}&lt;/script&gt;

&lt;p&gt;The assertion holds true for \(n=1\), because,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_1 = 1^2 = \frac{1(1+1)(2(1)+1)} {6} = 1 \tag{9} \label{9}&lt;/script&gt;

&lt;p&gt;Letâ€™s assume \(A_r\) is true, i.e.,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_r = 1^2 + 2^2 + \cdots + r^2 = \frac {r(r+1)(2r+1)} {6} \tag{10} \label{10}&lt;/script&gt;

&lt;p&gt;Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
A_{r+1} = (1^2 + 2^2 + \cdots + r^2) + (r+1)^2 &amp;= \frac {r(r+1)(2r+1)} {6} + (r+1)^2 \\
&amp;= (r+1)\frac{r(2r+1) + 6(r+1)} {6} \\
&amp;= (r+1) \frac {2r^2 + 7r + 6} {6} \\
&amp;= \frac{(r+1) (r+2) (2r+3)} {6} \\
\label{11} \tag{11}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is precisely the assertion \eqref{8} for \(n = r+1\). So, using \eqref{9} and \eqref{11}, mathematical induction proves the assertion.&lt;/p&gt;

&lt;h3 id=&quot;proof-for-sum-of-first-n-cubes&quot;&gt;Proof for Sum of First n Cubes&lt;/h3&gt;

&lt;p&gt;The theorem states that, for every value of \(n\),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_n = 1^3 + 2^3 + \cdots + n^3 = \left[\frac {n(n+1)} {2}\right]^2 \tag{12} \label{12}&lt;/script&gt;

&lt;p&gt;The assertion holds true for \(n=1\), because,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_1 = 1^3 = \left[\frac {1(1+1)} {2}\right]^2 = 1^2 \tag{13} \label{13}&lt;/script&gt;

&lt;p&gt;Letâ€™s assume \(A_r\) is true, i.e.,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_r = 1^3 + 2^3 + \cdots + r^3 = \left[\frac {r(r+1)} {2}\right]^2 \tag{14} \label{14}&lt;/script&gt;

&lt;p&gt;Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
A_{r+1} = (1^3 + 2^3 + \cdots + r^3 ) + (r+1)^3 &amp;= \left[\frac {r(r+1)} {2}\right]^2 + (r+1)^3 \\
&amp;= (r+1)^2\frac{r^2 + 4r + 4} {4} \\
&amp;= (r+1)^2 \frac {(r+2)^2} {2^2} \\
&amp;= \left[\frac {(r+1)(r+2)} {2}\right]^2
\label{15} \tag{15}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is precisely the assertion \eqref{12} for \(n = r+1\). So, using \eqref{13} and \eqref{15}, mathematical induction proves the assertion.&lt;/p&gt;

&lt;h3 id=&quot;proof-for-bernoullis-inequality&quot;&gt;Proof for Bernoulliâ€™s Inequality&lt;/h3&gt;

&lt;p&gt;The assertion, \(A_n \) states that, for every value of \(n\),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1+p)^n \geq 1+np \text{, where } p&gt;-1\tag{16} \label{16}&lt;/script&gt;

&lt;p&gt;The assertion holds true for \(n=1\), because,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1+p)^1 \geq 1+p \tag{17} \label{17}&lt;/script&gt;

&lt;p&gt;Letâ€™s assume \(A_r\) is true, i.e.,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1+p)^r \geq 1+rp \tag{18} \label{18}&lt;/script&gt;

&lt;p&gt;Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
(1+p)^{r+1} = (1+p)^r \cdot (1+p) &amp;\geq (1+rp)(1+p) \\
&amp;\geq 1 + rp + p + rp^2 \\
&amp;\geq 1+(r+1)p \text{, because } rp^2 \gt 0 \\
\label{19} \tag{19}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is precisely the assertion \eqref{16} for \(n = r+1\). So, using \eqref{17} and \eqref{19}, mathematical induction proves the assertion.&lt;/p&gt;

&lt;p&gt;If \(p \lt -1\), then \((1+p)\) is negative and the inequality in \eqref{19} would be reversed. So, the restriction introduced in \eqref{16} is essential.&lt;/p&gt;

&lt;h3 id=&quot;proof-of-binomial-theorem&quot;&gt;Proof of Binomial Theorem&lt;/h3&gt;

&lt;p&gt;The assertion, \(C_i^n \) states that, for every value of \(n\),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_i^n = \frac{n(n-1) \cdots (n-i+1) } {1 \cdot 2 \cdots i} = \frac {n!} {i!(n-i)!} \text{, for } i \in \{0, 1, \cdots, n\} \tag{20} \label{20}&lt;/script&gt;

&lt;p&gt;The assertion holds true for \(n=1\), because,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_i^1 = \frac{1!} {i!(1-i)!} = 1 \text{, for } i \in \{0, 1\} \tag{21} \label{21}&lt;/script&gt;

&lt;p&gt;which is exactly the value for \(C_0^1 = C_1^1 = 1\) in Pascalâ€™s Triangle.&lt;/p&gt;

&lt;p&gt;Letâ€™s assume \(C_i^r\) is true, i.e.,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_i^r = \frac {r!} {i!(r-i)!} \text{, for } i \in \{0, 1, \cdots, r\} \tag{22} \label{22}&lt;/script&gt;

&lt;p&gt;Then using the relation, \(C_i^{r+1} = C_{i-1}^{r} + C_i^{r} \text{, } C_i^{r+1}\) is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
C_i^{r+1} &amp;= C_{i-1}^{r} + C_i^{r} \\
&amp;= \frac{r!}{(i-1)!(r - i + 1)!} + \frac{r!}{i!(r-i)!} \\
&amp;= \frac{r!}{(i-1)!(r-i)!} \left[ \frac{1}{r-i+1} + \frac{1}{i} \right] \\
&amp;= \frac{r!}{(i-1)!(r-i)!} \left[ \frac{r+1}{i(r-i+1)} \right] \\
&amp;= \frac{(r+1)!}{(i)!((r+1)-i)!} \\
\tag{23} \label{23}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is precisely the assertion \eqref{20} for \(n = r+1\). So, using \eqref{21} and \eqref{23}, mathematical induction proves the assertion.&lt;/p&gt;

&lt;h3 id=&quot;generalized-mathematical-induction&quot;&gt;Generalized Mathematical Induction&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;If a sequence of statements \(A_s, A_{s+1}, \cdots\) is given where \(s\) is a some positive integer, and if &lt;br /&gt;
a) For every value \(r \geq s\), the truth of \(A_{r+1}\) will follow from the truth of \(A_r\),&lt;br /&gt;
b) \(A_s\) is known to be true, &lt;br /&gt;
then all the statements \(A_s, A_{s+1}, \cdots\) are true; i.e. \(A_n\) is true for all \(n \geq s\)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;proof-for-bernoullis-inequality-strict-version&quot;&gt;Proof for Bernoulliâ€™s Inequality (Strict version)&lt;/h3&gt;

&lt;p&gt;The assertion, \(A_n \) states that, for every value of \(n\),&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1+p)^n \gt 1+np \text{, for } p&gt;-1 \text{ and } p\ne0 \text{ and } n \geq 2\tag{24} \label{24}&lt;/script&gt;

&lt;p&gt;The assertion holds true for \(n=s=2\), because,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1+p)^2 = 1 + 2p + p^2 \gt 1+2p \text{, because } p \ne 0 \text{, so } p^2 \gt 0   \tag{25} \label{25}&lt;/script&gt;

&lt;p&gt;Letâ€™s assume \(A_r\) is true, i.e.,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(1+p)^r \gt 1+rp \tag{26} \label{26}&lt;/script&gt;

&lt;p&gt;Then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
(1+p)^{r+1} = (1+p)^r \cdot (1+p) &amp;\gt (1+rp)(1+p) \\
&amp;\gt 1 + rp + p + rp^2 \\
&amp;\gt 1+(r+1)p \text{, because } rp^2 \gt 0 \\
\label{27} \tag{27}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is precisely the assertion \eqref{24} for \(n = r+1\). So, using \eqref{25} and \eqref{27}, mathematical induction proves the assertion.&lt;/p&gt;

&lt;p&gt;If \(p \lt -1\), then \((1+p)\) is negative and the inequality in \eqref{19} would be reversed. So, the restriction introduced in \eqref{16} is essential.&lt;/p&gt;

&lt;h3 id=&quot;principle-of-smallest-integer&quot;&gt;Principle of Smallest Integer&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Every non-empty set \(C\) of positive integers has a smallest number. (Set may be finite or infinite.)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At first it seems like a trivial principle but it actually does not apply to many sets that are not integers, e.g. the set of positive fractions \(1, {1 \over 2} {1 \over 3} \cdots \) does not contain a smallest number.&lt;/p&gt;

&lt;h3 id=&quot;proof-for-mathematical-induction&quot;&gt;Proof for Mathematical Induction&lt;/h3&gt;

&lt;p&gt;The principle of smallest integer can be used to prove the principle of mathematical induction as a theorem.&lt;/p&gt;

&lt;p&gt;Let us consider any sequence of statements \(A_1, A_2, \cdots \) such that,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;For any integer \(r\) the truth of \(A_{r+1}\) will follow from that of \(A_r\)&lt;/li&gt;
  &lt;li&gt;\(A_1\) is known to be true.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;if 1 of the \(Aâ€™s\) were false, the set \(C\) of all positive integers \(n\) for which \(A_n\) is false would be non-empty. By the principle of smallest integer, \(C\) would have a smallest integer \(p\), which must be greater than 1 because of condition (2) in the theorem. Hence, \(A_p\) would be false, but \(A_{p-1}\) true, which contradicts condition (1) of the theorem. So, the assumption in the first place, that any one of the \(Aâ€™s\) is false is untenable.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mathematical Induction must be applied very carefully to prove an assertion because, it is often fallacious because of a misleading base case. A popular example of this is â€œAll number are equal fallacyâ€.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://drive.google.com/open?id=0BxedRvE84NXkSy1sdzJKNDlHZGM&quot; target=&quot;_blank&quot;&gt;What is Mathematics? Second Edition - Chapter I: Natural Numbers&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 28 Oct 2017 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2017/10/28/mathematical-induction/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2017/10/28/mathematical-induction/</guid>
        
        <category>mathematics</category>
        
        <category>number system</category>
        
        <category>what is mathematics</category>
        
        <category>theorems</category>
        
        
      </item>
    
      <item>
        <title>What is Number System ?</title>
        <description>&lt;h3 id=&quot;what-is-mathematics-series&quot;&gt;What is Mathematics Series&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;/2017/10/27/number-system/&quot;&gt;What is Number System ?&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/10/28/mathematical-induction/&quot;&gt;Mathematical Induction&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/11/01/prime-numbers/&quot;&gt;The Mysterious Primes&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/2017/12/20/congruence-and-modulo/&quot;&gt;Congruence and Modular Arithmetic&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;horizontal-divider&quot;&gt;Â· Â· Â·&lt;/div&gt;

&lt;h3 id=&quot;the-natural-numbers&quot;&gt;The Natural Numbers&lt;/h3&gt;

&lt;p&gt;The natural numbers were created by the human mind out of the necessity to count the objects around us. Most basics of mathematics are associated with association of numbers with tangible objects. But advanced mathematics is built on top of the &lt;strong&gt;abstract concept of number system&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;God created natural numbers; everything else is manâ€™s handiwork.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;laws-of-arithmetic&quot;&gt;Laws of Arithmetic&lt;/h3&gt;

&lt;p&gt;Natural numbers have only &lt;strong&gt;two basic operations&lt;/strong&gt;, namely, addition and multiplication. The mathematical theory of the natural numbers or positive integers is known as &lt;strong&gt;arithmetic&lt;/strong&gt;. Arithmetic is based on the fact that operations on numbers are governed by certain laws.&lt;/p&gt;

&lt;p&gt;(a, b, c â€¦ symbolically denote integers)&lt;/p&gt;

&lt;p&gt;The fundamental laws of arithmetic are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Commutative&lt;/strong&gt; law, i.e. \(a+b = b+a\) and \(ab = ba\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Associative&lt;/strong&gt; law, i.e. \(a + (b + c) = (a + b) + c\) and \(a(bc) = (ab)c\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Distributive&lt;/strong&gt; law, i.e. \(a(b+c) = ab + ac\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Addition and subtraction are &lt;strong&gt;inverse operations&lt;/strong&gt; because, \((a+d)-d = a\). Similarly, multiplication and division are inverse because \({a \over d} \cdot d = a\). Also, &lt;strong&gt;0 and 1 are the identities of operations addition and multiplication respectively&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;representation-of-integers&quot;&gt;Representation of Integers&lt;/h3&gt;

&lt;p&gt;A number system has a set of &lt;strong&gt;digit symbols and numbers&lt;/strong&gt; where digit symbols are used to denote the larger numbers not available directly in the set of digit symbols. Modern number systems are associated with the place values of individual digits in the number, such as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;372 = 3 \cdot 10^2 + 7 \cdot 10^1 + 2 \cdot 10^0&lt;/script&gt;

&lt;p&gt;These are called &lt;strong&gt;positional notations&lt;/strong&gt;. Here, a large number can also be represented using the basic symbols from the set of digit symbols. It is useful to have a way of indicating the result in a general form using a uniform logic, i.e. a general method for representing an integer, \(z\) in the decimal system is to express it as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z = a_n \cdot 10^n + a_{n-1} \cdot 10^{n-1} + \cdots + a_1 \cdot 10 + a_0&lt;/script&gt;

&lt;p&gt;which would be represented by the symbol \(a_na_{n-1}\cdots a_1a_0\)&lt;/p&gt;

&lt;p&gt;Specifically, in case of decimal system the base is 10 as can be seen in the equations above. But, in general, for a number system &lt;strong&gt;any number greater than 1 can serve as the base&lt;/strong&gt; of the system. E.g. a &lt;strong&gt;septimal&lt;/strong&gt; system has base 7 and an integer is expressed as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_n \cdot 7^n + b_{n-1} \cdot 7^{n-1} + \cdots + b_1 \cdot 7 + b_0&lt;/script&gt;

&lt;p&gt;and it would be represented by the symbol \(b_nb_{n-1}\cdots b_1b_0\)&lt;/p&gt;

&lt;p&gt;As a result 109 in decimal system is represented by 214 in septimal system for the reason shown below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2 \cdot 7^2 + 1 \cdot 7 + 4 = 109&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;As a general rule, in order to convert from base 10 to base B, perform successive divisions of number z by B; the remainders in reverse order would be the number representation in the system with base B.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Too small a base base has disadvantages (even &lt;strong&gt;a small number like 72 has a lengthy representation 1,001,111 in the dyadic system i.e. the binary system&lt;/strong&gt;), while a large base requires learning many digit symbols, and an extended multiplication table.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The duodecimal system, i.e., choice of base 12 is advocated in many places, because it is exactly divisible by two, three, four, and six. Due to this property, works involving division and fraction are simplified.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Early systems of numerations were not positional in nature, but were based on purely additive principle. E.g. in roman symbolism,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;CXVIII = \text{one hundred} + \text{ten} + \text{five} + \text{one} + \text{one} + \text{one}&lt;/script&gt;

&lt;p&gt;i.e., the symbol \(CXVIII\) represents 118. Same was true about other early number systems like the Egyptian, Hebrew, Greek systems of numeration.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Disadvantage of any purely additive notation is that it requires more and more number of new symbols as the number to be represented gets larger. Also, the computation with additive systems is so difficult to perform, that computation was confined to a few adepts in the ancient times.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The positional system has the property that all numbers, however large or small, can be represented by the use of a small set of different digit symbols. Also, the major advantage lies in the &lt;strong&gt;ease of computation&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;computation-in-other-number-systems&quot;&gt;Computation in Other Number Systems&lt;/h3&gt;

&lt;p&gt;A very curious property of the decimal number system that points to usage of other number systems in the past is the number words used. E.g. the words for 11 and 12 are not constructed as the other teens are, suggesting a linguistic independence from words used for 10. Such peculiarities suggest remnants of use of other bases, notably 12 and 20.&lt;/p&gt;

&lt;p&gt;Words vingt and quartevingt used for 20 and 80 respectively in French suggest a possibility of number with &lt;strong&gt;base 20&lt;/strong&gt;. There are traces of Babylonian astronomers using a number system called &lt;strong&gt;sexagesimal&lt;/strong&gt; (base 60).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The tables for addition and multiplication change with number system while the rules of arithmetic remain the same.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Below is the &lt;strong&gt;table for addition&lt;/strong&gt; followed by &lt;strong&gt;table for multiplication&lt;/strong&gt; in duodecimal number system. (&lt;strong&gt;10 and 11 in the duodecimal representation below are represented using A and B respectively.&lt;/strong&gt;)&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;+&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;3&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;4&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;5&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;6&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;7&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;8&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;9&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;A&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1A&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;.&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;3&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;4&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;5&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;6&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;7&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;8&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;9&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;A&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1A&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;26&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;29&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;24&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;28&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;30&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;34&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;38&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;21&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;26&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;34&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;39&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;42&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;47&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;26&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;30&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;36&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;40&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;46&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;56&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;12&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;24&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;36&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;41&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;48&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;53&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;65&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;28&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;34&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;40&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;48&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;54&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;60&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;68&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;74&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;16&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;30&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;39&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;46&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;53&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;60&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;69&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;76&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;83&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;26&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;34&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;42&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;68&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;76&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;84&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;92&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;B&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1A&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;29&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;38&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;47&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;56&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;65&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;74&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;83&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;92&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;101&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://drive.google.com/open?id=0BxedRvE84NXkSy1sdzJKNDlHZGM&quot; target=&quot;_blank&quot;&gt;What is Mathematics? Second Edition - Chapter I: Natural Numbers&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 27 Oct 2017 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2017/10/27/number-system/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2017/10/27/number-system/</guid>
        
        <category>mathematics</category>
        
        <category>number system</category>
        
        <category>what is mathematics</category>
        
        
      </item>
    
      <item>
        <title>Neural Networks: Cost Function and Backpropagation</title>
        <description>&lt;h3 id=&quot;notation&quot;&gt;Notation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;\({(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots , (x^{(m)}, y^{(m)})}\) are the \(m\) training examples&lt;/li&gt;
  &lt;li&gt;L is the &lt;strong&gt;total number of the layers&lt;/strong&gt; in the network&lt;/li&gt;
  &lt;li&gt;\(s_l\) is the &lt;strong&gt;number of units (not counting the bias unit)&lt;/strong&gt; in the layer l&lt;/li&gt;
  &lt;li&gt;K is the &lt;strong&gt;number of units in the output layer&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-10-03-neural-networks-cost-function-and-back-propagation/fig-1-neural-network-notations.png?raw=true&quot; alt=&quot;Neural Network Notations&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For example in the network shown above,
    &lt;ul&gt;
      &lt;li&gt;L = 4&lt;/li&gt;
      &lt;li&gt;\(s_1 = 3,\, s_2 = 3,\, s_3 = 2,\, s_4 = 2\)&lt;/li&gt;
      &lt;li&gt;K = 2&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;One vs All method is only needed if number of classes is greater than 2, i.e. if \(K \gt 2\), otherwise only one output unit is sufficient to build the model.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;cost-function-of-neural-networks&quot;&gt;Cost Function of Neural Networks&lt;/h3&gt;
&lt;p&gt;Cost function of a neural network is a &lt;strong&gt;generalization of the cost function of the logistic regression&lt;/strong&gt;. The &lt;strong&gt;L2-Regularized&lt;/strong&gt; cost function of logistic regression from the post &lt;a href=&quot;/2017/09/15/regularized-logistic-regression/&quot; target=&quot;_blank&quot;&gt;Regularized Logistic Regression&lt;/a&gt;  is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = -{1 \over m} \sum_{i=1}^m \left( y^{(i)}\,log(h_\theta(x^{(i)})) + (1-y^{(i)})\,log(1 - h_\theta(x^{(i)})) \right) + {\lambda \over 2m } \sum_{j=1}^n \theta_j^2 \tag{1}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where
    &lt;ul&gt;
      &lt;li&gt;\({\lambda \over 2m } \sum_{j=1}^n \theta_j^2\) is the &lt;strong&gt;regularization term&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;\(\lambda\) is the &lt;strong&gt;regularization factor&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Extending (1) to then neural networks which can have K units in the output layer the cost function is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = -{1 \over m} \left[ \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)}\,log(h_\theta(x^{(i)}))_k + (1-y_k^{(i)})\,log(1 - (h_\theta(x^{(i)}))_k) \right] + {\lambda \over 2m } \sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} (\theta_{ji}^{(l)})^2 \tag{1}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where
    &lt;ul&gt;
      &lt;li&gt;\(h_\Theta(x) \in \mathbb{R}^K \)&lt;/li&gt;
      &lt;li&gt;\((h_\theta(x))_i\) is the \(i^{th}\) output&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here the summation term \(\sum_{k=1}^K\) is to &lt;strong&gt;generalize over the K output units&lt;/strong&gt; of the neural network by calculating the cost function and summing over all the output units in the network. Also following the convention in regularization, the &lt;strong&gt;bias term in skipped from the regularization penalty&lt;/strong&gt; in the cost function defination. Even if one includes the index 0, it would not effect the process in practice.&lt;/p&gt;

&lt;h3 id=&quot;backpropagation-algorithm&quot;&gt;Backpropagation Algorithm&lt;/h3&gt;
&lt;p&gt;Backpropagation algorithm is based on the &lt;strong&gt;repeated application of the error calculation&lt;/strong&gt; used for gradient descent similar to the regression techniques, and since it is repeatedly applied in the &lt;strong&gt;reverse order starting from output layer and continuing towards input layer&lt;/strong&gt; it is termed as backpropagation.&lt;/p&gt;

&lt;p&gt;For a network with L layers the computation during &lt;strong&gt;foward propagation&lt;/strong&gt;, for an input \((x, y)\) would be as follows,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    a^{(1)} &amp;= x \\
    z^{(2)} &amp;= \Theta^{(1)}\,a^{(1)} \\
    a^{(2)} &amp;= g(z^{(2)}) \\
    &amp; \vdots \\
    a^{(L-1)} &amp;= g(z^{(L-1)}) \\
    z^{(L)} &amp;= \Theta^{(L-1)}\,a^{(L-1)} \\
    a^{(L)} &amp;= h_\Theta(x) = g(z^{(L)})
  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The \(h_\Theta(x)\) is the prediction. In order to reduce the error between the prediction and the actual value backpropagation is used. Say, \(\delta_j^{(l)}\) is the &lt;strong&gt;error of node j in the layer l&lt;/strong&gt; is associated with the prediction made at that node given by \(a_j^{(l)}\), then backpropagation aims to calculate this error term propating backwards starting from the output unit in the last layer (layer L in the example above).&lt;/p&gt;

&lt;p&gt;So for each output unit in layer L, the error term is given by, \(\delta_j^{(L)} = a_j^{(L)} - y_j\) which can be vectorized and written as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta^{(L)} = a^{(L)} - y&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where \(a^{(L)}\) is \(h_\Theta(x)\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now the error terms for the previous layers are calculated as follows,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta^{(l)} = (\Theta^{(l)})^T\,\delta^{(l+1)} .* g'(z^{(l)})&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where
    &lt;ul&gt;
      &lt;li&gt;\(gâ€™(z^{(l)}) = a^{(l)} .* (1 - a^{(l)}) \)&lt;/li&gt;
      &lt;li&gt;.* is the element-wise multiplication.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;This backward propagation stops at l = 2 because l = 1 correponds to the input layer and no weights needs to be calculated there.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now the the gradient for the cost function which is needed for the minimization of the cost function is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac {\partial} {\partial \Theta_{ij}^{(l)} } = a_j^{(l)}\, \delta_i^{(l+1)}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where regularization is ignored for the simplicity of expression.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Summarizing&lt;/strong&gt; backpropagation:&lt;/p&gt;

&lt;p&gt;Given training set \({(x^{(1)}, y^{(1)}), \cdots, (x^{(m)}, y^{(m)})}\)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set \(\Delta_{ij}^{l} = 0\) for all (i, j, l)&lt;/li&gt;
  &lt;li&gt;For i = 1 to m:
    &lt;ul&gt;
      &lt;li&gt;Set \(a^{(1)} = x^{(i)} \)&lt;/li&gt;
      &lt;li&gt;Perform forward propagation to compute \(a^{(l)}\) for l = 1, â€¦, L&lt;/li&gt;
      &lt;li&gt;Using \(y^{(i)}\) compute \( \delta^{(L)} = a^{(L)} - y^{(i)} \)&lt;/li&gt;
      &lt;li&gt;Compute \(\delta^{(L-1)}, \cdots, \delta^{(2)}\) using backpropagation&lt;/li&gt;
      &lt;li&gt;\(\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)} \delta_i^{(l+1)} \)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Vectorized implementation of the equation above is given by,&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}\,a^{(l)}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;\(D_{ij}^{(l)} := {1 \over m} \Delta_{ij}^{(l)} + \lambda \, \Theta_{ij}^{(l)} \) if \(j \ne 0\)&lt;/li&gt;
  &lt;li&gt;\(D_{ij}^{(l)} := {1 \over m} \Delta_{ij}^{(l)} \) if \(j = 0\)&lt;/li&gt;
  &lt;li&gt;And finally, \( \frac {\partial} {\partial \Theta_{ij}^{(l)} } = D_{ij}^{(l)} \)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Formally, the &lt;strong&gt;\(\delta\) terms are the partial derivatives of the cost function&lt;/strong&gt; given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_j^{(l)} = \frac {\partial} {\partial z_j^{(l)}} cost(t)&lt;/script&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/na28E/cost-function&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Cost Function&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 03 Oct 2017 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/</guid>
        
        <category>machine learning</category>
        
        <category>andrew ng</category>
        
        
      </item>
    
      <item>
        <title>Neural Networks Intuition</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Neural networks can be used to build all types of function. This post tries to &lt;strong&gt;map functions of logical operations using the network&lt;/strong&gt;. How the parameters are derived in explained in the later posts.&lt;/p&gt;

&lt;h3 id=&quot;and-gate&quot;&gt;AND Gate&lt;/h3&gt;
&lt;p&gt;Using a single neuron, it is possible to achieve the approximation of an and gate. The architecture with the parameters can be seen below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-09-27-neural-network-intuition/fig-1-and-gate.png?raw=true&quot; alt=&quot;And Implementation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The hypothesis for the above network is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(x) = g(-30 + 20\,x_1 + 20\,x_2)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where g is the sigmoid function which &lt;strong&gt;asymptotes at 0 and 1&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The output table for the hypothesis above is given below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_1\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_2\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(h_\theta(x)\)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(-30) \approx 0\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(-10) \approx 0\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(-10) \approx 0\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(10) \approx 1\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The values of \(h_\theta(x)\) is nothing but the expected value of the AND gate.&lt;/p&gt;

&lt;h3 id=&quot;or-gate&quot;&gt;OR Gate&lt;/h3&gt;
&lt;p&gt;Similarly, using a single neuron, it is possible to achieve the approximation of an or gate. The architecture with the parameters can be seen below. It is same as AND gate but the bias weight is changed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-09-27-neural-network-intuition/fig-2-or-gate.png?raw=true&quot; alt=&quot;Or Implementation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The hypothesis for the above network is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(x) = g(-10 + 20\,x_1 + 20\,x_2)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where g is the sigmoid function which &lt;strong&gt;asymptotes at 0 and 1&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The output table for the hypothesis above is given below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_1\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_2\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(h_\theta(x)\)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(-10) \approx 0\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(10) \approx 1\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(10) \approx 1\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(30) \approx 1\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The values of \(h_\theta(x)\) is nothing but the expected value of the OR gate.&lt;/p&gt;

&lt;h3 id=&quot;not-gate&quot;&gt;NOT Gate&lt;/h3&gt;
&lt;p&gt;Unlike the previous two examples, NOT gate is a unary operator, but still simple weights can give easy implementation of the NOT gate. The architecture and the parameters are shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-09-27-neural-network-intuition/fig-3-not-gate.png?raw=true&quot; alt=&quot;Not Implementation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The hypothesis for the above network is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(x) = g(10 - 20\,x_1)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where g is the sigmoid function which &lt;strong&gt;asymptotes at 0 and 1&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The output table for the hypothesis above is given below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_1\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(h_\theta(x)\)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(10) \approx 1\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(-10) \approx 0\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The values of \(h_\theta(x)\) is nothing but the expected value of the NOT gate.&lt;/p&gt;

&lt;h3 id=&quot;not-x_1-and-not-x_2&quot;&gt;(NOT \(x_1\)) AND (NOT \(x_2\))&lt;/h3&gt;
&lt;p&gt;Unlike the previous examples, this operation does not look straight forward, but actually it is. Here is an architecture implementation of the above gate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-09-27-neural-network-intuition/fig-4-not-x1-and-not-x2.png?raw=true&quot; alt=&quot;Not x1 and Not x2 Implementation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The hypothesis for the above network is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(x) = g(10 - 20\,x_1 - 20\,x_2)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where g is the sigmoid function which &lt;strong&gt;asymptotes at 0 and 1&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The output table for the hypothesis above is given below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_1\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_2\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(h_\theta(x)\)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(10) \approx 1\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(-10) \approx 0\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(-10) \approx 0\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(g(-30) \approx 0\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The values of \(h_\theta(x)\) is nothing but the expected value of the (NOT \(x_1\)) AND (NOT \(x_2\)) operation.&lt;/p&gt;

&lt;h3 id=&quot;perceptron-limitation&quot;&gt;Perceptron Limitation&lt;/h3&gt;
&lt;p&gt;All the examples untill this one were linearly seperable and hence were solved using a single neuron. But and XOR Gate is not linearly seperable as was the case with AND and OR gates and this can be clearly seen in the plot below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-09-27-neural-network-intuition/fig-5-linearly-seperable.png?raw=true&quot; alt=&quot;Linearly Seperable&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is evident that there is no single straight line that can seperate the two classes in plot (c) and hence termed as &lt;strong&gt;not linearly seperable&lt;/strong&gt;. This is a major drawback of &lt;strong&gt;perceptron (single layer neural networks)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;There is a simple &lt;strong&gt;proof&lt;/strong&gt; for concluding that the XOR is not linearly seperable. Say, perceptron were capable of separating the two classes, then it would mean that there exists a set of weights (or parameters), \(\theta_0,\, \theta_1,\,\theta_2\) such that the hypothesis is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(x) = g(\theta_0\,x_0 + \theta_1\,x_1 + \theta_2\,x_2)&lt;/script&gt;

&lt;p&gt;Then the above hypothesis should satisfy the following truth table,&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_1\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_2\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_1\,XOR\,x_2\)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Substituting and equating to 0, the hypothesis, following inequalities are generated that would determine the decision boundary.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    \theta_0 \cdot 1 + 0 \cdot x_1 + 0 \cdot x_2 &amp;\lt 0 \\
    \theta_0 \cdot 1 + 0 \cdot x_1 + 1 \cdot x_2 &amp;\gt 0 \\
    \theta_0 \cdot 1 + 1 \cdot x_1 + 0 \cdot x_2 &amp;\gt 0 \\
    \theta_0 \cdot 1 + 1 \cdot x_1 + 1 \cdot x_2 &amp;\lt 0 \\
  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Substituting \(b = -\theta_0\), above inequalities can be written as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    b &amp;\gt 0 \\
    x_2 &amp;\gt b \\
    x_1 &amp;\gt b \\
    x_1 + x_2 &amp;\lt b \\
  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;It can be seen that the first three inequalities directly contradict the fourth one. Which means that the very first assumption made that there exist such parameters \(\theta_0,\, \theta_1,\,\theta_2\) was incorrect. Hence, &lt;strong&gt;the XOR gate is not linearly seperable&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is where the utility and finesse of multi-layer neural network in deriving intricate features from the input features can be seen in action.&lt;/p&gt;

&lt;h3 id=&quot;xnor-gate&quot;&gt;XNOR Gate&lt;/h3&gt;
&lt;p&gt;For simplicity, letâ€™s consider a XNOR gate which is nothing but the negation of an XOR gate. So, it would not be wrong to say that if XNOR gate is achieved, XOR gate is not very far. Consider the following neural network with one hidden layer and the given weights.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-09-27-neural-network-intuition/fig-6-xnor-gate.png?raw=true&quot; alt=&quot;XNOR&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here if weights are seen carefully, \(a_1^{(2)}\) is nothing but the AND gate and \(a_2^{(2)}\) is nothing but (NOT \(x_1\)) AND (NOT \(x_2\)). Similarly the output neuron is nothing but the OR gate. It calculates the following result table,&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_1\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(x_2\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(a_1^{(2)}\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(a_2^{(2)}\)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;\(h_\theta(x)\)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The values of \(h_\theta(x)\) is nothing but the expected value of the XNOR operation.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This gives the intuition that the hidden layers are calculating a more complex input which inturn helps to turn the problem into a linearly seperable one using the transformations. This is the main reason the neural networks are fairly powerful classifiers because as the depth (or number of hidden layers) of the neural network increases it can derive more and more complex features for the final layer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/rBZmG/examples-and-intuitions-i&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Examples and Intutions I&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;
&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/solUx/examples-and-intuitions-ii&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Examples and Intutions II&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 27 Sep 2017 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2017/09/27/neural-network-intuition/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2017/09/27/neural-network-intuition/</guid>
        
        <category>machine learning</category>
        
        <category>andrew ng</category>
        
        
      </item>
    
      <item>
        <title>Neural Networks Theory</title>
        <description>&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Neural networks were developed to &lt;strong&gt;mimic the way neurons in a brain work&lt;/strong&gt;. Typically, a neuron has input connections and output connections. So basically, &lt;strong&gt;neuron is a computational unit&lt;/strong&gt; which takes a set of inputs and produces output. So the basic functionality of neurons is tried to be replicated in these computational units. For example, neurons in brain interact with each other using electrical signals and are selectively activated based on certain parameters. This behaviour is transferred to a unit in neural networks using &lt;strong&gt;activation function&lt;/strong&gt; which would be explained later in the post.&lt;/p&gt;

&lt;h3 id=&quot;neuron-model-logistic-unit&quot;&gt;Neuron Model: Logistic Unit&lt;/h3&gt;
&lt;p&gt;Below is the representation of a basic &lt;strong&gt;logistic unit neuron model&lt;/strong&gt;,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-09-21-neural-networks/fig-1-logistic-unit.png?raw=true&quot; alt=&quot;Logistic Unit&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, \(x_0, \cdots, x_3\) are the input units connected to the neuron and can be considered &lt;strong&gt;similar to dendrites&lt;/strong&gt; getting signals from other neurons. \(h_\theta(x)\) is the activation function which basically decides whether or not should the neuron get activated or excited. The term &lt;strong&gt;\(x_0 = 1\), also called bias unit, plays an important role in the decision made for excitation of neuron.&lt;/strong&gt; In case of a logistic unit the activation function is a logistic function or sigmoid function i.e. the &lt;strong&gt;neuron has a sigmoid (logistic) activation function&lt;/strong&gt;. Sigmoid function is given by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(x) = \frac {1} {1 + e^{-x}} \tag{1}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Parameters of the model, \(\theta_0, \cdots, \theta_n\) are also sometimes called weights of the model and represented by \(w_1, \cdots, w_n\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;neural-network&quot;&gt;Neural Network&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A group of these neuron units together forms a neural network&lt;/strong&gt;. Below is a representation of neural network,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2017-09-21-neural-networks/fig-2-neural-network.png?raw=true&quot; alt=&quot;Neural Network&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Where
    &lt;ul&gt;
      &lt;li&gt;network has 3 layers&lt;/li&gt;
      &lt;li&gt;layer 1 is called &lt;strong&gt;input layer&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;layer 3 is called &lt;strong&gt;output layer&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;remaining layers are called &lt;strong&gt;hidden layers&lt;/strong&gt;. In this case there is only one hidden layer, but it is &lt;strong&gt;possible to have multiple hidden layers&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;\(x_0\) and \(a_0^{(2)}\) are the bias terms and equal 1 always. They are &lt;strong&gt;generally not counted when the number of units in a layer are calculated&lt;/strong&gt;. So, layer 1 and layer 2 have 3 units each.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Notations
    &lt;ul&gt;
      &lt;li&gt;\(a_i^{(j)}\) is the activation of unit i in layer j&lt;/li&gt;
      &lt;li&gt;\(\Theta^{(j)}\) is the matrix of weights controlling function mapping from layer j to layer j+1.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, the network diagram above depicts the following computations,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    a_1^{(2)} &amp;= g(\Theta_{10}^{(1)}\,x_0 + \Theta_{11}^{(1)}\,x_1 + \Theta_{12}^{(1)}\,x_2 + \Theta_{13}^{(1)}\,x_3) \\
    a_2^{(2)} &amp;= g(\Theta_{20}^{(1)}\,x_0 + \Theta_{21}^{(1)}\,x_1 + \Theta_{22}^{(1)}\,x_2 + \Theta_{23}^{(1)}\,x_3) \\
    a_3^{(2)} &amp;= g(\Theta_{30}^{(1)}\,x_0 + \Theta_{31}^{(1)}\,x_1 + \Theta_{32}^{(1)}\,x_2 + \Theta_{33}^{(1)}\,x_3) \\
  \end{align}
  \tag{2} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}\,a_0^{(2)} + \Theta_{11}^{(2)}\,a_1^{(2)} + \Theta_{12}^{(2)}\,a_2^{(2)} + \Theta_{13}^{(2)}\,a_3^{(2)}) \tag{3}&lt;/script&gt;

&lt;p&gt;From the equation above one can generalize, &lt;strong&gt;if a network has \(s_j\) units in layer j and \(s_{j+1}\) units in layer j+1, then \(\Theta^{(j)}\) is a matrix of dimension \((s_{j+1} * (s_j + 1))\)&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;vectorization-of-network-computation&quot;&gt;Vectorization of Network Computation&lt;/h3&gt;

&lt;p&gt;Consider (2) can be written as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    a_1^{(2)} &amp;= g(z_1^{(2)}) \\
    a_2^{(2)} &amp;= g(z_2^{(2)}) \\
    a_3^{(2)} &amp;= g(z_3^{(2)}) \\
  \end{align}
  \tag{4} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where \(z_i^{(j)} = \Theta_{i0}^{(j-1)}\,x_0 + \Theta_{i1}^{(j-1)}\,x_1 + \Theta_{i2}^{(j-1)}\,x_2 + \Theta_{i3}^{(j-1)}\,x_3\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given \(x=\begin{bmatrix} x_0 \\ x_1 \\ \vdots \\ x_n\end{bmatrix}\) and \(z^{(j)}=\begin{bmatrix} z_1^{(j)} \\ \vdots \\ z_n^{(j)} \end{bmatrix}\), then computation for \(z^{(j)}\) can be vectorized as follows,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    z^{(j)} &amp;= \Theta^{(j-1)}\,a^{(j-1)}\\
    a^{(j)} &amp;= g(z^{(j)})
  \end{align}
  \tag{5} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where
    &lt;ul&gt;
      &lt;li&gt;\(a^{(1)} = x\)&lt;/li&gt;
      &lt;li&gt;\(\Theta^{(j-1)}\) is a matrix of dimensions \((s_j * (s_{j-1}+1))\)&lt;/li&gt;
      &lt;li&gt;\(s_j\) is number of activation nodes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After (5), bias unit, \(a_0^{(j)} = 1\) is added to the activation vector of layer j and then the process repeats to get activation for the next layer, i.e. (3) is written as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
    z^{(j+1)} &amp;= \Theta^{(j)}\,a^{(j)}\\
    h_\Theta(x) &amp;= a^{(j+1)} = g(z^{(j+1)})
  \end{align}
  \tag{6} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Where \(\Theta^{(j)}\) is a matrix of dimensions \((s_{j+1} * (s_j + 1)) \)&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Process of calculation of activations cascading across layers is called Forward Propagation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;nueral-network-and-logistic-regression&quot;&gt;Nueral Network and Logistic Regression&lt;/h3&gt;

&lt;p&gt;If one looks closely at the neural network diagram above, it can be easily seen that if one removes the layer 1, the schema looks same as a logistic regression. It is infact a logistic regression model if the hidden layer is the direct features (input layer) fed to the neuron because the activation function of the neuron is logistic (sigmoid) function. So, effectively the neural network shown above is a logistic regression where the &lt;strong&gt;features for classification are learnt by the hidden layer and not fed manually by human intervention&lt;/strong&gt;. Each feature in hidden layer is mapped from the input layer. Because of this architecture there is a chance of learning much better features than started with or one can make using the higher order polynomial terms and hence there is a probability of reaching better hypotheses with neural networks.&lt;/p&gt;

&lt;h3 id=&quot;architecture-of-neural-network&quot;&gt;Architecture of Neural Network&lt;/h3&gt;
&lt;p&gt;It is possible to have different kinds of architecture for the neural network. By architecture, it means to have different schema, i.e. &lt;strong&gt;number of neurons&lt;/strong&gt; per layer can vary, &lt;strong&gt;number of layers&lt;/strong&gt; used can vary, the &lt;strong&gt;way the neurons are connected to each other&lt;/strong&gt; can vary and similarly various other variations are possible in terms of &lt;strong&gt;optimization parameters, activation&lt;/strong&gt; etc. When then number of hidden layers is more than one, they are known as &lt;strong&gt;deep neural networks&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;REFERENCES:&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/ka3jK/model-representation-i&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Model Representation I&lt;/a&gt;&lt;/small&gt;&lt;br /&gt;
&lt;small&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/lecture/Hw3VK/model-representation-ii&quot; target=&quot;_blank&quot;&gt;Machine Learning: Coursera - Model Representation II&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Sep 2017 00:00:00 +0530</pubDate>
        <link>https://machinelearningmedium.com/2017/09/21/neural-networks/</link>
        <guid isPermaLink="true">https://machinelearningmedium.com/2017/09/21/neural-networks/</guid>
        
        <category>machine learning</category>
        
        <category>andrew ng</category>
        
        
      </item>
    
  </channel>
</rss>
