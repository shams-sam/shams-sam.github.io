<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Improvements on Word2Vec | Machine Learning Medium</title>
  <meta name="description" content="Importance of optimization and its effect on training time for Word2Vec. Effective utilization of Hierarchical Softmax, Negative Sampling and Subsampling of frequent words." />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
  
  <meta property="og:site_name" content="Machine Learning Medium" />
  <meta property="og:title" content="Improvements on Word2Vec"/>
  
  <meta property="og:description" content="Importance of optimization and its effect on training time for Word2Vec. Effective utilization of Hierarchical Softmax, Negative Sampling and Subsampling of frequent words." />
  
  <meta property="og:image" content="https://machinelearningmedium.com/assets/images/wordembedding.png" />
  <meta property="og:url" content="https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/" >
  <meta property="og:type" content="blog" />
  <meta property="article:published_time" content="2017-07-26T00:00:00+05:30">

  <link rel="canonical" href="https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/"/>
  <link rel="shortcut icon" href="/public/fav.png" type="image/png"/>
  <link rel="stylesheet" href="//brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
  <link rel="stylesheet" type="text/css" href="/css/custom.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/share_bar.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/syntax.css" />
  <link href="https://fonts.googleapis.com/css?family=Fira+Sans:400,600|Open+Sans:400,700" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" >
  <link rel="shortcut icon" href="/public/fav.ico?v1">


  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

</head>

  <body itemscope itemtype="http://schema.org/Article">
    <div class="metabar metabar-header">
  <div class="metabar-inner">
    <div class="quote-div">
      <a class="icon-quote" href="/">
        <i class="fa fa-quote-left fa-pull-left fa-border" aria-hidden="true"></i>
      </a>
    </div>
  </div>
</div>

<div class="sectionbar sectionbar-header">
  <div class="sectionbar-inner">
    <div class="table">
      <div class="category-links table-cell">
        
          <a href=/>Home</a>
        
        
        <a href="/tag/machine-learning">Machine Learning</a>
        
        <a href="/tag/mathematics">Mathematics</a>
        
        <a href="/tag/papers">Papers</a>
        
        <a href="/tag/dsa">DSA</a>
        
        <span class="right-padding-22">|</span>
        <a href=/tags/ class="">Tags</a>
      </div>
      <div class="social-links table-cell">
        <!-- 
          
              <a class="icon-github-alt" href="https://github.com/shams-sam/shams-sam.github.io"  title="Gihub Project" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-github-alt fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-facebook" href="https://facebook.com/shams-sam"  title="Facebook" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-facebook fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-twitter" href="https://twitter.com/sshamssam"  title="Twitter" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-twitter fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-linkedin" href="https://linkedin.com/in/shams-sheikh-20328154"  title="LinkedIn" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-envelope" href="mailto:shams.sam@live.com"  title="E-mail" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-envelope fa-stack-1x"></i>
                </span>
              </a>
          
         -->
        <div class="fb-like" data-href="https://www.facebook.com/machinelearningmedium/" data-layout="button" data-action="like" data-size="large" data-show-faces="false" data-share="false"></div>
        <a href="https://twitter.com/sshamssam" class="twitter-follow-button" data-size="large" data-show-screen-name="false" data-dnt="true" data-show-count="false"></a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
      </div>
    </div>
  </div>
</div>
    <div class="author-strip">
      <div class="author-strip-inner">
        <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
        <div class="author-detail">
          <h4 class="author-name" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/about/" style="color: #000000; text-decoration: none;">Shams S</a></h4>
          <h5 class="author-name"> Data Scientist @ Practo </h5>
          <time datetime="2017-07-26T00:00:00+05:30">Jul 26</time>
          <span class="middot">&#183;</span>
          <div class="post-reading">
            <span class="post-reading-time"></span> read
          </div>
        </div>
      </div>
    </div>
    
    <div class="postcover">
    <div class="postimage">
        <div class="postimage-image"  style="background-image: url(/assets/images/wordembedding.png) ">
        </div>
    </div>
    
    <figcaption>Image Source: http://www.deep-solutions.net/blog/wordembeddings/wordembedding.png</figcaption>
    
    </div>
    <main class="content post-content" role="main">
      <article class="post">
        <div class="noarticleimage">
          <div class="post-meta">
            <h1 class="post-title">Improvements on Word2Vec</h1>
            <h2 class="post-description">Importance of optimization and its effect on training time for Word2Vec. Effective utilization of Hierarchical Softmax, Negative Sampling and Subsampling of frequent words.</h2>
          </div>
        </div>
        <div class="horizontal-divider">&#183; &#183; &#183;</div>
        <section class="post-content">
          <a name="topofpage"></a>
          <h3 id="distributed-vector-representation-series">Distributed Vector Representation Series</h3>

<blockquote>
  <p><a href="/2017/07/11/word-to-vector-word-representations/">Word2Vec</a></p>

  <p><a href="/2017/07/26/improvements-on-word2vec/">Improvements on Word2Vec</a></p>
</blockquote>

<div class="horizontal-divider">· · ·</div>

<h3 id="skip-gram-model">Skip-Gram Model</h3>
<ul>
  <li>Training objective of skip-gram model is to deduce word representations that help in predicting the surrounding words in a sentence or a document, i.e. give a sequence of training words \(w_1, w_2, w_3, … , w_T\), the objective is to maximize the average log probability,</li>
</ul>

<script type="math/tex; mode=display">{1 \over T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} log\,p(w_{t+j} | w_t)</script>

<ul>
  <li>Where c is the size of the <strong>training context</strong></li>
  <li>
    <p>Larger c results in more training examples and thus higher accuracy, at the expense of the training time.</p>
  </li>
  <li>Basic skip-gram formulation defines \(p(w_{t+j}|w_t)\) using softmax function</li>
</ul>

<script type="math/tex; mode=display">p(w_O|w_I) = \frac {exp((v_{w_O}^{'})^T v_{w_I})} {\sum_{w=1}^W exp((v_w^{'})^T v_{w_I})}</script>

<ul>
  <li>Where
    <ul>
      <li>\(v_w\) and \(v_w^{'}\) are the <strong>input</strong> and <strong>output</strong> vector representations of \(w\)</li>
      <li>\(W\) is the number of words in the vocabulary</li>
      <li>cost of computing \(\nabla log\,p(w_O| w_I)\) is proportional to \(W\) which is quite large in order of \(10^5 - 10^7\) terms</li>
    </ul>
  </li>
</ul>

<h3 id="drawbacks-of-initial-word2vec-proposed">Drawbacks of Initial <a href="/2017/07/11/word-to-vector-word-representations/" target="_blank">Word2Vec</a> Proposed</h3>

<ul>
  <li>Training time</li>
  <li>Indifference to word order and inability to represent idiomatic phrases.</li>
</ul>

<h3 id="improvements">Improvements</h3>

<ul>
  <li><strong>Hierarchical Softmax</strong>
    <ul>
      <li>Computationally efficient approximation of full softmax.</li>
      <li>Advantageous because instead of evaluating W output nodes in the neural network, only need to evaluate \(log_2(W)\) nodes.</li>
      <li>Uses binary tree representation of the output layer with W nodes as its leaves. Each node represents the <strong>relative probability</strong> of its child nodes. So, probability is assigned to a leaf node through <strong>random walk</strong> from root node</li>
      <li>Each word can be reached by following an appropriate path from the root. Let \(n(w, j)\) be the j-th node on the path from the root to w, and let \(L(w)\) be the length of this path. So,</li>
    </ul>

    <p>\[n(w, 1) = root\]</p>

    <p>\[n(w, L(w)) = w\]</p>

    <ul>
      <li>For any inner child n, let \(ch(n)\) be arbitrary fixed child of n and let \(\unicode{x27E6} x \unicode{x27E7}\) be 1 if x is true and -1 otherwise, then hierarchical softmax defines \(p(w_O|w_I)\) as</li>
    </ul>

    <script type="math/tex; mode=display">p(w_O|w_I) = \prod_{j=1}^{L(w) - 1} \sigma (\unicode{x27E6} n(w, j+1) = ch(n(w, j)) \unicode{x27E7} . (v_{n(w, j)}^{'})^T v_{w_I})</script>

    <ul>
      <li>Where \(\sigma (x) = {1 \over (1 + exp(-x))}\)</li>
      <li>\(\sum_{w=1}^W p(w|w_I) = 1\) is verifiable.</li>
    </ul>
  </li>
  <li><strong>Negative Sampling</strong>
    <ul>
      <li>Alternative to Hierarchical softmax.</li>
      <li>Based on <strong>Noise Contrastive Estimation(NCE)</strong> which posits that a good model should be able to differentiate data from noise by means of <strong>logistic regression</strong>.</li>
      <li>NCE approximately maximizes the log probability of softmax, but skip-gram only aims at learning high quality word representations and hence NCE can be simplified.</li>
      <li><strong>Negative Sampling (NEG)</strong> is defined as</li>
    </ul>

    <script type="math/tex; mode=display">log \, \sigma ((v_{w_O}^{'})^T v_{w_I}) + \sum_{i=1}^k \unicode{x1D53C}_{w_i \sim P_n(w)} [log\, \sigma (- (v_{w_I}^{'})^T v_{w_I})]</script>

    <ul>
      <li>
        <p>It replaces the \(log\, p(w_O | w_I)\) term in skip-gram objective</p>
      </li>
      <li>
        <p>Aiming at distinguishing between \(w_O\) from draws from noise distribution \(P_n(w)\) using logistic regression, where k is the number of negative samples for each data sample, because this use case does not require maximization of the softmax log probability.</p>
      </li>
      <li>
        <p>k value ranges 5-20 for small datasets and 2-5 for large datasets.</p>
      </li>
      <li>
        <p>NCE differs from NEG in that NCE needs the sample and numerical probabilities of the noise distribution, but NEG uses only samples.</p>
      </li>
      <li>
        <p>Both NCE and NEG have \(P_n(w)\) as a free parameter but <strong>unigram distribution</strong> U(w) raised to 3/4 power i.e. \(U(w)^{3/4}/Z\) is found to outperform other options like <strong>unigram</strong> and <strong>uniform</strong> distribution.</p>
      </li>
    </ul>
  </li>
  <li><strong>Subsampling of frequent words</strong>
    <ul>
      <li>In a corpus, most frequent words can occur hundreds of millions of time such as the stopwords.</li>
      <li>These words give little information by co-occuring with other words. Consequently, vector representations of frequent words do not change significantly after training on several million examples.</li>
      <li>Imbalance between rare and frequent words are thus countered using sub-sampling approach.</li>
      <li>Each word \(w_i\) in the training set is discarded with a probability given by</li>
    </ul>

    <script type="math/tex; mode=display">P(w_i) = 1 - \sqrt (\frac {t} {f(w_i)})</script>

    <ul>
      <li>Where
        <ul>
          <li>\(f(w_i)\) is frequency of word \(w_i\)</li>
          <li>t is a chosen threshold, around \(10^{-5}\)</li>
        </ul>
      </li>
      <li>Subsampling formula is chosen <strong>heuristically</strong></li>
    </ul>
  </li>
  <li><strong>Learning Phrases</strong>
    <ul>
      <li>Aim is to learn phrases where in individual words meaning is entirely different when compared to the group of words.</li>
      <li>Start off by finding words that frequently occur together and infrequently in other contexts.</li>
      <li>Theoretically, skip-gram model can be trained with all n-grams, but would be a very memory intensive operation.</li>
      <li>A data driven approach is put forward based on unigram and bigram counts, given by</li>
    </ul>

    <script type="math/tex; mode=display">score(w_i, w_j) = \frac {count(w_i w_j) - \delta} {count(w_i) * count(w_j)}</script>

    <ul>
      <li>
        <p>Where \(\delta\) is a <strong>discounting coefficient</strong> and prevents phrases formed by very infrequent words. The bigrams with score above a given threshold only are used as phrases.</p>
      </li>
      <li>
        <p>Often it is needed to run the process 2-4 times changing the threshold values and seeing the quality of phrases formed.</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="conclusions">Conclusions</h3>

<ul>
  <li>
    <p>Subsampling results in faster training and significantly better representations of uncommon words.</p>
  </li>
  <li>
    <p>Negative sampling helps accurately train for frequent words using a simple method.</p>
  </li>
</ul>

<h2 id="references">REFERENCES</h2>

<p><small><a href="http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a></small></p>

        </section>
        <div class="horizontal-divider">· · ·</div>
        <footer class="post-footer">
          <div class="wrap">
            
              <div class="tile">
                <div class="text">
                <a href="/tag/nlp" class='category-links'><h1>NLP</h1></a>
                </div>
              </div>
            
              <div class="tile">
                <div class="text">
                <a href="/tag/machine-learning" class='category-links'><h1>machine learning</h1></a>
                </div>
              </div>
            
              <div class="tile">
                <div class="text">
                <a href="/tag/papers" class='category-links'><h1>papers</h1></a>
                </div>
              </div>
            
          </div>
          <section class="share">
            <span id="share-bar">

    <span><i class="fa fa-share-alt"></i></span>

    <span class="share-buttons">
        <span>
        <a  href="https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Facebook" >
            <i class="fa fa-facebook-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://twitter.com/intent/tweet?text=Improvements on Word2Vec&url=https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Twitter" >
            <i class="fa fa-twitter-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://plus.google.com/share?url=https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Google+" >
            <i class="fa fa-google-plus-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://pinterest.com/pin/create/button/?url=https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Pinterest" >
            <i class="fa fa-pinterest-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.tumblr.com/share/link?url=https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Tumblr" >
            <i class="fa fa-tumblr-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.reddit.com/submit?url=https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Reddit" >
            <i class="fa fa-reddit-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://www.linkedin.com/shareArticle?mini=true&url=https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/&title=Improvements on Word2Vec&summary=Importance of optimization and its effect on training time for Word2Vec. Effective utilization of Hierarchical Softmax, Negative Sampling and Subsampling of frequent words.&source=Machine Learning Medium"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on LinkedIn" >
            <i class="fa fa-linkedin-square"></i>
        </a>
        </span>
        <span>
        <a  href="mailto:?subject=Improvements on Word2Vec&amp;body=Check out this site https://machinelearningmedium.com/2017/07/26/improvements-on-word2vec/"
            title="Share via Email" >
            <i class="fa fa-envelope-square"></i>
        </a>
        </span>
    </span>

</span>
          </section>
        </footer>
        <!-- <div class="bottom-teaser cf">
          <div class="isLeft">
            <h5 class="index-headline featured"><span>Written by</span></h5>
            <section class="author">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4>Shams S</h4>
              <p class="bio">Data Scientist @ Practo</p>
              <hr>
              <p class="published">Published on <time datetime="2017-07-26 00:00">26 Jul 2017</time></p>
            </section>
          </div>
          <div class="isRight">
            <h5 class="index-headline featured"><span>Supported by</span></h5>
            <footer class="site-footer">
              <section class="poweredby">Proudly published with <a href="http://jekyllrb.com"> Jekyll</a></section>
              <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> You should subscribe to my feed.</span></a>
              <div class="inner">
                <section class="copyright">All content copyright <a href="https://machinelearningmedium.com/">Machine Learning Medium</a> &copy; 2017<br>All rights reserved.</section>
              </div>
            </footer>
          </div>
        </div> -->
        
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE */
            var disqus_shortname = 'shams-sam'; /* required: replace example with your forum shortname */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        
      </article>
    </main>
    <!-- footer start -->
<div class="footer-container">

<footer class="site-footer">
  <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> Subscribe!</span></a>
    <div class="inner">
         <section class="copyright">All content copyright <a href="/">Machine Learning Medium</a> &copy; 2017 &bull; All rights reserved.</section>
         <section class="poweredby">Made with <a href="http://jekyllrb.com"> Jekyll</a></section>
         <section class="poweredby">Inspired by <a href="https://medium.com/"> Medium</a> and <a href="https://github.com/dirkfabisch/mediator">Mediator</a></section>
    </div>
</footer>

<div class="bottom-closer">
  <div class="background-closer-image"  style="background-image: url(/assets/images/footer.jpg)">
    Image
  </div>
  <div class="inner">
    <h1 class="footer-title">Machine Learning Medium</h1>
    <h2 class="footer-description">Mathematics, Machine Learning, Natural Language Processing Etc.</h2>
    <a href=/about/ class="btn">About Me</a>
  </div>
</div>

<!-- footer end -->
</div>

    <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script src="/public/js/typed.js"></script>
<script>
  $(function(){
    $(".blog-description").typed({
      strings: ['Mathematics, Machine Learning, Natural Language Processing Etc.'],
      typeSpeed: 50,
      backSpeed: 25,
      loop: true
    });
  });
</script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      },
      CommonHTML: {matchFontHeight: false},
      "HTML-CSS": {matchFontHeight: false},
      SVG: {matchFontHeight: false}
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>



<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-108148924-1', 'auto');
ga('send', 'pageview');

</script>


<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = 'https://connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v2.10&appId=391866664565981';
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>
  </body>
</html>
