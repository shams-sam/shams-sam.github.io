<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Gradient Descent for Linear Regression | Machine Learning Medium</title>
  <meta name="description" content="Application of gradient descent for optimization of a linear regression cost function and its comparison with normal equation method of finding solution" />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
  
  <meta property="og:site_name" content="Machine Learning Medium" />
  <meta property="og:title" content="Gradient Descent for Linear Regression"/>
  
  <meta property="og:description" content="Application of gradient descent for optimization of a linear regression cost function and its comparison with normal equation method of finding solution" />
  
  <meta property="og:image" content="https://machinelearningmedium.com/assets/images/gradient-descent.png" />
  <meta property="og:url" content="https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/" >
  <meta property="og:type" content="blog" />
  <meta property="article:published_time" content="2017-08-17T00:00:00+05:30">

  <link rel="canonical" href="https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/"/>
  <link rel="shortcut icon" href="/public/fav.png" type="image/png"/>
  <link rel="stylesheet" href="//brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
  <link rel="stylesheet" type="text/css" href="/css/custom.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/share_bar.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/syntax.css" />
  <link href="https://fonts.googleapis.com/css?family=Fira+Sans:400,600|Open+Sans:400,700" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" >
  <link rel="shortcut icon" href="/public/fav.ico?v1">


  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

</head>

  <body itemscope itemtype="http://schema.org/Article">
    <div class="metabar metabar-header">
  <div class="metabar-inner">
    <div class="quote-div">
      <a class="icon-quote" href="/">
        <i class="fa fa-quote-left fa-pull-left fa-border" aria-hidden="true"></i>
      </a>
    </div>
  </div>
</div>

<div class="sectionbar sectionbar-header">
  <div class="sectionbar-inner">
    <div class="table">
      <div class="category-links table-cell">
        
          <a href=/>Home</a>
        
        
        <a href="/tag/machine-learning">Machine Learning</a>
        
        <a href="/tag/mathematics">Mathematics</a>
        
        <a href="/tag/papers">Papers</a>
        
        <a href="/tag/dsa">DSA</a>
        
        <span class="right-padding-22">|</span>
        <a href=/tags/ class="">Tags</a>
      </div>
      <div class="social-links table-cell">
        <!-- 
          
              <a class="icon-github-alt" href="https://github.com/shams-sam/shams-sam.github.io"  title="Gihub Project" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-github-alt fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-facebook" href="https://facebook.com/shams-sam"  title="Facebook" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-facebook fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-twitter" href="https://twitter.com/sshamssam"  title="Twitter" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-twitter fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-linkedin" href="https://linkedin.com/in/shams-sheikh-20328154"  title="LinkedIn" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-envelope" href="mailto:shams.sam@live.com"  title="E-mail" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-envelope fa-stack-1x"></i>
                </span>
              </a>
          
         -->
        <div class="fb-like" data-href="https://www.facebook.com/machinelearningmedium/" data-layout="button" data-action="like" data-size="large" data-show-faces="false" data-share="false"></div>
        <a href="https://twitter.com/sshamssam" class="twitter-follow-button" data-size="large" data-show-screen-name="false" data-dnt="true" data-show-count="false"></a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
      </div>
    </div>
  </div>
</div>
    <div class="author-strip">
      <div class="author-strip-inner">
        <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
        <div class="author-detail">
          <h4 class="author-name" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/about/" style="color: #000000; text-decoration: none;">Shams S</a></h4>
          <h5 class="author-name"> Data Scientist @ Practo </h5>
          <time datetime="2017-08-17T00:00:00+05:30">Aug 17</time>
          <span class="middot">&#183;</span>
          <div class="post-reading">
            <span class="post-reading-time"></span> read
          </div>
        </div>
      </div>
    </div>
    
    <div class="postcover">
    <div class="postimage">
        <div class="postimage-image"  style="background-image: url(/assets/images/gradient-descent.png) ">
        </div>
    </div>
    
    <figcaption>Image Source: https://upload.wikimedia.org/wikipedia/commons/3/31/Gradient99.png</figcaption>
    
    </div>
    <main class="content post-content" role="main">
      <article class="post">
        <div class="noarticleimage">
          <div class="post-meta">
            <h1 class="post-title">Gradient Descent for Linear Regression</h1>
            <h2 class="post-description">Application of gradient descent for optimization of a linear regression cost function and its comparison with normal equation method of finding solution</h2>
          </div>
        </div>
        <div class="horizontal-divider">&#183; &#183; &#183;</div>
        <section class="post-content">
          <a name="topofpage"></a>
          <h3 id="introduction">Introduction</h3>
<p>The posts <a href="/2017/08/11/cost-function-of-linear-regression/" target="_blank">Cost Function of Linear Regression</a> and <a href="/2017/08/15/gradient-descent/">Gradient Descent</a> introduced the linear regression cost function and the gradient descent algorithm individually. If the gradient descent is applied to the linear regression cost function would help reach an optimal solution without much manual intervention.</p>

<h3 id="gradient-descent"><a href="/2017/08/15/gradient-descent/" target="_blank">Gradient Descent</a></h3>
<p>Gradient descent algorithm can be summarized as,</p>

<script type="math/tex; mode=display">\text{repeat until convergence} \{ \theta_j := \theta_j - \alpha \frac {\partial} {\partial \theta_j} J(\theta_0, \theta_1)\, \forall j \in \{0, 1\} \} \tag{1}</script>

<ul>
  <li>Where
    <ul>
      <li>:= is the assignment operator</li>
      <li>\(\alpha\) is the <strong>learning rate</strong> which basically defines how big the steps are during the descent</li>
      <li>\( \frac {\partial} {\partial \theta_j} J(\theta_0, \theta_1)\) is the <strong>partial derivative</strong> term</li>
      <li>j = 0, 1 represents the <strong>feature index number</strong></li>
    </ul>
  </li>
</ul>

<h3 id="linear-regression"><a href="/2017/08/11/cost-function-of-linear-regression/" target="_blank">Linear Regression</a></h3>
<p>Linear regression hypothesis is given by,</p>

<script type="math/tex; mode=display">h_\theta (x) = \theta_0 + \theta_1\,x \tag{2}</script>

<p>And the corresponding cost function is given by,</p>

<script type="math/tex; mode=display">J(\theta_0, \theta_1) = {1 \over 2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 \tag{3}</script>

<h3 id="gradient-descent-for-linear-regression">Gradient Descent for Linear Regression</h3>
<p>Gradient descent is applied to the optimazation problem of the cost function of the linear regression given in (2) in order to find parameters that minimize the cost.</p>

<p>In order to apply gradient descent, the derivative term i.e. \(\frac {\partial} {\partial \theta_j} J(\theta_0, \theta_1)\) needs to be calculated.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \frac {\partial} {\partial \theta_j} J(\theta_0, \theta_1) & = \frac {\partial} {\partial \theta_j} \left( {1 \over 2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 \right) \\
    & = {1 \over 2m} \left( \frac {\partial} {\partial \theta_j} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 \right) \\
    & = {1 \over 2m} \left( \frac {\partial} {\partial \theta_j} \sum_{i=1}^m \left( \theta_0 + \theta_1\,x^{(i)} - y^{(i)} \right)^2 \right) \\
    & = 
    \begin{cases}
      {1 \over m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) \text{ for j = 0} \\
      {1 \over m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x^{(i)} \text{ for j = 1}
    \end{cases}
  \end{align} %]]></script>

<p>Updating the values of the derivative term in the gradient descent algorithm given in (1) where the update must be done simultaneously,</p>

<script type="math/tex; mode=display">\begin{align}
    \text{repeat until convergence}
    \begin{cases}
      \theta_0 := \theta_0 - \alpha {1 \over m} \sum_{i=1}^m (h(x^{(i)} - y^{(i)})) \\
      \theta_1 := \theta_1 - \alpha {1 \over m} \sum_{i=1}^m (h(x^{(i)} - y^{(i)})) \cdot x^{(i)}
    \end{cases}
  \end{align}</script>

<p><strong>The cost function for a linear regression is a convex function i.e. a bow-shaped function and has only one global minimum and no local minima. So it does not face the issue of getting stuck in local minima</strong>. It can be understood better with the below plots.</p>

<p><img src="/assets/2017-08-17-gradient-descent-for-linear-regression/fig-1-convex-function.png?raw=true" alt="Convex Function" /></p>

<p>The gradient descent technique that uses all the training examples in each step is called <strong>Batch Gradient Descent</strong>. This is basically the calculation of the derivative term over all the training examples as it can be seen it the equation above.</p>

<p>The equation of linear regression can also be solved using <strong>Normal Equations</strong> method, but it poses a disadvantage that it does not scale very well on larger data while gradient descent does.</p>

<h3 id="implementation">Implementation</h3>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="s">"""
Dummy Data for Linear Regression
"""</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mf">8.4</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)]</span>    

<span class="s">"""
Plot the line using theta_values
"""</span>
<span class="k">def</span> <span class="nf">plot_line</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">x_range</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_range</span><span class="p">)</span>  
    <span class="n">y</span> <span class="o">=</span> <span class="n">formula</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="s">"""
Hypothesis Function
"""</span>
<span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">theta_0</span> <span class="o">+</span> <span class="n">theta_1</span> <span class="o">*</span> <span class="n">x</span>

<span class="s">"""
Partial Derivative w.r.t. theta_1
"""</span>
<span class="k">def</span> <span class="nf">j_prime_theta_1</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data</span> <span class="p">:</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
    
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">result</span>

<span class="s">"""
Partial Derivative w.r.t. theta_0
"""</span>
<span class="k">def</span> <span class="nf">j_prime_theta_0</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data</span> <span class="p">:</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">result</span>

<span class="s">"""
Cost Function
"""</span>
<span class="k">def</span> <span class="nf">j</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">):</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">cost</span> <span class="o">+=</span> <span class="n">math</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="n">cost</span>

<span class="s">"""
Simultaneous Update
"""</span>
<span class="k">def</span> <span class="nf">update_theta</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">):</span>
    <span class="n">temp_0</span> <span class="o">=</span> <span class="n">theta_0</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">j_prime_theta_0</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">)</span>
    <span class="n">temp_1</span> <span class="o">=</span> <span class="n">theta_1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">j_prime_theta_1</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">)</span>
    <span class="n">theta_0</span> <span class="o">=</span> <span class="n">temp_0</span>
    <span class="n">theta_1</span> <span class="o">=</span> <span class="n">temp_1</span>
    <span class="k">return</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span>
    
<span class="s">"""
Gradient Descent For Linear Regression
"""</span>
<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">,</span> <span class="n">theta_0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">theta_1</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">theta_0</span><span class="p">:</span>
        <span class="n">theta_0</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">theta_1</span><span class="p">:</span>
        <span class="n">theta_1</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="n">prev_j</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">curr_j</span> <span class="o">=</span> <span class="n">j</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">)</span>
    <span class="n">theta_0_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">theta_1_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cost_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">while</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">curr_j</span> <span class="o">-</span> <span class="n">prev_j</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tolerance</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">cost_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">curr_j</span><span class="p">)</span>
            <span class="n">theta_0_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta_0</span><span class="p">)</span>
            <span class="n">theta_1_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta_1</span><span class="p">)</span>
            <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span> <span class="o">=</span> <span class="n">update_theta</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">)</span>
            <span class="n">prev_j</span> <span class="o">=</span> <span class="n">curr_j</span>
            <span class="n">curr_j</span> <span class="o">=</span> <span class="n">j</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Stopped with Error at </span><span class="si">%.5</span><span class="s">f"</span> <span class="o">%</span> <span class="n">prev_j</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span>

<span class="n">theta_0</span><span class="p">,</span> <span class="n">theta_1</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="/assets/2017-08-17-gradient-descent-for-linear-regression/fig-2-gradient-descent.gif?raw=true" alt="Gradient Descent In Action" /></p>

<p>The plot shows the adjustment of the values of \(\theta_0\) and \(\theta_1\) for fitting the best line through the given data points.</p>

<h2 id="references">REFERENCES:</h2>

<p><small><a href="https://www.coursera.org/learn/machine-learning/lecture/kCvQc/gradient-descent-for-linear-regression" target="_blank">Machine Learning: Coursera - Gradient Descent for Linear Regression</a></small></p>

        </section>
        <div class="horizontal-divider">· · ·</div>
        <footer class="post-footer">
          <div class="wrap">
            
              <div class="tile">
                <div class="text">
                <a href="/tag/machine-learning" class='category-links'><h1>machine learning</h1></a>
                </div>
              </div>
            
              <div class="tile">
                <div class="text">
                <a href="/tag/andrew-ng" class='category-links'><h1>andrew ng</h1></a>
                </div>
              </div>
            
          </div>
          <section class="share">
            <span id="share-bar">

    <span><i class="fa fa-share-alt"></i></span>

    <span class="share-buttons">
        <span>
        <a  href="https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Facebook" >
            <i class="fa fa-facebook-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://twitter.com/intent/tweet?text=Gradient Descent for Linear Regression&url=https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Twitter" >
            <i class="fa fa-twitter-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://plus.google.com/share?url=https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Google+" >
            <i class="fa fa-google-plus-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://pinterest.com/pin/create/button/?url=https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Pinterest" >
            <i class="fa fa-pinterest-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.tumblr.com/share/link?url=https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Tumblr" >
            <i class="fa fa-tumblr-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.reddit.com/submit?url=https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Reddit" >
            <i class="fa fa-reddit-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://www.linkedin.com/shareArticle?mini=true&url=https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/&title=Gradient Descent for Linear Regression&summary=Application of gradient descent for optimization of a linear regression cost function and its comparison with normal equation method of finding solution&source=Machine Learning Medium"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on LinkedIn" >
            <i class="fa fa-linkedin-square"></i>
        </a>
        </span>
        <span>
        <a  href="mailto:?subject=Gradient Descent for Linear Regression&amp;body=Check out this site https://machinelearningmedium.com/2017/08/17/gradient-descent-for-linear-regression/"
            title="Share via Email" >
            <i class="fa fa-envelope-square"></i>
        </a>
        </span>
    </span>

</span>
          </section>
        </footer>
        <!-- <div class="bottom-teaser cf">
          <div class="isLeft">
            <h5 class="index-headline featured"><span>Written by</span></h5>
            <section class="author">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4>Shams S</h4>
              <p class="bio">Data Scientist @ Practo</p>
              <hr>
              <p class="published">Published on <time datetime="2017-08-17 00:00">17 Aug 2017</time></p>
            </section>
          </div>
          <div class="isRight">
            <h5 class="index-headline featured"><span>Supported by</span></h5>
            <footer class="site-footer">
              <section class="poweredby">Proudly published with <a href="http://jekyllrb.com"> Jekyll</a></section>
              <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> You should subscribe to my feed.</span></a>
              <div class="inner">
                <section class="copyright">All content copyright <a href="https://machinelearningmedium.com/">Machine Learning Medium</a> &copy; 2017<br>All rights reserved.</section>
              </div>
            </footer>
          </div>
        </div> -->
        
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE */
            var disqus_shortname = 'shams-sam'; /* required: replace example with your forum shortname */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        
      </article>
    </main>
    <!-- footer start -->
<div class="footer-container">

<footer class="site-footer">
  <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> Subscribe!</span></a>
    <div class="inner">
         <section class="copyright">All content copyright <a href="/">Machine Learning Medium</a> &copy; 2017 &bull; All rights reserved.</section>
         <section class="poweredby">Made with <a href="http://jekyllrb.com"> Jekyll</a></section>
         <section class="poweredby">Inspired by <a href="https://medium.com/"> Medium</a> and <a href="https://github.com/dirkfabisch/mediator">Mediator</a></section>
    </div>
</footer>

<div class="bottom-closer">
  <div class="background-closer-image"  style="background-image: url(/assets/images/footer.jpg)">
    Image
  </div>
  <div class="inner">
    <h1 class="footer-title">Machine Learning Medium</h1>
    <h2 class="footer-description">Mathematics, Machine Learning, Natural Language Processing Etc.</h2>
    <a href=/about/ class="btn">About Me</a>
  </div>
</div>

<!-- footer end -->
</div>

    <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script src="/public/js/typed.js"></script>
<script>
  $(function(){
    $(".blog-description").typed({
      strings: ['Mathematics, Machine Learning, Natural Language Processing Etc.'],
      typeSpeed: 50,
      backSpeed: 25,
      loop: true
    });
  });
</script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      },
      CommonHTML: {matchFontHeight: false},
      "HTML-CSS": {matchFontHeight: false},
      SVG: {matchFontHeight: false}
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>



<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-108148924-1', 'auto');
ga('send', 'pageview');

</script>


<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = 'https://connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v2.10&appId=391866664565981';
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>
  </body>
</html>
