<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <!-- (1) Optimize for mobile versions: http://goo.gl/EOpFl -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- (1) force latest IE rendering engine: bit.ly/1c8EiC9 -->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  

  <title>Neural Networks: Cost Function and Backpropagation | Machine Learning Medium</title>
  <meta name="description" content="Intuition behind the idea of backpropagation and its extension to calculate cost function" />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
  
  <meta property="og:site_name" content="Machine Learning Medium" />
  <meta property="og:title" content="Neural Networks: Cost Function and Backpropagation"/>
  
  <meta property="og:description" content="Intuition behind the idea of backpropagation and its extension to calculate cost function" />
  
  <meta property="og:image" content="https://machinelearningmedium.com/assets/images/neural-network.png" />
  <meta property="og:url" content="https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/" >
  <meta property="og:type" content="blog" />
  <meta property="article:published_time" content="2017-10-03T00:00:00+00:00">

  <link rel="canonical" href="https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/"/>
  <link rel="shortcut icon" href="/public/fav.png" type="image/png"/>
  <link rel="stylesheet" href="//brick.a.ssl.fastly.net/Linux+Libertine:400,400i,700,700i/Open+Sans:400,400i,700,700i">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="stylesheet" type="text/css" media="screen" href="/css/main.css" />
  <link rel="stylesheet" type="text/css" media="print" href="/css/print.css" />
  <link rel="stylesheet" type="text/css" href="/css/custom.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/share_bar.css" />
  <link rel="stylesheet" type="text/css" href="/public/css/syntax.css" />
  <link href="https://fonts.googleapis.com/css?family=Fira+Sans:400,600|Open+Sans:400,700" rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" >
  <link rel="shortcut icon" href="/public/fav.ico?v1">


  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

</head>

  <body itemscope itemtype="http://schema.org/Article">
    <div class="metabar metabar-header">
  <div class="metabar-inner">
    <div class="quote-div">
      <a class="icon-quote" href="/">
        <i class="fa fa-quote-left fa-pull-left fa-border" aria-hidden="true"></i>
      </a>
    </div>
  </div>
</div>

<div class="sectionbar sectionbar-header">
  <div class="sectionbar-inner">
    <div class="table">
      <div class="category-links table-cell">
        
          <a href=/>Home</a>
        
        
        <a href="/tag/machine-learning">Machine Learning</a>
        
        <a href="/tag/mathematics">Mathematics</a>
        
        <a href="/tag/papers">Papers</a>
        
        <span class="right-padding-22">|</span>
        <a href=/collections/ class="">Collections</a>
        <a href=/tags/ class="">Tags</a>
        
          <span class="right-padding-22">|</span>
          <a class="icon-search" href="/search/"><i class="fa fa-search"></i></a>
        
      </div>
      <div class="social-links table-cell">
        <!-- 
          
              <a class="icon-github-alt" href="https://github.com/shams-sam/shams-sam.github.io"  title="Gihub Project" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-github-alt fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-facebook" href="https://facebook.com/shams-sam"  title="Facebook" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-facebook fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-twitter" href="https://twitter.com/sshamsofficial"  title="Twitter" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-twitter fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-linkedin" href="https://linkedin.com/in/shams-sam"  title="LinkedIn" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x"></i>
                </span>
              </a>
          
        
          
              <a class="icon-envelope" href="mailto:azam1@purdue.edu"  title="E-mail" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-square-o fa-stack-2x"></i>
                  <i class="fa fa-envelope fa-stack-1x"></i>
                </span>
              </a>
          
         -->
        <div class="fb-like" data-href="https://www.facebook.com/machinelearningmedium/" data-layout="button" data-action="like" data-size="large" data-show-faces="false" data-share="false"></div>
        <a href="https://twitter.com/sshamsofficial" class="twitter-follow-button" data-size="large" data-show-screen-name="false" data-dnt="true" data-show-count="false"></a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
      </div>
    </div>
  </div>
</div>
    <div class="author-strip">
      <div class="author-strip-inner">
        <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
        <div class="author-detail">
          <h4 class="author-name" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/about/" style="color: #000000; text-decoration: none;">Shams S</a></h4>
          <h5 class="author-name"> Research Assistant @ Purdue University </h5>
          <time datetime="2017-10-03T00:00:00+00:00">Oct 3</time>
          <span class="middot">&#183;</span>
          <div class="post-reading">
            <span class="post-reading-time"></span> read
          </div>
        </div>
      </div>
    </div>
    
    <div class="postcover">
    <div class="postimage">
        <div class="postimage-image"  style="background-image: url(/assets/images/neural-network.png) ">
        </div>
    </div>
    
    <figcaption>Image Source: https://s3.amazonaws.com/f6s-public/media/480903.png</figcaption>
    
    </div>
    <main class="content post-content" role="main">
      <article class="post">
        <div class="noarticleimage">
          <div class="post-meta">
            <h1 class="post-title">Neural Networks: Cost Function and Backpropagation</h1>
            <section class="share">
            <span id="share-bar">

    <span><i class="fa fa-share-alt"></i></span>

    <span class="share-buttons">
        <span>
        <a  href="https://www.facebook.com/sharer/sharer.php?u=https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Facebook" >
            <i class="fa fa-facebook-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://twitter.com/intent/tweet?text=Neural Networks: Cost Function and Backpropagation&url=https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Twitter" >
            <i class="fa fa-twitter-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://plus.google.com/share?url=https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Google+" >
            <i class="fa fa-google-plus-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://pinterest.com/pin/create/button/?url=https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Pinterest" >
            <i class="fa fa-pinterest-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.tumblr.com/share/link?url=https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Tumblr" >
            <i class="fa fa-tumblr-square"></i>
        </a>
        </span>
        <span>
        <a  href="http://www.reddit.com/submit?url=https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on Reddit" >
            <i class="fa fa-reddit-square"></i>
        </a>
        </span>
        <span>
        <a  href="https://www.linkedin.com/shareArticle?mini=true&url=https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/&title=Neural Networks: Cost Function and Backpropagation&summary=Intuition behind the idea of backpropagation and its extension to calculate cost function&source=Machine Learning Medium"
            onclick="window.open(this.href, 'pop-up', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;"
            title="Share on LinkedIn" >
            <i class="fa fa-linkedin-square"></i>
        </a>
        </span>
        <span>
        <a  href="mailto:?subject=Neural Networks: Cost Function and Backpropagation&amp;body=Check out this site https://machinelearningmedium.com/2017/10/03/neural-networks-cost-function-and-back-propagation/"
            title="Share via Email" >
            <i class="fa fa-envelope-square"></i>
        </a>
        </span>
    </span>

</span>
          </section>
            <h2 class="post-description">Intuition behind the idea of backpropagation and its extension to calculate cost function</h2>
          </div>
        </div>
        <div class="horizontal-divider">&#183; &#183; &#183;</div>
        <section class="post-content">
          <a name="topofpage"></a>
          <h3 id="basics-of-machine-learning-series">Basics of Machine Learning Series</h3>

<blockquote>
  <p><a href="/collection/basics-of-machine-learning">Index</a></p>
</blockquote>

<div class="horizontal-divider">· · ·</div>

<h3 id="notation">Notation</h3>
<ul>
  <li>\({(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots , (x^{(m)}, y^{(m)})}\) are the \(m\) training examples</li>
  <li>L is the <strong>total number of the layers</strong> in the network</li>
  <li>\(s_l\) is the <strong>number of units (not counting the bias unit)</strong> in the layer l</li>
  <li>K is the <strong>number of units in the output layer</strong></li>
</ul>

<p><img src="/assets/2017-10-03-neural-networks-cost-function-and-back-propagation/fig-1-neural-network-notations.png?raw=true" alt="Neural Network Notations" width="50%" /></p>

<ul>
  <li>For example in the network shown above,
    <ul>
      <li>L = 4</li>
      <li>\(s_1 = 3,\, s_2 = 3,\, s_3 = 2,\, s_4 = 2\)</li>
      <li>K = 2</li>
    </ul>
  </li>
</ul>

<p><strong>One vs All method is only needed if number of classes is greater than 2, i.e. if \(K \gt 2\), otherwise only one output unit is sufficient to build the model.</strong></p>

<h3 id="cost-function-of-neural-networks">Cost Function of Neural Networks</h3>
<p>Cost function of a neural network is a <strong>generalization of the cost function of the logistic regression</strong>. The <strong>L2-Regularized</strong> cost function of logistic regression from the post <a href="/2017/09/15/regularized-logistic-regression/" target="_blank">Regularized Logistic Regression</a>  is given by,</p>

<script type="math/tex; mode=display">J(\theta) = -{1 \over m} \sum_{i=1}^m \left( y^{(i)}\,log(h_\theta(x^{(i)})) + (1-y^{(i)})\,log(1 - h_\theta(x^{(i)})) \right) + {\lambda \over 2m } \sum_{j=1}^n \theta_j^2 \tag{1} \label{1}</script>

<ul>
  <li>Where
    <ul>
      <li>\({\lambda \over 2m } \sum_{j=1}^n \theta_j^2\) is the <strong>regularization term</strong></li>
      <li>\(\lambda\) is the <strong>regularization factor</strong></li>
    </ul>
  </li>
</ul>

<p>Extending (1) to then neural networks which can have K units in the output layer the cost function is given by,</p>

<script type="math/tex; mode=display">J(\theta) = -{1 \over m} \left[ \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)}\,log(h_\theta(x^{(i)}))_k + (1-y_k^{(i)})\,log(1 - (h_\theta(x^{(i)}))_k) \right] + {\lambda \over 2m } \sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} (\theta_{ji}^{(l)})^2 \tag{2} \label{2}</script>

<ul>
  <li>Where
    <ul>
      <li>\(h_\Theta(x) \in \mathbb{R}^K \)</li>
      <li>\((h_\theta(x))_i\) is the \(i^{th}\) output</li>
    </ul>
  </li>
</ul>

<p>Here the summation term \(\sum_{k=1}^K\) is to <strong>generalize over the K output units</strong> of the neural network by calculating the cost function and summing over all the output units in the network. Also following the convention in regularization, the <strong>bias term in skipped from the regularization penalty</strong> in the cost function defination. Even if one includes the index 0, it would not effect the process in practice.</p>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% y_vec is the target vector in one-hot encoded matrix form</span>
<span class="c1">% y_hat is the prediction from the network after forward propagation</span>
<span class="n">J</span> <span class="o">=</span> <span class="nb">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="o">-</span> <span class="p">(</span><span class="n">y_vec</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">))</span> <span class="o">-</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">)</span> <span class="o">.*</span> <span class="nb">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)))</span> <span class="o">*</span> <span class="nb">ones</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>

<span class="c1">% vectorized implementation of regularization terms</span>
<span class="c1">% note that (2:end) ensures that the biases are not regularized</span>
<span class="c1">% which is also seen the equations (2) above</span>
<span class="n">reg_Theta1</span> <span class="o">=</span> <span class="nb">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">size</span><span class="p">(</span><span class="n">Theta1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="n">Theta1</span> <span class="o">.*</span> <span class="n">Theta1</span><span class="p">)(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">*</span> <span class="nb">ones</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">Theta1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">));</span>
<span class="n">reg_Theta2</span> <span class="o">=</span> <span class="nb">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">size</span><span class="p">(</span><span class="n">Theta2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="n">Theta2</span> <span class="o">.*</span> <span class="n">Theta2</span><span class="p">)(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">*</span> <span class="nb">ones</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">Theta2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">));</span>

<span class="c1">% final cost with regularization</span>
<span class="n">J</span> <span class="o">=</span> <span class="n">J</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambda</span> <span class="p">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">reg_Theta1</span> <span class="o">+</span> <span class="n">reg_Theta2</span><span class="p">));</span>
</code></pre></div></div>

<h3 id="backpropagation-algorithm">Backpropagation Algorithm</h3>
<p>Backpropagation algorithm is based on the <strong>repeated application of the error calculation</strong> used for gradient descent similar to the regression techniques, and since it is repeatedly applied in the <strong>reverse order starting from output layer and continuing towards input layer</strong> it is termed as backpropagation.</p>

<p>For a network with L layers the computation during <strong>foward propagation</strong>, for an input \((x, y)\) would be as follows,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    a^{(1)} &= x \\
    z^{(2)} &= \Theta^{(1)}\,a^{(1)} \\
    a^{(2)} &= g(z^{(2)}) \qquad (add\, a_0^{(2)})\\
    & \vdots \\
    a^{(L-1)} &= g(z^{(L-1)}) \qquad (add\, a_0^{(L-1)})\\
    z^{(L)} &= \Theta^{(L-1)}\,a^{(L-1)} \\
    a^{(L)} &= h_\Theta(x) = g(z^{(L)})
  \end{align} %]]></script>

<p>The \(h_\Theta(x)\) is the prediction. In order to reduce the error between the prediction and the actual value backpropagation is used. Say, \(\delta_j^{(l)}\) is the <strong>error of node j in the layer l</strong> is associated with the prediction made at that node given by \(a_j^{(l)}\), then backpropagation aims to calculate this error term propating backwards starting from the output unit in the last layer (layer L in the example above).</p>

<p>So for each output unit in layer L, the error term is given by, \(\delta_j^{(L)} = a_j^{(L)} - y_j\) which can be vectorized and written as,</p>

<script type="math/tex; mode=display">\delta^{(L)} = a^{(L)} - y \tag{3} \label{3}</script>

<ul>
  <li>Where \(a^{(L)}\) is \(h_\Theta(x)\).</li>
</ul>

<p>Now the error terms for the previous layers are calculated as follows,</p>

<script type="math/tex; mode=display">\delta^{(l)} = (\Theta^{(l)})^T\,\delta^{(l+1)} .* g'(z^{(l)}) \tag{4} \label{4}</script>

<ul>
  <li>Where
    <ul>
      <li>\(g’(z^{(l)}) = a^{(l)} .* (1 - a^{(l)}) \)</li>
      <li>.* is the element-wise multiplication.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>This backward propagation stops at l = 2 because l = 1 correponds to the input layer and no weights needs to be calculated there.</p>
</blockquote>

<p>Now the the gradient for the cost function which is needed for the minimization of the cost function is given by,</p>

<script type="math/tex; mode=display">\frac {\partial J} {\partial \Theta_{ij}^{(l)} } = a_j^{(l)}\, \delta_i^{(l+1)}  \tag{5} \label{5}</script>

<ul>
  <li>Where regularization is ignored for the simplicity of expression.</li>
</ul>

<p><strong>Summarizing</strong> backpropagation:</p>

<p>Given training set \({(x^{(1)}, y^{(1)}), \cdots, (x^{(m)}, y^{(m)})}\)</p>

<ul>
  <li>Set \(\Delta_{ij}^{l} = 0\) for all (i, j, l)</li>
  <li>For i = 1 to m:
    <ul>
      <li>Set \(a^{(1)} = x^{(i)} \)</li>
      <li>Perform forward propagation to compute \(a^{(l)}\) for l = 1, …, L</li>
      <li>Using \(y^{(i)}\) compute \( \delta^{(L)} = a^{(L)} - y^{(i)} \)</li>
      <li>Compute \(\delta^{(L-1)}, \cdots, \delta^{(2)}\) using backpropagation</li>
      <li>\(\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)} \delta_i^{(l+1)} \)</li>
    </ul>
  </li>
</ul>

<p><strong>Vectorized implementation of the equation above is given by,</strong></p>

<script type="math/tex; mode=display">\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}\,(a^{(l)})^T \tag{6} \label{6}</script>

<ul>
  <li>\(D_{ij}^{(l)} := {1 \over m} \Delta_{ij}^{(l)} + \lambda \, \Theta_{ij}^{(l)} \) if \(j \ne 0\)</li>
  <li>\(D_{ij}^{(l)} := {1 \over m} \Delta_{ij}^{(l)} \) if \(j = 0\)</li>
  <li>And finally, \( \frac {\partial} {\partial \Theta_{ij}^{(l)} } = D_{ij}^{(l)} \)</li>
</ul>

<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">% example of backward propgation of error signal</span>
<span class="n">del_3</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y_vec</span><span class="p">;</span>
<span class="n">del_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">del_3</span> <span class="o">*</span> <span class="n">Theta2</span><span class="p">)(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">.*</span> <span class="n">sigmoidGradient</span><span class="p">(</span><span class="n">z2</span><span class="p">);</span>

<span class="c1">% calculation of partial derivatives from the error signals</span>
<span class="n">Theta2_grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">del_3</span><span class="o">'</span> <span class="o">*</span> <span class="n">a2</span><span class="p">)</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>
<span class="n">Theta1_grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">del_2</span><span class="o">'</span> <span class="o">*</span> <span class="n">a1</span><span class="p">)</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>

<span class="c1">% calculation of regularization parameters</span>
<span class="c1">% (2:end) ensures no regularization of biases</span>
<span class="c1">% as seen in equation (6)</span>
<span class="n">Theta2_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">=</span> <span class="n">Theta2_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="n">Theta2</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>
<span class="n">Theta1_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">=</span> <span class="n">Theta1_grad</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span> <span class="o">*</span> <span class="n">Theta1</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span> <span class="p">/</span> <span class="n">m</span><span class="p">;</span>
</code></pre></div></div>

<p>The capital delta matrix \(D\), is used as an <strong>accumulator</strong> to add up the values as backpropagation proceeds and finally compute the partial derivatives.</p>

<p>The complete vectorized implementation for the MNIST dataset using vanilla neural network with a single hidden layer can be found <a href="https://github.com/shams-sam/CourseraMachineLearningAndrewNg/tree/master/Assignments/machine-learning-ex4" target="_blank"><strong>here</strong></a>.</p>

<h3 id="backpropagation-intuition">Backpropagation Intuition</h3>

<p>Say \((x^{(i)}, y^{(i)})\) is a training sample from a set of training examples that the neural network is trying to learn from. If the cost function is applied to this single training sample while setting \(\lambda = 0\) for simplicity, then \eqref{2} can be reduced to,</p>

<script type="math/tex; mode=display">J(\Theta) = - {1 \over m} \left [  \sum_{i=1}^m y^{(i)} log(h_\Theta(x^{(i)})) + (1-y^{(i)}) log(1-h(_\Theta(x^{(i)}))) \right ]</script>

<p>where,</p>

<script type="math/tex; mode=display">cost(i) = y^{(i)} log(h_\Theta(x^{(i)})) + (1-y^{(i)}) log(1-h(_\Theta(x^{(i)}))) \tag{7} \label{7}</script>

<p>which is very similar to the cost for a logistic regression, which can also be seen analogous to the cost for a linear regression (<strong>mean-squared error</strong>), i.e.</p>

<script type="math/tex; mode=display">cost(i) \approx (h_\Theta(x^{(i)}) - y^{(i)})^2</script>

<p>This basically gives a magnitude of how well the network is doing in prediction of the output for a single training sample.</p>

<p>Formally, the <strong>\(\delta\) terms are the partial derivatives of the cost function</strong> given by,</p>

<script type="math/tex; mode=display">\delta_j^{(l)} = \frac {\partial} {\partial z_j^{(l)}} cost(i) \tag{8} \label{8}</script>

<p>where \(cost(i)\) is given by \eqref{7}.</p>

<p>So, \eqref{8} conveys mathematically the intent to change the cost function (by changing the network parameters), in order to effect the intermediate values calculated in \(z’s\), so as to minimize the differences in the final output of the network.</p>

<p>The basis of backpropagation is benched on the propagating the error term calculated for the final layer using \eqref{3} and \eqref{4} backwards to the preceding layers.</p>

<p>For more on mathematics of backpropagation, refer <a href="/2018/03/20/backpropagation-derivation/"><strong>Mathematics of Backpropagation</strong></a>. For an approximate implementation of backpropagation using NumPy and checking results using Gradient Checking technique refer <a href="/2018/03/29/backpropagation-implementation-and-gradient-checking/"><strong>Backpropagation Implementation and Gradient Checking</strong></a>.</p>

<h2 id="references">REFERENCES:</h2>

<p><small><a href="https://www.coursera.org/learn/machine-learning/lecture/na28E/cost-function" target="_blank">Machine Learning: Coursera - Cost Function</a></small></p>

        </section>
        <footer class="post-footer">
          <div class="wrap">
            
              <div class="tile">
                <div class="text">
                <a href="/tag/machine-learning" class='category-links'><h1>machine-learning</h1></a>
                </div>
              </div>
            
              <div class="tile">
                <div class="text">
                <a href="/tag/andrew-ng" class='category-links'><h1>andrew-ng</h1></a>
                </div>
              </div>
            
          </div>
        </footer>
        <div class="horizontal-divider">· · ·</div>
        <!-- <div class="bottom-teaser cf">
          <div class="isLeft">
            <h5 class="index-headline featured"><span>Written by</span></h5>
            <section class="author">
              <div class="author-image" style="background-image: url(/assets/images/author.jpg)">Blog Logo</div>
              <h4>Shams S</h4>
              <p class="bio">Research Assistant @ Purdue University</p>
              <hr>
              <p class="published">Published on <time datetime="2017-10-03 00:00">03 Oct 2017</time></p>
            </section>
          </div>
          <div class="isRight">
            <h5 class="index-headline featured"><span>Supported by</span></h5>
            <footer class="site-footer">
              <section class="poweredby">Proudly published with <a href="http://jekyllrb.com"> Jekyll</a></section>
              <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> You should subscribe to my feed.</span></a>
              <div class="inner">
                <section class="copyright">All content copyright <a href="https://machinelearningmedium.com/">Machine Learning Medium</a> &copy; 2021<br>All rights reserved.</section>
              </div>
            </footer>
          </div>
        </div> -->
        <br>
        
        <div id="vc-feelback-main" data-access-token="334e9d2a5dc341469cda15d8c4bd935e" data-display-type="4"></div>
        
        <nav class="page-navigation" role="navigation">
  
    <a href=/2017/09/27/neural-network-intuition/ class="arrow-button left-arrow-button"><span class="left-arrow"></span><span class="label left-label">Neural Networks Intuition</span></a>
  
  
    <a href=/2017/10/27/number-system/ class="arrow-button right-arrow-button"><span class="label right-label">What is Number System ?</span><span class="right-arrow"></span></a>
  
</nav>
        
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE */
            var disqus_shortname = 'shams-sam'; /* required: replace example with your forum shortname */

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        
      </article>
    </main>
    <!-- footer start -->
<div class="footer-container">

<footer class="site-footer">
  <a class="subscribe" href="/feed.xml"> <span class="tooltip"> <i class="fa fa-rss"></i> Subscribe!</span></a>
    <div class="inner">
         <section class="copyright">All content copyright <a href="/">Machine Learning Medium</a> &copy; 2021 &bull; All rights reserved.</section>
         <section class="poweredby">Made with <a href="http://jekyllrb.com"> Jekyll</a></section>
         <section class="poweredby">Inspired by <a href="https://medium.com/"> Medium</a> and <a href="https://github.com/dirkfabisch/mediator">Mediator</a></section>
    </div>
</footer>

<div class="bottom-closer">
  <div class="background-closer-image"  style="background-image: url(/assets/images/footer.jpg)">
    Image
  </div>
  <div class="inner">
    <h1 class="footer-title">Machine Learning Medium</h1>
    <h2 class="footer-description">Recursing the Rabbit Hole</h2>
    <a href="https://sshamsazam.com/" target="_blank" class="btn">About Me</a>
  </div>
</div>

<!-- footer end -->
</div>

    <script src="https://code.jquery.com/jquery-1.11.1.min.js"></script>
<script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
<script type="text/javascript" src="/assets/js/readingTime.min.js"></script>
<script type="text/javascript" src="/assets/js/index.js"></script>
<script>
(function ($) {
  "use strict";
  $(document).ready(function(){

    var $window = $(window),
    $image = $('.post-image-image, .teaserimage-image');
    
      $window.on('scroll', function() {
        var top = $window.scrollTop();

        if (top < 0 || top > 1500) { return; }
        $image
          .css('transform', 'translate3d(0px, '+top/3+'px, 0px)')
          .css('opacity', 1-Math.max(top/700, 0));
      });
      $window.trigger('scroll');

      var height = $('.article-image').height();
      $('.post-content').css('padding-top', height + 'px');

      $('a[href*=#]:not([href=#])').click(function() {
        if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'')
         && location.hostname == this.hostname) {
          var target = $(this.hash);
          target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
          if (target.length) {
            $('html,body').animate({ scrollTop: target.offset().top }, 500);
            return false;
          }
        }
      });

  });
}(jQuery));
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script src="/public/js/typed.js"></script>
<script>
  $(function(){
    $(".blog-description").typed({
      strings: ['Recursing the Rabbit Hole'],
      typeSpeed: 50,
      backSpeed: 25,
      loop: true
    });
  });
</script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      },
      CommonHTML: {matchFontHeight: false},
      "HTML-CSS": {matchFontHeight: false},
      SVG: {matchFontHeight: false}
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>



<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-108148924-1', 'auto');
ga('send', 'pageview');

</script>


<div id="fb-root"></div>

<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = 'https://connect.facebook.net/en_GB/sdk.js#xfbml=1&version=v2.10&appId=391866664565981';
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>


<script> 
(function() { 
  var v = document.createElement('script'); v.async = true; 
  v.src = "https://assets-prod.vicomi.com/vicomi.js?token=334e9d2a5dc341469cda15d8c4bd935e"; 
  var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(v, s); 
})(); 
</script>

  </body>
</html>
