







var tipuesearch = {"pages": [

  
  
  
  {
    "title": "Backpropagation Implementation and Gradient Checking",
    "text": "Basics of Machine Learning Series Index · · · Backpropagation NumPy Example import numpy as np # define XOR training data X = np.array([ [0, 0], [0, 1], [1, 0], [1, 1], ]) y = np.atleast_2d([0, 1, 1, 0]).T print('X.shape:', X.shape) print('y.shape:', y.shape) # defining network parameters # [2, 2, 1] will also work for the XOR problem presented LAYERS = [2, 2, 2, 1] ETA = .1 THETA = [] # sigmoid activation function def sigmoid(x): return 1/(1+np.exp(-x)) # derivative of sigmoid activation function def sigmoid_prime(x): return sigmoid(x) * (1-sigmoid(x)) # calculating cost given the prediction and actual vectors def cost(y_hat, y): return np.mean([_ * _ for _ in (y_hat - y)]) # initialing THETA params for all the layers def initialize_parameters(): for idx in range(1, len(LAYERS)): THETA.append(np.random.rand(LAYERS[idx], LAYERS[idx-1]+1)) # vectorized forward propagation def forward_propagation(X,initialize=True): if initialize: initialize_parameters() # adding bias column to the input X A = [np.hstack((np.ones((X.shape[0],1)), X))] Z = [] activate = False for idx, theta in enumerate(THETA): Z.append(np.matmul(A[-1], theta.T)) # adding bias column to the output of previous layer A.append(np.hstack((np.ones((Z[-1].shape[0],1)), sigmoid(Z[-1])))) # bias is not needed in the final output A[-1] = A[-1][:, 1:] y_hat = A[-1] return A, Z, y_hat # vectorized backpropagation def back_propagation(X, y, initialize=True, debug=False, verbose=False): # run a forward pass A, Z, y_hat = forward_propagation(X, initialize) # calculate delta at final output del_ = [(y_hat - y) * sigmoid_prime(Z[len(Z)-1])] if verbose: print(cost(y_hat, y)) # flag to signify whether a layer has bias column of not bias_free = True # running in reverse because delta is propagated backwards for idx in reversed(range(1, len(THETA))): if bias_free: # true only for the final layer where there is no bias temp = np.matmul(del_[0], THETA[idx]) * np.hstack((np.ones((Z[idx-1].shape[0], 1)), sigmoid_prime(Z[idx-1]))) bias_free=False else: # true for all the layers except the input and output layer temp = np.matmul(del_[0][:,1:], THETA[idx]) * np.hstack((np.ones((Z[idx-1].shape[0], 1)), sigmoid_prime(Z[idx-1]))) del_ = [temp] + del_ del_theta = [] bias_free = True # calculation for the delta in the parameters for idx in reversed(range(len(del_))): if bias_free: # true only for the final layer where there is no bias del_theta = [-ETA * np.matmul(del_[idx].T, A[idx])] + del_theta bias_free = False else: # true for all the layers except the input and output layer del_theta = [-ETA * np.matmul(del_[idx][:, 1:].T, A[idx])] + del_theta # update parameters for idx in range(len(THETA)): # asserting that the matrix sizes are same assert THETA[idx].shape == del_theta[idx].shape THETA[idx] = THETA[idx] + del_theta[idx] if debug: return (A, Z, y_hat, del_, del_theta) # training epochs initialize=True verbose=True THETA=[] for i in range(10000): if i % 1000 == 0: verbose=True back_propagation(X, y, initialize, debug=False, verbose=verbose) verbose=False initialize=False # inference after training A, Z, y_hat = forward_propagation(X, initialize=False) # final output of the network print(y_hat) Sometimes, it can been seen that the network get stuck over a few epochs and then continues to converge quickly. It might be due to the fact that the implementation of neural network above is not the most optimum because of using the mean squared error cost function which is not the recommended for classification purposes because of the issues explained in Classification and Logistic Regression. Gradient Checking Often times, it is normal for small bugs to creep in the backpropagtion code. There is a very simple way of checking if the written code is bug free. It is based on calculating the slope of cost function manually by taking marginal steps ahead and behind the point at which the gradient is returned by backpropagation. As visible in the plot above, the gradient approximation can be calculated using the centred difference formula as follows, The approximation in \\eqref{1}, works better than the single sided difference, i.e., This is basically because if the error is approximated for the terms using Taylor expansion, it is observed that \\eqref{1} has error of order \\(O(h)\\) while \\eqref{2} has error of order \\(O(h^2)\\), i.e. second order approximation. With multiple parameter matrices, using \\eqref{1}, the derivative with respect to \\(\\Theta_j\\) can be approximated as, Using the NumPy example of backpropagation, gradient checking can be confirmed as follows: # set epsilon and the index of THETA to check epsilon = 0.01 idx = 0 # reset THETA and run backpropagation THETA=[] A, Z, y_hat, del_, del_theta = back_propagation(X, y, True, True, verbose=True) # calculate the approximate gradients using centered difference formula grad_check = np.zeros(THETA[idx].shape) for i in range(THETA[idx].shape[0]): for j in range(THETA[idx].shape[1]): THETA[idx][i][j] = THETA[idx][i][j] + epsilon A, Z, y_hat = forward_propagation(X, initialize=False) J_plus_epsilon = cost(y_hat, y) THETA[idx][i][j] = THETA[idx][i][j] - 2* epsilon A, Z, y_hat = forward_propagation(X, initialize=False) J_minus_epsilon = cost(y_hat, y) grad_check[i][j] = (J_plus_epsilon - J_minus_epsilon)/ (2*epsilon) THETA[idx][i][j] = THETA[idx][i][j] + epsilon print(\"from backprop:\") print(del_theta[idx] / (-2*ETA)) print(\"from grad check:\") print(grad_check) Gives output: from backprop: [[ 0.00351937 0.00136117 0.00141332] [ 0.00254422 0.00088405 0.00082788]] from grad check: [[ 0.00353115 0.00136459 0.00141917] [ 0.00255167 0.00088687 0.00082796]] It can be seen that the two values are very similar and hence it proves that the back propagation is working well. Once the gradient checking is done, it should be turned off before running the network for entire set of training epochs. This is because the numerical approximation of gradient as explained works well for checking the results of backpropagation, but in practice the calculations are much slower than backpropagation and would slow down the training process. Note: Gradient from backpropagation is adjusted linearly by division with 2. Because of the approximate implementation of backpropagation there is a linear scaling in the derivatives given by the backpropagation function. This still works fine for the overall network because the application of learning rate nullifies the minor scaling error. Let me know if anyone can point out the mistake. REFERENCES: Machine Learning: Coursera - Gradient Checking CS231n Gradient checks",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2018/03/29/backpropagation-implementation-and-gradient-checking/"
  },

  
  
  
  {
    "title": "Backpropagation Derivation",
    "text": "Basics of Machine Learning Series Index · · · Derivative of Sigmoid The sigmoid function, represented by \\(\\sigma\\) is defined as, So, the derivative of \\eqref{1}, denoted by \\(\\sigma’\\) can be derived using the quotient rule of differentiation, i.e., if \\(f\\) and \\(g\\) are functions, then, Since \\(f\\) is a constant (i.e. 1) in this case, \\eqref{2} reduces to, Also, by the chain rule of differentiation, if \\(h(x) = f(g(x))\\), then, Applying \\eqref{3} and \\eqref{4} to \\eqref{1}, \\(\\sigma’(x)\\) is given by, Mathematics of Backpropagation (* all the derivations are based scalar calculus and not the matrix calculus for simplicity of calculations) In most of the cases of algorithms like logistic regression, linear regression, there is no hidden layer, which basically zeroes down to the fact that there is no concept of error propagation in backward direction because there is a direct dependence of model cost function on the single layer of model parameters. Backpropagation tries to do the similar exercise using the partial derivatives of model output with respect to the individual parameters. It so happens that there is a trend that can be observed when such derivatives are calculated and backpropagation tries to exploit the patterns and hence minimizes the overall computation by reusing the terms already calculated. Consider a simple neural network with a single path (following the notation from Neural Networks: Cost Function and Backpropagation) as shown below, where, where \\(g\\) is a linear function defined as \\(g(x) = x\\), and hence \\(g’(x) = 1\\). \\(\\sigma\\) represents the sigmoid function. For the simplicity of derivation, let the cost function, \\(J\\) be defined as, where, Now, in order to find the changes that should be made in the parameters (i.e. weights), partial derivatives of the cost function is calculated w.r.t. individual \\(\\theta’s\\), One can see a pattern emerging among the partial derivatives of the cost function with respect to the individual parameters matrices. The expressions in \\eqref{9}, \\eqref{10} and \\eqref{11} show that each term consists of the derivative of the network error, the weighted derivative of the node output with respect to the node input leading upto that node. So, for this network the updates for the matrices are given by, Forward propagation is a recursive algorithm takes an input, weighs it along the edges and then applies the activation function in a node and repeats this process until the output node. Similarly, backpropagation is a recursive algorithm performing the inverse of the forward propagation, i.e. it takes the error signal from the output layer, weighs it along the edges and performs derivative of activation in an encountered node until it reaches the input. This brings in the concept of backward error propagation. Error Signal Following the concept of backward error propagation, error signal is defined as the accumulated error at each layer. The recursive error signal at a layer l is defined as, Intuitively, it can be understood as the measure of how the network error changes with respect to the change in input to unit \\(l\\). So, \\(\\delta^{(4)}\\) in \\eqref{8}, can be derived using \\eqref{13}, Similary the error signal at previous layers can be derived and it can be seen how the error signal of the forward layers get transmitted to the backward layers Using \\eqref{14}, \\eqref{15} and \\eqref{16}, \\eqref{12} can be written as, which is nothing but the updates to individual parameter matrices based on partial derivatives of cost w.r.t. individual matrices. Activation Function Generally, the choice of activation function at the output layer is dependent on the type of cost function. This is mainly to simplify the process of differentiation. For example, as shown in the example above, if the cost function is mean-squared error then choice of linear function as activation for the output layer often helps simplify calculations. Similarly, the cross-entropy loss works well with sigmoid or softmax activation functions. But this is not a hard and fast rule. One is free to use any activation function with any cost function, although the equations for partial derivatives might not look as nice. Similarly, the choice of activation function in hidden layers are plenty. Although sigmoid functions are widely used, they suffer from vanishing gradient as the depth increases, hence other activations like ReLUs are recommended for deeper neural networks. REFERENCES: Artificial Neural Networks: Mathematics of Backpropagation (Part 4) Which activation function for output layer?",
    "tags": "andrew-ng machine-learning mathematics basics-of-machine-learning",
    "url": "/2018/03/20/backpropagation-derivation/"
  },

  
  
  
  {
    "title": "Program in PyTorch",
    "text": "Introduction Pytorch provides two high-level features: Tensor computation analogous to numpy but with option of GPU acceleration. Deep Neural Networks built on a tape-based autograd system. And is generally used either as replacement for NumPy (for the GPU acceleration) or as a deep learning research platform. Components of PyTorch torch: Tensor library like NumPy with GPU support. torch.autograd: Automatic differentiation library that supports all differentiable Tensor operations. torch.nn: neural network library integrated with autograd. torch.optim: optimization package used along with torch.nn with standard optimization methods like SGD, RMSProp, LBGFS etc. torch.multiprocessing: python multiprocessing, but with memory sharing of Tensors across processes. torch.utils: Data loader, trainer and other utility functions. torch.legacy(.nn/.optim): legacy code that is ported for backward compatibility. Tensors Tensors in torch are analogous to ndarrays in NumPy but differ in that Tensors in torch can be loaded on to GPU for hardware acceleration. Tensors can be initialized by calling a normal Tensor object or using special purpose functions like torch.rand. The size function gives the dimension of the Tensor initialized. Unlike Tensors in TensorFlow, the ones in PyTorch can be seen after initialization without running a session. # for python 2.* users from __future__ import print_function import torch x = torch.Tensor(4, 3) print(x) x = torch.rand(4, 3) print(x) print(x.size()) Tensor Operations PyTorch gives various options and aliases for operations on tensors as can be seen for addition below y = torch.rand(4, 3) # addition using + operator print(x + y) # addition using add function print(torch.add(x, y)) # addition using out parameter of add function result = torch.Tensor(5, 3) torch.add(x, y, out=result) # result is casted for the new dimensions [4*3] print(result) # in-place addition y.add_(x) print(y) Standard NumPy-like indexing works on PyTorch Tensors. print(x[:, 1]) Resizing can be done using torch.view. x = torch.randn(4, 4) y = x.view(16) # the size -1 is inferred from other dimensions z = x.view(-1, 8) print(x.size(), y.size(), z.size()) Some operations available on Tensors are: torch.numel: returns the total number of elements in a Tensor. torch.eye: returns a 2D tensor representing an identity matrix. torch.from_numpy: create a Tensor from NumPy array where the two share the same memory and modifications are reflected across. torch.linspace: returns a 1D tensor of equally spaced steps with start and end of a range. torch.ones: returns tensor of a defined shape filled with scalar value 1. torch.zeros: returns tensor of a defined shape filled with scalar value 0. torch.cat: concatenate tensors. torch.chunk: splits the tensors into chunks. CUDA Tensors Moving Tensors to CUDA is as simple as calling .cuda method. Calling a simple function, torch.cuda.is_available checks if CUDA is available. The type of a variable moved to GPU differs from the ones not on GPU, and hence addition would lead to TypeError. if torch.cuda.is_available(): x = x.cuda() y = y.cuda() print(x+y) x = torch.rand(4, 3) # raises TypeError because of Type Mismatch [torch.FloatTensor, torch.cuda.FloatTensor] print(x + y) Autograd: Automatic Differentiation As the name suggests, the autograd package provides automatic differentiation for all operations on Tensors. The define-by-run framework ensures that the backprop is defined by how the code is run, and hence every single iteration can be different allowing dynamic modifications between epochs during training which is not possible in other static libraries like TensorFlow, Theano etc. which require a graph compilation by running something like a session and any change in network requires a recompilation of the graph. Variable autograd.Variable is the main class under the autograd package, which wraps a tensor along with almost all of operations defined on it. Upon completion of a process, .backward method can be called to calculate all the gradients in the backward direction making the back propagation a very minor automatic step in designing the network. .data gives the raw tensor in the variable. .grad gives the gradient w.r.t. this variable. The other important class in autograd package is the Function class. Variable and Function are interconnected to build an acyclic graph, the encodes the complete history of computation. Every variable has a .grad_fn attribute that references the Function that has created the Variable. The variables created by user have grad_fn as None. In order to compute derivatives, .backward function can be called on a variable. if Variable is a scalar, .backward does not require any argument. if Variable holds more types of elements, a gradient argument is defined, which is a tensor of a matching shape. import torch from torch.autograd import Variable x = Variable(torch.ones(2, 2), requires_grad=True) y = x + 2 print(y.grad_fn) z = y * y * 3 out = z.mean() print(z, out) out.backwards() print(x.grad) Let the out variable be \\(o\\), then, where \\(z_i\\) is given by, So \\(o\\) in \\eqref{1} can be written as, Differentiating w.r.t. x, The autograd package can be in general seen as the library that implements these basic differentials and then carefully employs the chain rule to generate gradients for the most complex functions in the program also. This simplifying of the process helps achieve gradients on the fly instead of predefining it for a graph. Neural Networks The neural network package, torch.nn, depends heavily on the autograd package to define the models and differentiate them. A basic training process in the neural network involves the following steps: define the neural network with learnable parameters (i.e. weights) iterate over the training data. feed the input through the network. compute the loss or other error metrics. propagate the gradients back into the network parameters. update the weights. Define a network: import torch from torch.autograd import Variable import torch.nn as nn import torch.nn.functional as F # inherit from nn.Module class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features net = Net() print(net) Only the forward function needs to be defined because the backward function is already defined using autograd. And the learnable parameters of the network are returned by net.parameters. params = list(net.parameters()) print(len(params)) print(params[0].size()) The input to the forward method is autograd.Variable, and so is the output. input = Variable(torch.randn(1, 1, 32, 32)) out = net(input) print(out) The gradients of all the parameters should be reset to zero before calling the backprops. net.zero_grad() out.backward(torch.randn(1, 10)) torch.nn only supports mini-batches, i.e. input to any nn layer is a 4D Tensor of samples * channels * height * width. If a single sample, input.unsqueeze(0) adds a fake dimension. Loss Function A loss function takes (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target. output = net(input) target = Variable(torch.arange(1, 11)) # a dummy target, for example criterion = nn.MSELoss() loss = criterion(output, target) print(loss) Using the .grad and .next_functions one can see the entire graph in the backward direction from the loss. Upon calling the .backward function the whole graph is differentiated w.r.t. the loss, and all .grad variables are updated with the accumulated gradients. Before calling the backward function, existing gradients need to be cleared or they will be accumulated along with the existing gradients, if any. net.zero_grad() print('conv1.bias.grad before backward') print(net.conv1.bias.grad) loss.backward() print('conv1.bias.grad after backward') print(net.conv1.bias.grad) The simplest way to update the weights is the Stochastic Gradient Descent (SGD). learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) Apart from this, PyTorch also allows to use various other algorithms for the purpose of optimizing the network parameters. This is enabled by using the torch.optim package that implements most of these methods. import torch.optim as optim optimizer = optim.SGD(net.parameters(), lr=0.01) optimizer.zero_grad() output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() It can be seen that, even the optimizer package requires the manual reseting of the gradient buffer before backpropagation is invoked, to prevent the accumulation of gradients from different calls. Handling Data Data may deal with one of the formats, namely, image, text, audio or video. Standard python packages may be used to load the datasets into the NumPy arrays, which can then easily be converted into the Tensors because of the seamless bridge between the two libraries. Following packages are recommended: Images: Pillow, OpenCV Audio: scipy, librosa Text: raw Python or Cython based loading, or NLTK and SpaCy Specifically for vision, the package torchvision is defined, that has loaders for common datasets such as Imagenet, CIFAR10, MNIST etc. and data transformers for images. REFERENCES: PyTorch Official Page PyTorch - Wikipedia PyTorch - Tensor Torch (machine learning) - Wikipedia",
    "tags": "machine-learning pytorch library",
    "url": "/2018/03/08/program-in-pytorch/"
  },

  
  
  
  {
    "title": "Images, Noise and Filters",
    "text": "Basics to Computer Vision Series Index · · · Images as Function Images are generally associated with concepts of vision and perception. But in the field of computer vision it is important to understand that image can be represented as a function as well. An image can be represented as a function \\(I(x, y)\\) which gives the intensity represented by height of the bar for a given coordinate \\(x\\) and \\(y\\). So basically, an image can be thought of as one of the following: a matrix of numbers pixel intensity as a function of coordinates output of a camera Theoretically, an image can be modelled as a function \\(f\\) or \\(I\\) from \\(\\mathbb{R}^2 \\to \\mathbb{R}\\) where \\(f(x,y)\\) gives the intensity or value at position \\((x,y)\\). Practically, an image can be modelled over a rectangle with a finite range, given by: Similarly, color images are three functions stacked together representing the three channels (i.e. often R, G and B), written as, vector-valued functions (i.e. every pixel is a vector of numbers), Such as function can be generalized as, Digital Images Another important realization in the computer vision is the fact that images in a computer system are discrete images and not continuous ones like human eyes see, i.e. images in computer have a matrix representation with non-continuous intensity values between two adjacent pixel which might not be the case for the actual ground truth image it is capturing. Discretization of the digital image is two fold: Sample the actual image onto a 2D space of a regular matrix Quantize the intensity values (as digital images do not take continuous values for intensities) to the nearest integer. Such operations often will lead to loss of some information but if often minimal and can be worked with. Loading and Exploring Image % import image and display img = imread('tree.jpg'); imshow(img); % load the image package pkg load image; % size of 3 channel image disp(size(img)); % red channel extraction img_red = img(:, :, 1); imshow(img_red); plot(img_red(80, :)); % convert to grayscale gry = rgb2gray(img); % size and class of image disp(size(img)); disp(class(img)); % extract intensities from image disp(gry(50, 100)); plot(gry(50, :)); Slicing a matrix is same as cropping. % cropping image by slicing disp(gry(101:103, 201:203)); Multiplying a matrix with a scalar (i.e scaling an image) helps adjust the brightness, i.e. if scalar greater than 1, the image becomes brighter else it gets darker because pixel value 255 represents white and 0 represents black. % change brightness function result = scale(image, value) result = image .* value; endfunction imshow(scale(img, 2)); imshow(scale(img, 0.5)); Since images can be treated as functions and are represented as matrices, they can undergo matrix operations such as addition substraction. % import test images img_1 = imread('cycle.jpeg'); img_2 = imread('tree.jpg'); min_h = min(size(img_1)(1), size(img_2)(1)); min_w = min(size(img_1)(2), size(img_2)(2)); img_1 = img_1(1:min_h, 1:min_w, :); img_2 = img_2(1:min_h, 1:min_w, :); % add images imshow(img_1 + img_2); imshow(img_1./2 + img_2./2); % difference of images imshow(img_1 - img_2); imshow(img_2 - img_1); imshow(img_1 - img_2 + img_2 - img_1); imshow(imabsdiff(img_1, img_2)); The default data type of image pixels in many environments (including octave and matlab) is 8-bit unsigned integer which has range [0, 255]. So special care must be taken during addition and subtraction because values can often go out of this range and be clipped at lower and higher limit. Alpha blending has roots in weighted addition of two images to maintain pixel limits within the range. function result = blend(image_1, image_2, alpha) result = alpha .* image_1 + (1-alpha) .* image_2; endfunction imshow(blend(img_1, img_2, 0.75)); imshow(blend(img_1, img_2, 0.25)); The value \\(\\alpha\\) and \\((1 - \\alpha)\\) ensure that the pixel values during alpha blending does not overflow the limits of pixel intensities. Noise in Image Noise in an image can be represented with a function, i.e. where \\(\\eta\\) is the noise. Common types of noise functions are: Salt and pepper noise is random occurences of white and black pixels. imshow(imnoise(gry, 'salt &amp; pepper', 0.1)); Impulse noise has random occurences of white pixels only. Guassian noise has variations in intensity drawn from a Guassian normal distribution. imshow(imnoise(gry, 'gaussian')); noise = randn(size(gry)) .* 200; imshow(gry + noise); It is possible to check that the values produced by randn function follow a normal distribution by plotting the values produced by the function. % plotting randn noise = randn([1, 1000000]); [n, x] = hist(noise, linspace(-5, 5, 50)); plot(x, n); title('plot of randn function'); xlabel('x'); ylabel('n'); Filtering Noise filtering is the process of eliminating or reducing the effect of noise from an image. Moving Average Filter or mean filter is one of the basic filtering techniques that tries to smooth a noisy image by means of averaging values over a window in the image. as the window size increases the smoothness of the curve will increase. This generally does not mean that the quality of the image will be improved as it can cause a blurring effect. This averaging is based on the following assumptions: The value of a pixel at position must be similar to the ones nearby, surrounding it. The noise added to each pixel is independent of other noise values and hence would average to zero. Since noise is basically an addition of a noise function to the image function, it can be argued that it is possible to remove noise by performing the additive inverse, i.e. subtract noise function and hence retrieve the original image. But the fallacy in such an argument lies in the fact that the noise functions are generally not reproducible and would not have a standard form associated with them. They might have a standard statistical form such as following a given probabilistic distribution. Additive noise may also lead to loss of information if the value of pixels is scaled beyond the limits of the image pixel range. Effect of moving average filter can be seen on a noisy sine wave using basic octave operations, % moving average filters f_3 = fspecial('average', 3); f_10 = fspecial('average', 10); % adding noise to sine wave x = linspace(0, 20, 1000); sin_x = sin(x); noise = randn(size(sin_x)) .* 0.05; sin_x_noisy = sin_x + noise; % moving average filter on sin wave sin_x_filtered_10 = imfilter(sin_x_noisy, f_10); sin_x_filtered_3 = imfilter(sin_x_noisy, f_3); plot( x, sin_x_filtered_10, 'r', x, sin_x_filtered_3, 'g', x, sin_x_noisy, 'b' ); title('effect of moving average filter on noisy sine wave'); xlabel('x'); ylabel('sin(x)'); legend ( 'moving average with window size 10', 'moving average with window size 3', 'noisy sin(x)' ); Similarly it can be applied to an image, % import library pkg load image; img = imread('tree.jpg'); img = rgb2gray(img); imshow(img); % adding noise to image sigma = 20; noise = randn(size(img)) .* sigma; img_noisy = img + noise; imshow(img_noisy); % moving average filter on image img_filtered_3 = imfilter(img_noisy, f_3); img_filtered_10 = imfilter(img_noisy, f_10); x = linspace(100, 200, 101); plot( x, img_filtered_10(10, 100:200), 'r', x, img_filtered_3(10, 100:200), 'g', x, img_noisy(10, 100:200), 'b' ); title('plot over column range [100, 200] for row 10'); xlabel('column'); ylabel('pixel intensity'); legend ( 'moving average with window size 10', 'moving average with window size 3', 'noisy image' ); w = size(img)(2); x = linspace(0, w, w); plot( x, img_filtered_10(100, :), '+', x, img_filtered_3(100, :), '.', x, img_noisy(100, :), '*' ); title('plot for row 100'); xlabel('column'); ylabel('pixel intensity'); legend ( 'moving average with window size 10', 'moving average with window size 3', 'noisy image' ); Weighted Moving Average takes the assumption a step further than the moving average, positing that if a pixel is similar to nearby pixels then it should be more dependent on the nearer ones than on the farther ones. This information is encoded as weighted average for such filtering. Effect of weighted moving average vs that of moving average can be seen below, % weighted moving average for 1D % assuming weights to be an odd sized vector function result = weighted_moving_average(series, weights) size_weights = size(weights)(2); size_series = size(series)(2); padding = zeros(1, size_weights/2); size_padding = size(padding)(2); series_padded = [padding, series, padding]; for idx = (size_padding + 1) : (size_series + size_padding) series(idx-size_padding) = mean( series_padded(idx-size_padding: idx-size_padding+size_weights-1) ./sum(weights) .*weights); endfor result = series; endfunction % weighted moving average over a random vector n = 3; x = linspace(1, n, n*10); sin_x = sin(x); noise = randn(1, n*10) .* 0.1; sin_x = noise + sin_x; % uniform weights weights = [1, 1, 1, 1, 1]; sin_x_mov_avg = weighted_moving_average(sin_x, weights); % center biased weights weights = [1, 2, 4, 2, 1]; sin_x_wgt_avg = weighted_moving_average(sin_x, weights); plot( x, sin_x_mov_avg .* 5, 'r', x, sin_x_wgt_avg .* 5, 'b', x, sin_x, 'g' ); title('moving average vs weighted moving average'); xlabel('x'); ylabel('sin(x)'); legend('moving average', 'weighted moving average', 'noisy sin(x)'); The weight masks generally used are odd sized, as this makes the mask centred around the central pixel. Also, the results are divided by the sum of the weights to scale the results back to one. The advantage of weighted moving average over normal moving average can be seen in Fig. 5. In region A and B it is clear that while the data (green plot) and weighted moving average (blue plot) are moving in one direction the normal moving average (red plot) is deviating in the other direction. This is usually becuase the normal moving average gives excessive importance to farther off pixels and hence would not catch sudden trend shifts accurately. Correlation and Cross Correlation Filtering Filtering in 2D is very similar to the filtering explained in the section above for the 1D filters on noisy signals. The only point of difference lies in the dimensions of the filter kernel i.e. in 2D filtering the filters are 2D. But the octave code for 2D filtering remains the same as shown below using fspecial function. % function to plot with labels function plot_with_labels(matrix, color='white') size_x = size(matrix, 1); size_y = size(matrix, 2); imagesc(matrix); hold on; [X Y] = meshgrid(1:size_x, 1:size_y); string = mat2cell(transpose(matrix), ones(size_x, 1), ones(1, size_y)); text( Y(:)-.45,X(:)+.15, string, 'HorizontalAlignment','left', 'VerticalAlignment', 'middle', 'color', color, 'fontsize', 12); grid_x = .5:1:(size_x + .5); grid_y = .5:1:(size_y + .5); grid1 = [grid_x;grid_y]; grid2 = repmat([.5;size_x + .5],1,length(grid_x)); plot(grid1,grid2,'k'); plot(grid2,grid1,'k'); endfunction % moving average in 2D img = zeros(10, 10); img(3:7, 4:8) = 90; img(6, 5) = 0; img(9, 3) = 90; f_3 = fspecial('average', 3); f_5 = fspecial('average', 5); subplot(3, 1, 1); plot_with_labels(img); title('original noisy image'); img_3 = imfilter(img, f_3); subplot(3, 1, 2); plot_with_labels(img_3); title('image after average filter with window size 3'); img_5 = imfilter(img, f_5); subplot(3, 1, 3); plot_with_labels(img_5); title('image after average filter with window size 5'); Mathematically, the operation performed in the above code is called correlation filtering with uniform weights. For an averaging window of size \\((2k+1 * 2k+1)\\) (odd sized window explained in last section), it is given by, where \\(F\\) is the image we start with and \\(G\\) is the final after correlation filtering. Similarly, one can implement correlation filtering with non-uniform weights, called cross correlation filtering, given by, where \\(G\\) is the cross-correlation of \\(H\\) with \\(F\\) denoted by where \\(H\\) is the matrix of linear weights, also called kernel, mask, or coefficient. The kernel mentioned here have a slight relation with the machine learning kernels but are entirely different things and hence generally dealt with as seperate topics. Generally in image processing the average filtering is not used as they are not smooth in magnitude transitions and this is where cross cross-correlation filtering (eg. guassian filter) proves to be advantageuous. The smoothness of a given filter being applied to an image can be seen from the surface plots and the corresponding effect on the image of a dot as shown below, % smoothness of filters f_average = fspecial('average', 2); f_smooth = fspecial('gaussian', 7, 1); img = zeros(11, 11); img(6,6) = 90; img_average = imfilter(img, f_average); img_smooth = imfilter(img, f_smooth); subplot(2, 2, 1); surf(f_average); title('surface plot of average filter'); subplot(2, 2, 2); surf(f_smooth); title('surface plot of average filter'); subplot(2, 2, 3); imagesc(img_average); title('image after average filtering of a dot'); subplot(2, 2, 4); imagesc(img_smooth); title('image after guassian filtering of a dot'); Gaussian Filter The isotropic (i.e. circularly symmetrical) Guassian function given by, where sigma is the standard deviation of the function determining the spread of the plot. The effect of \\(\\sigma\\) on the plot can be seen in the plot below. For more on guassian or normal distribution read Normal Distribution % comparison of sigma f_gaussian_10 = fspecial('gaussian', 100, 10); f_gaussian_20 = fspecial('gaussian', 100, 20); subplot(2, 1, 1); surf(f_gaussian_10); title('plot with sigma = 10'); subplot(2, 1, 2); surf(f_gaussian_20); title('plot with sigma = 20'); For a given filter size one can vary \\(\\sigma\\) to change the mask or the kernel properties. The bigger the value of \\(\\sigma\\) is the more the blur. It can be seen from \\eqref{7} that the function sigma depends on a single parameter \\(\\sigma\\) that determines the spread of the plot. But, in actual programming, the filter would be represented by a matrix and hence has two properties, namely, size of the matrix and the size of the \\(\\sigma\\). So one can have different \\(\\sigma\\) within same size of matrix (filter size) as shown in fig 8. The variance (\\(\\sigma^2\\)) or standard deviation (\\(\\sigma\\)) determines the amount of smoothing. The two filters in Fig. 8 have same size but different variance and hence different amount of smoothing. Similarly, one can have different size matrices (filter size) for the same size of \\(\\sigma\\) (kernel size) as in the plot below. % same sigma in varying size matrix f_gaussian_50 = fspecial('gaussian', 50, 10); f_gaussian_20 = fspecial('gaussian', 20, 10); subplot(2, 1, 1); surf(f_gaussian_10); title('plot with matrix side 50'); subplot(2, 1, 2); surf(f_gaussian_20); title('plot with matrix side 20'); Among the two, filter of size 50 would work better because it is more smooth among the two. The Two Sigmas There are two sigma’s discussed above. One is the intensity of noise and the other is standard deviation of the gaussian filter. The sigma in the intensity of the noise determines the amount of noise added to an image while the sigma of gaussian filter determines the amount of smoothing that occurs on applying the sigma. The two are different that should not be confused. The reason they are both called sigma is because they both use the normal distribution. The plot of noise signals normal distribution can be seen in Fig. 1 (normal distribution over intensity) and similarly the plot of normal distribution of filtering kernel can be seen in Fig. 9 (isotropic normal distribution in space). The effect of both the sigmas can be seen in the plot below. Hence, it is important to understand the difference between the two sigma’s which are generally clear from the context but often create confusion. REFERENCES: Introduction to Computer Vision - Udacity",
    "tags": "machine-learning computer-vision image-processing udacity basics-of-computer-vision",
    "url": "/2018/02/04/images-noise-and-filters/"
  },

  
  
  
  {
    "title": "What is Computer Vision?",
    "text": "Basics to Computer Vision Series Index · · · What is Computer Vision? Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images such as a video. What is Computational Photography? Computational photography refers to analysis, manipulation and synthesis of images using numerical algorithms. It combines methodologies from image processing, computer vision, computer graphics and photography. Applications of Computer Vision OCR and Face Recognition Object Recognition Special Effects and 3D Modeling Smart Cars and Sports Vision based computer interactions Security and Medical Imaging Why is Computer Vision Hard? In order to understand why computer vision is hard, one has to familiarize themselves with the difference between measurements of metrics of an image and the perceptions that we draw from them. Essentially if one looks at the image below, it would seem that the boxes A and B are of different shade (essentially box A seems darker than box B). But, in reality if we place a grayscale intesity matcher for comparison of the block shades, it is seen that the two intensities are the same as seen in the image below. Another classic example showing the way perception differs based on image manipulation can be seen below. The shadow manipulation demo by Kersten Labs shows an apt example of how brain changes perception on slight changes in visual input to match the accepted norms. It is these intricate details and variations among them that make the problem of computer vision a challenging one. REFERENCES: Introduction to Computer Vision - Udacity",
    "tags": "machine-learning computer-vision image-processing udacity basics-of-computer-vision",
    "url": "/2018/02/01/what-is-computer-vision/"
  },

  
  
  
  {
    "title": "Understanding TensorFlow",
    "text": "Prerequisites Python Programming Basics of Arrays Basics of Machine Learning TensforFlow APIs The lowest level TensorFlow API, TensorFlow Core, provides the complete programming control, recommended for machine learning researchers who require fine levels of control over their model. The higher level TensorFlow APIs are built on top of TensorFlow Core. These APIs are easier to learn and use. Higher level APIs also provide convinient wrappers for repetitive tasks, e.g. tf.estimator helps to manage datasets, estimators, training, inference etc. Terminologies Computational Graph defines the series of TensorFlow operations arranged in the form of graph nodes. Dataset: Similar to placeholders, but Dataset represents a potentially large set of elements that can be accessed using iterators. These are the preferred method of streaming data into a model. Node in a TensorFlow graph takes zero or more tensors as inputs and produces a tensor as an output, e.g. Constant is a type of node in TensorFlow that takes no inputs and outputs the value that it stores internally (as defined in the defination of Tensorflow nodes earlier). Operations are another kind of node in TensorFlow used to build the computational graphs, that consume and produce tensors. Placeholder is a promise to provide value later. These serve the purpose or parameters or arguments to a graph which represents a dynamic function based on inputs. Rank: The number of dimensions of a tensor (similar to rank of a matrix). Session, used for evaluation of TensorFlow graphs, encapsulates the control and state of the TensorFlow runtime. Shape: Often confused with rank, shape refers to the tuple of integers specifying the length of tensor along each dimension. Tensor: A set of primitive types shaped into an array of any number of dimensions. It can be considered a higher-dimensional vector. Tensorflow used numpy arrays to represent tensor values. TensorBoard is a TensorFlow utility used to visualize the computational graphs. TensorFlow Imports from __future__ import absolute_import from __future__ import division from __future__ import print_function import numpy as np import tensorflow as tf absolute_import: The distinction between absolute and relative imports can be considered to be analogous to the concept of absolute or relative file paths or even URLs, i.e. absolute imports specify the exact path of the imports while the relative imports work w.r.t. the working directory. Therefore for the code that is to be shared among peers, it is recommended to use absolute imports. division: The import belongs to era when the debate on true division vs floor division was on in the python community i.e. for python 2.*. The import in python 3.* is not required as the regular division operator itself is the true division while the floor division is denoted by \\\\. print_function: This import is again not necessary in python 3.*. It is used to invalidate the print as a statement in python 2.*. Post call to this function, print only has valid represetnation as a function, which has some apparent advantages over the print as a statement, e.g. print function can be used inside lambda function or list and dictionary comprehensions. Generally all the __future__ imports are recommended to be kept at the top of the file because it changes the way the compiler behaves and the set of rules it follows. Initialization Basically, a TensorFlow Core program can be split into two sections: Building the computational graph Running the computational graph Tensor initialization is done as follows: import tensorflow as tf a = tf.constant(3.0, dtype=tf.float32, name='a') b = tf.constant(4.0, name='b') print(a) print(b) The output shows that the default type of a tensor is float32. Tensor(\"a:0\", shape=(), dtype=float32) Tensor(\"b:0\", shape=(), dtype=float32) Also, the print statement does not print value assigned to the nodes. The actual values will be displayed only on evaluation of the nodes. In TensorFlow the evaluation of a node can only be done within a session as shown below. Session sess = tf.Session() sess.run([a, b]) More complex TensorFlow graphs are built using operation nodes. For example, total = tf.add(a, b) print(total) sess.run(total) Mutliple tensors can be passed to a tf.Session.run, i.e., the run method handles any combination of tuples dictionaries, sess.run({'ab': (a, b), 'total': total}) Some tensorflow functions return tf.Operations instead of tf.Tensors. Also, the result of calling run on an Operation is None because Operations are run to cause a side-effect and not to retrieve a value. During a call to tf.Session.run, and tf.Tensor holds a single value throughout that run. This is consistent with the notion that state of graph is saved in a session making sure once initialized a tensor will not have updated values unless operated upon. TensorBoard In order to visualize the TensorFlow graph, following command can be followed: tf.summary.FileWriter('/path/to/save/', sess.graph) or writer = tf.summary.FileWriter('/path/to/save') writer.add_graph(tf.get_default_graph()) Now run the following command on the terminal, ensure TensorBoard is installed. tensorboard --logdir=/path/to/save It leads to the above graph displayed, which is an apt representation of the minimal graph that has been built so far. But it is a constant graph as the input nodes are constants. In order to build a parameterized graph, placeholders are used as shown below. Feeding Data using Placeholders a = tf.placeholder(tf.float32, name='a') b = tf.placeholder(tf.float32, name='b') z = a+b sess = tf.Session() print(sess.run(z, {a: 3, b: 4})) print(sess.run(z, feed_dict={a: [1, 2, 3], b: [4, 5, 6]})) tf.summary.FileWriter('/path/to/save', sess.graph) The feed_dict argument can be used to overwrite any tensor in the graph. The only difference between placeholders and other tf.Tensors is that placeholders throw and error if no value is fed to them. run( fetches, feed_dict=None, options=None, run_metadata=None ) Runs operations and evaluates tensors listed in fetches argument. The method will run one step of TensorFlow computation, by running necessary graph consisting of tensors and operations to evaluate the dependencies of Tensors listed in fetches, substituting the values listed in the feed_dict for the corresponding values. The fetches argument can be a single graph element, nested list, tuple, namedtuple, dict, or OrderedDict that consists of graph elements as its leaves. The graph element may belong to one of the following classes: tf.Operation: fetched value is None. tf.Tensor: fetched value is a numpy ndarray containing the value of the tensor. tf.SparseTensor: fetched value is a tf.SparseTensorValue. get_tensor_handle op: fetched value is a numpy ndarray containing the handle of the tensor. string: name of a tensor or operation in the graph. The value returned by run() has the same shape as the fetches argument, where leaves are replaced by corresponding values returned by TensorFlow. Example code: import collections a = tf.constant([10, 20]) b = tf.constant([1.0, 2.0]) data = collections.namedtuple('data', ['a', 'b']) v = sess.run({'k1': data(a, b), 'k2': [b, a]}) print(v) It can be observed on executing the code that the output of run saved has the same structure as the input to the fetches argument, i.e. a dictionary of namedtuple of lists and list of lists. The keys in feed_dict can belong to one of the following: If key is tf.Tensor: value maybe a scalar, string, list, or numpy array. If key is tf.Placeholder: shape of the value will be checked with shape of the placeholder for compatibility. If key is tf.SparseTensor: value should be tf.SparseTensorValue. If key is nested tuple of Tensors or SparseTensors, then the value should be follow the same nested structure that maps to corresponding values in the key’s structure. Each value in the feed_dict must be convertible to a numpy array of the dtype of corresponding key. The following errors are raised by run function: RuntimeError: If the Session is in invalid state or closed. TypeError: If fetches or feed_dict keys are of inappropriate types. ValueError: If fetches or feed_dict keys are invalid or refer to a tensor that does not exist. Importing Data tf.data api helps to build complex input pipelines. It basically helps deal with large amounts of data, maybe belonging to different formats and apply complicated transformations to the data such as image augmentation or handling text sequences of different lengths for preprocessing and batch processing use cases. The pipelines help abstract processes and also modularize the code for easier debugging and managing of the code. Some of the abstractions dataset introduces in TensorFlow are summarized below: tf.data.Dataset is used to represent a sequence of elements where each element contains one or more Tensor objects. Creating a source (Dataset.from_tensor_slices()) contructs a dataset from one or more tf.Tensor objects, while applying a transformation (Dataset.batch()) contructs a dataset from one or more tf.data.Dataset objects. tf.data.Iterator is used to extract elements from a dataset, and acts as an interface between the input pipeline and the models. The get_next method of iterator is called for streaming the data. Simplest iterator is made using make_one_shot_iterator method as shown below. It is associated with a particular dataset and iterates through it once. data = [ [0, 1,], [2, 3,], [4, 5,], [6, 7,], ] slices = tf.data.Dataset.from_tensor_slices(data) next_item = slices.make_one_shot_iterator().get_next() while True: try: print(sess.run(next_item)) except tf.errors.OutOfRangeError: break Reaching the end of dataset, if get_next is called, OutOfRangeError is thrown by the Dataset. For more sophisticated uses, Iterator.initializer is used that helps reinitialize and parameterize an iterator with different datasets, including running over a single or a set of datasets multiple number of times in the same program. Placeholders work for simple experiments, but Datasets are the preferred method of streaming data into a model. A dataset consists of elements that each have exactly the same structure, i.e. an element contains one or more tf.Tensor objects, called components. A component has tf.DType representing the type of elements in the tensor, and a tf.TensorShape (maybe partially specified, for example batch size might be missing but dimension of elements in batch may be present) representing the static shape of each element. Similarly the Dataset.output_types and Dataset.output_shapes inspect the inferred types and shapes of each component of the dataset element. It is optional but often helps to name the components in a dataset element. To summarize, one can use tuples, collections.namedtuples or a dictionary mapping strings to tensors to represent a single element in a Dataset. Sample code to see the above properties: def print_dtype(input_data, print_string=\"\"): if type(input_data) == tuple: print_string += \"(\" for _ in input_data: print_string = print_tuple(_, print_string) print_string += \")\" else: print_string += str(input_data.dtype) + \", \" return print_string def print_dimension(input_data, print_string=\"\"): if type(input_data) == tuple: print_string += \"(\" for _ in input_data: print_string = print_dimension(_, print_string) print_string += \")\" else: print_string += str(input_data.get_shape()) + \", \" return print_string dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10])) element1 = dataset1.make_initializable_iterator().get_next() print(\"dataset1:\") print(\"\\t\", print_dtype(element1)) print(\"\\t\", print_dimension(element1)) print(\"\\t\", dataset1.output_types) print(\"\\t\", dataset1.output_shapes) dataset2 = tf.data.Dataset.from_tensor_slices( (tf.random_uniform([4]), tf.random_uniform([4, 100], maxval=100, dtype=tf.int32))) element2 = dataset2.make_initializable_iterator().get_next() print(\"dataset2:\") print(\"\\t\", print_dtype(element2)) print(\"\\t\", print_dimension(element2)) print(\"\\t\", dataset2.output_types) print(\"\\t\", dataset2.output_shapes) dataset3 = tf.data.Dataset.zip((dataset1, dataset2)) element3 = dataset3.make_initializable_iterator().get_next() print(\"dataset3:\") print(\"\\t\", print_dtype(element3)) print(\"\\t\", print_dimension(element3)) print(\"\\t\", dataset3.output_types) print(\"\\t\", dataset3.output_shapes) Outputs: dataset1: &lt;dtype: 'float32'&gt;, (10,), &lt;dtype: 'float32'&gt; (10,) dataset2: (second:&lt;dtype: 'int32'&gt;, first:&lt;dtype: 'float32'&gt;, ) (second:(100,), first:(), ) {'second': tf.int32, 'first': tf.float32} {'second': TensorShape([Dimension(100)]), 'first': TensorShape([])} dataset3: (d1:&lt;dtype: 'float32'&gt;, (second:&lt;dtype: 'int32'&gt;, first:&lt;dtype: 'float32'&gt;, )) (d1:(10,), (second:(100,), first:(), )) {'d1': tf.float32, 'd2': {'second': tf.int32, 'first': tf.float32}} {'d1': TensorShape([Dimension(10)]), 'd2': {'second': TensorShape([Dimension(100)]), 'first': TensorShape([])}} So, the basic mechanics of import data can be listed as follows: Define a source is the first step in defining an input pipeline. For example, data can be imported from in-memory tensors using tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices(). It can also be imported from disk if the data is in recommended TFRecord format using tf.data.TFRecordDataset Transformations can be applied on any dataset to obtain subsequent dataset objects. This can be achieved by chaining preprocessing or transformation operations. For example, per-element transformations can be applied using Dataset.map() or multi-element transformations can be applied using Dataset.batch(). Dataset transformations support datasets of any structure. The element structure determines the argument of a function. For example, dataset1 = dataset1.map(lambda x: ...) dataset2 = dataset2.flat_map(lambda x, y: ...) dataset3 = dataset3.filter(lambda x, (y, z): ...) From the example above it can be seen that the lambda function can take any structure as the input based on the structure of an element in the dataset, however complex it is. Define an iterator to stream data into the model. The iterator can be one-shot iterator as in the example above or one of the types listed below. One-shot iterator is the simplest iterator that supports iterating only once through the dataset and does not require an explicit initialization. Hence, as a by-product, it does not allow parameterization. Initializable iterator requires an explicit iterator.initializer operation before using it. At the cost of this inconvinience, it gives the flexibility to parameterize the defination of dataset using placeholders. max_value = tf.placeholder(tf.int64, shape=[]) dataset = tf.data.Dataset.range(max_value) iterator = dataset.make_initializable_iterator() next_element = iterator.get_next() # parameter passing for the placeholder sess.run(iterator.initializer, feed_dict={max_value: 10}) for i in range(10): value = sess.run(next_element) assert i == value Reinitializable iterator can be initialized from multiple different dataset objects. Basically, while the datasets may change they have the same structure attributed to each element of the dataset. A reinitializable iterator is defined by its structure and any dataset complying to that structure can used to initialize the iterator. # two different datasets with same structure training_dataset = tf.data.Dataset.range(100).map( lambda x: x + tf.random_uniform([], -10, 10, tf.int64)) validation_dataset = tf.data.Dataset.range(50) # define the iterator using the structure property iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes) next_element = iterator.get_next() training_init = iterator.make_initializer(training_dataset) validation_init = iterator.make_initializer(validation_dataset) # initialize for training set sess.run(training_init) sess.run(next_element) # reinitialize for validation set sess.run(validation_init) sess.run(next_element) Feedable iterator is used along with tf.placeholder to select Iterator to use in each call of tf.Session.run, via the feed_dict mechanism. It does not require one to initialize the iterator from the start of a dataset when switching between the iterators. The tf.data.Iterator.from_string_handle can be used to define a feedable iterator. string_handle = tf.placeholder(tf.string, shape=[]) iterator = tf.data.Iterator.from_string_handle( string_handle, training_dataset.output_types, training_dataset.output_shapes ) next_element = iterator.get_next() # get string handles of each iterator training_handle = sess.run(training_iterator.string_handle()) validation_handle = sess.run(validation_iterator.string_handle()) # using training iterator sess.run(next_element, feed_dict={string_handle: training_handle}) # using validation iterator sess.run(next_element, feed_dict={string_handle: validation_handle}) Layers A trainable model is implemented in TensorFlow by means of using Layers which adds trainable parameters to a graph. A layer packages the variables and the operations that act on them. For example, densely-connected layer performs weighted sum across all inputs from each output and applies an optional activation function. The connection weights and biases are managed by the layer object. In order to apply layer to an input, the layer is called as a function with input as an argument. After calling the layer as a function, based on the inputs to the layer it sets up shape of weight matrices compatible with the input. Now, the layers contain variables that must be initialized before they can be used. import tensorflow as tf x = tf.placeholder(tf.float32, shape=[None, 3], name='x') linear_model = tf.layers.Dense(units=1, name='dense_layer') y = linear_model(x) init = tf.global_variables_initializer() sess = tf.Session() sess.run(init) sess.run(y, {x: [[1, 2, 3],[4, 5, 6]]}) Calling tf.global_variables_initializer only creates and returns a handle to tensorflow operation which will initialize all global variables on call of tf.Session.run. For each layer class (like tf.layers.Dense) there exists a shortcut function in TensorFlow (like tf.layers.dense) that creates and runs the layer in a single call. But this approach allows no access for the tf.layers.Layer object which might cause difficulties in debugging and introspection or layer reuse possibilities. The graph of a linear model looks as shown below. where the linear_model block internally has the structure as shown in Fig. 4 which abides by \\eqref{1}. where W is the weights being fed by the kernel, x is the input vector being fed by the input placeholder and b is the bias. Feature columns can be experimented with using tf.feature_column.input_layer function for dense columns as input and tf.feature_column.indicator_column for categorical indicators as input. features = { 'sales' : [[5], [10], [8], [9]], 'department': ['sports', 'sports', 'gardening', 'gardening']} department_column = tf.feature_column.categorical_column_with_vocabulary_list( 'department', ['sports', 'gardening']) department_column = tf.feature_column.indicator_column(department_column) columns = [ tf.feature_column.numeric_column('sales'), department_column ] inputs = tf.feature_column.input_layer(features, columns) var_init = tf.global_variables_initializer() table_init = tf.tables_initializer() sess = tf.Session() sess.run((var_init, table_init)) print(sess.run(inputs)) Feature columns can have internal state, like layers and so need to be initialized. Similarly, categorical columns use lookup tables internally and hence require tf.table_initializer additionally. Categorical input fed using indicator vectors are one-hot encoded. Training Define the data and labels. x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32) y_true = tf.constant([[2], [4], [6], [8]], dtype=tf.float32) Define the model y_pred = tf.layers.dense(x, units=1) At this point, model output can be evaluated # initialize session and variable sess = tf.Session() init = tf.global_variables_initializer() sess.run(init) # extract model output sess.run(y_pred) The output would not be correct as the model is not trained to optimize model parameters for accuracy. To optimize the model, a loss has to be defined. Using mean squared error, loss = tf.losses.mean_squared_error( labels=y_true, predictions=y_pred) sess.run(loss) After defining a loss, one of the optimizers provided by TensorFlow out of box can be used as optimization algorithm. The optimizers are defined in tf.train.Optimizer. Using the simplest gradient descent implemented in tf.train.GradientDescentOptimizer, Gradient Descent modifies each variable according to the magnitude of the derivative of loss with respect to that variable. optimizer = tf.train.GradientDescentOptimizer(0.01) train = optimizer.minimize(loss) Training the model At this stage the model graph is built and the only task pending is to call the evaluate train iteratively to minimize loss by optimizing model trainable parameters by updating the corresponding variables. for i in range(100): _, loss_value = sess.run((train, loss)) Evaluate model predictions predictions = session.run(y_pred) Model Graph The model graph generated from the above training example can be seen below. Since the model inputs are constants, it can be seen that the dense layer in the graph has no input being fed from outside, rather is a part of the dense block. REFERENCES: TensorFlow Low-level Introduction",
    "tags": "machine-learning tensorflow library featured",
    "url": "/2018/01/02/understanding-tensorflow/"
  },

  
  
  
  {
    "title": "B-Money",
    "text": "Cryptocurrency Series Blockchain Bitcoin B-Money · · · Introduction B-Money, an early proposal by Wei Dai for an anonymous, distributed electronic cash system was referred by Satoshi Nakamoto for creating Bitcoin, the current cryptocurrency giant. In the referred post, the author identified existing operational problems in an online community named Crypto-Anarchy. Crypto-Anarchy is explained as an online community where participants are identified with psuedonyms which are in no way associated with their true names or physical locations. Because of this seperation of true identity from community identity, the possibility of voilence is rendered impotent. Together these make the role of government permanently forbidden and unnecessary in such a community. A community is defined by cooperation of its participants, and an efficient cooperation requires a medium of exchange (money) and a way to enforce contracts. While traditionally these services were provided by the government or government sponsored institutions, it was not clear theoretically how to implement a similar system among decentralized nodes without using a trusted central authority. Author proposed two different protocols which can solve these issues, first one is very similar to the current proof-of-work protocol and second is similar to proof-of-stake protocol. Author poses that the implementation of the first system is impractical, because it makes heavy use of synchronous and unjammable anonymous broadcast channel. Existence of untraceable network is assumed, where senders and receivers are identified only by their digital psuedonyms (i.e. public keys) and every message is signed by its sender and encrypted to its receiver. First Protocol Analogous to proof-of-work. A seperate database is maintained by every participant with details of how much money belongs to which pseudonym. The individual databases collectively define the ownership of the money. The accounts are update subject to the rules in this protocol listed below. The creation of money: Money is created by broadcasting the solution to a previously unsolved computational problem. Such solutions must be easy to determine how much computing effort it took to solve the problem and must not otherwise have any practical or intellectual value (similar to nonce in a proof of work). Upon broadcasting the solution and verifying it, everyone credits the amount to solver’s psuedonym, equivalent to the amount of units it would take to buy the electricity utilized by the most economical computer to solve the problem. The transfer of money: If owner of psuedonym pk_A (public key of A) sends X units of money to owner of psuedonym pk_B (public key of B), they have to broadcast a message signed with their public key. On receiving this broadcast, all the participants would debit amount X from psuedonym pk_A and credit it to psuedonym pk_B, unless this creates a negative balance in pk_A’s account in which case the broadcasted transaction is ignored. The effecting of contracts: A valid contract binds a maximum reperation for each participating party in case of default. It also specifies a third party who will do the arbitration in case any dispute arises. In order for the contract to be effective, all the participating parties and the arbitrator must broadcast their public keys (i.e. psuedonyms). Upon the broadcast of contract and all the associated signatures, every participant debits the account of each party by the amount of his maximum reperation and credits a special account identified by secure has of the contract by the sum of the maximum reperations. The contract is a success if the debits succeed for every party without any negative balances, failing which the contract is ignored and the accounts are rolled back. The conclusion of contracts: If the contract concludes without any dispute, each party must broadcast a message stating the same, following which each participant credits the accounts of each party by the amount of their maximum reperation, removes the contract account, then credits or debits the account of each party according to the reparation schedule if there is one. The enforcement of contract: In case of dispute, which cannot be sorted by the arbitrator, each party broadcasts a suggested reparation/ fine schedule and arguments or evidence in his favor. Each participant makes a determination as to the actual reparations and/or fines, and modifies his accounts accordingly. Second Protocol Similar to proof-of-stake. The accounts of every psuedonym is maintained by a subset of the participants (called servers) instead of everyone as in the previous protocol. Servers are connected (using Usenet-style broadcast channel). Format of broadcasted transaction is same as first protocol, but affected participant of each transaction should verify the changes on a randomly selected subset of the servers. In order to bring a degree of trust in the servers, each server is required to deposit a certain amount of money in a special account to be used as potential fines or rewards for proof of misconduct. Each server must periodically publish and commit to its current money creation and money ownership databases. Also they should verify that his own account balances are correct and that total sum of account balances in not greater than the total amount of money created. This would prevent the servers, even in total collusion, from permanently and costlessly expanding the money supply. New servers synchronize with the existing servers to used the published databases. Alternative B-Money Creation One of the major problems faced in decentralized money network protocols is reaching a consensus for the cost of a computing effort. The rapid advances of computing technology, often in a private development makes it difficult to gather accurate information about these metrics while making sure they are not outdated. Author proposes a subprotocol, in which the account keepers (i.e. the participants in first protocol or servers in second protocol) decide and agree on the amount of b-money to be create each period, where the cost of b-money is determined by an auction. Each money creation period is divided up into four phases: Planning: Account keepers compute and negotiate to determine an optimal increase in money supply for a given period. Whether or not the participants reach a concensus, they broadcast their money creation quota and any macroeconomic calculations done to support the figures. Bidding: Anyone who wants to create the b-money makes a bid of the form &lt;X, Y&gt;, where X is the amount of b-money he wants to create and Y is an unsolved problem from a predetermined problem class (proof-of-work solution in case of bitcoin), where each problem has a nominal cost in MIPS-year publically agreed on. Computation: After bidding, the ones who placed the bids in bidding phase solve the problem in their bid and broadcast the solutions. Money Creation: Each participant accepts the highest bids (among all those who broadcast solutions) in terms of nominal cost per unit of b-money created and credits the bidder’s account accordingly. REFERENCES: B-Money by Wei Dai",
    "tags": "concept references",
    "url": "/2017/12/28/b-money-wei-dai/"
  },

  
  
  
  {
    "title": "Bitcoin",
    "text": "Cryptocurrency Series Blockchain Bitcoin B-Money · · · Introduction A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Part solution to this task is given by digital signatures, but the other half of the solution posed a major question in developing such systems - the part that requires a system to prevent double spending. In the current centralized money tender distributed, this role of preventing the double spending is done by a centralized third-party authority. But in a peer-to-peer network this centralized server would not be available and thus would require a different methodology. Bitcoin, proposed by Satoshi Nakamoto, was to solve this very problem on the peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. Current System Current economy is based on trusted third-parties to process electronic payments. These systems suffer from the inherent weaknesses of the trust based model. Completely non-reversible transactions are not possible because banks cannot avoid mediating disputes. Because of cost of mediation, the transaction costs are high which inturn limit the minimum practical transaction size thus cutting off the possibility of small casual transaction. With the possibility of reversal, the need for trust spreads. Proposed System Bitcoin proposed a system where trust is replaced by cryptographic proof, allowing any two willing parties to transact directly without the need of a supporting trusted third party. Computationally irreversible transactions prevent frauds. Bitcoin proposed to tackle the issue of double-spending, using a peer-to-peer distributed timestamp server to generate computational proof of the chronological order of transactions. The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes. Transactions Electronic coin is defined as a chain of digital signatures. A coin is transferred by an owner signing a hash of previous transaction and the public key of the next owner and adding these to then end of the coin. The signature can be verified by the payee to check the chain of ownership. The payee can still however not check for double-spending. Traditional solution of this involves introducing a trusted central authority, or mint, that checks every transaction for double-spending. In such a system, after every transaction the coin must be returned to the mint to issue a new coin, and only coins issued directly from the mint are trusted not to be double-spent. This still does not solve the basic problem of dependency on company running the mint, which is basically serving the same purpose as that of a bank in the current system. The only way to confirm the absence of a transaction is to have a copy of all the transactions. To accomplish this in a decentralized system, the transactions must be publically announced and there is a need for a system for participants to agree on a single history of the order in which they were received. So, at the time of the transaction, the payee needs a proof that the majority of the nodes agreed it was the first received one. Timestamp Server A timestamp server basically takes a hash of a block of items to be timestamped and widely publishes it, where the timestamp proves that the data must have existed at the time, in order to get the hash. Each timestamp includes previous timestamp in its hash, forming a chain (blockchain), with each timestamp reinforcing the ones before it. Proof-of-Work A system to deter denial of service attacks and other service abuses such as spam on a network by requiring some work from the service requester, usually meaning processing time by a computer. In case of bitcoins, the proof-of-work involves scanning for a value that when hashed along with the contents of the transaction blocks using cryptographic hashes such as SHA-256, gives a hash that begins with a specific number of zero bits. The average work required is exponential in the number of zero bits. Proof-of-Work is generally a problem that is fairly tough to solve but very easy to verify. import time import hashlib import json import matplotlib.pyplot as plt transaction = [ 'A paid B 25 A_public_key', 'A paid C 50 A_public_key', 'C paid B 10 C_public_key' ] transaction = json.dumps(transaction).encode() difficulty_bits = list(range(0, 8)) time_taken = [] for num_bits in difficulty_bits: proved = False proof = 0 start_time = time.time() while not proved: string = transaction + str(proof).encode() current_hash = hashlib.sha256(string).hexdigest() if current_hash.startswith('0'*num_bits): print(current_hash) print(proof) proved = True time_taken.append(time.time()-start_time) proof = proof+1 plt.plot(difficulty_bits, time_taken) plt.show() \"\"\" Outputs ------- cb87ca935b8b44e94f5c670ecd375ba88ab7616aeab4c4c4369fe79e9c153f95 0 0c1d9b1549d14612571040e8352af4175a41b84a805a08880d3acdd5aad998be 5 00a51beea09d1fae73f3e18682445a3eb9bffc2cabf17f87f90f193c9eca1af3 396 00010ff4f55c002484ee13841be9a3c46aba3038a1046d604ac8cee7349a2228 514 0000a3c456eff341c9988b194231b02eb547a36a8134d095a5e1c4efb05dc7e3 22344 000002128ccfc4d1e662a6ceb8053860b887929b691c2f8506b0843e9a57fe34 3355879 000000bfc98081b9aff064455708d2d8fcc2afeac81b5a0b03a6fd926db1ce36 19659271 0000000cbd6302c37b40102ef55040e4315b63b39640110b679e033c44a8b2b0 310723836 Time Taken ---------- [0.0003619194030761719, 0.00020599365234375, 0.0012040138244628906, 0.0016019344329833984, 0.07679605484008789, 8.218470811843872, 47.986721992492676, 740.9898428916931] \"\"\" Basic needs of such system of proof-of-work are fulfilled by cryptographic hashes, that are often seen as one way functions, i.e. the only way to get the input given an output is to use the bruteforce approach and check all possible values of inputs and arriving at the correct candidate. A nonce is an arbitrary number that can only be used once In the bitcoin’s timestamp network, the proof-of-work is implemented by incrementing a nonce in the block until a value is found that gives the block the required number of leading zero bits. Once the CPU effort has been expended to satisfy the proof-of-work, the block cannot be changed without redoing the work. As the later blocks consume the hashes of previous blocks, changing a transaction in earlier block would require redoing the work for all the leading blocks. Majority Decision Making Proof-of-Work also gives a solution to determining representation in majority decision making. The majority decision is represented by the longest chain, i.e. the greatest proof-of-work. So if the majority CPU power is held by honest nodes, then the honest chain would grow faster and outpace any chain being proposed by a pool of attacking nodes. To compensate hardware power and varying number of nodes over time, the proof-of-work difficulty is determined by a moving average targeting an average number of blocks per hour, i.e. if the blocks are generated too fast, the difficulty increases for subsequent blocks. Network Protocols Steps to run the network are as follows: New transactions are broadcast to all nodes. Each node collects new transaction into a block. Each node works on finding a difficult proof-of-work for its block. Upon finding the solution, it broadcasts the block to all nodes. Nodes accept a new block if all the transactions on it are valid and not already spent. Nodes express their acceptance of the block by working on creating next block in the chain, using the hash of the accepted block as the previous hash. Nodes always consider the longest chain to be the correct one and keep on working to extend it. If a node does not receive a block, it will request it when it receives the next block and realizes that it missed one. Mining Incentive By convention of the protocol, the first transaction of a block is a special transaction that starts a new coin owned by the creator of the block. It is this, that acts as an incentive for the nodes to support the network, and provides a way to initially distribute coins into circulation. Incentive can also be funded in the form of a transaction fees. Incentive will also encourage nodes to stay honest. Because an attacker with more compute power would find it more favourable to play by rules and make more transaction fee, instead of competing with rest to fraud the system and invalidating his own wealth in the process by sabotaging the system. Memory Optimizations After the latest transaction is reinforced with several blocks stacking on top of it, the spent transactions can be discarded from the disk. Transactions are hashed in a Merkle Tree, with only root included in the block’s hash. A block header with no transaction is about 80 bytes, and blocks are generated every 10 minutes, then every hour about 6 blocks would be generated. So the total space required by blocks generated in a year would be about 4.2MB (i.e 80 bytes * 6 * 24 * 365). With the current capabilities of system memories, storage would not pose a problem to the system. Payment Verification The system makes it possible to verify payments without running the full network node. Verification can be done by keeping a copy of the block headers of the longest proof-of-work chain, and obtaining the Merkle branch linking the transaction to the block it’s timestamped in. By linking the transaction to a place in the chain, one can see if network nodes have accepted it by checking the proof-of-work, which would be further confirmed by blocks built on top of it in the chain. Again, the verification is only reliable as long as honest nodes control the network. Because the simplified verification method can be fooled by an attacker’s fabricated transactions for as long as the attacker can overpower the compute resources of the unified honest nodes. Combining and Splitting Value Even though it is possible to handle each coin individually, it would be unweildy to make a seperate transaction for every cent in a transfer. To allow value to be split and combined, transactions contain multiple inputs and outputs. Usually, there is a single input from a larger previous transaction or multiple inputs combining smaller amounts, and at most two outputs: one for the payment, and one to return the change, if any, back to the sender. Privacy Traditional banking models achieve privacy through limiting access to information to the parties involved and the trusted third party. In the proposed system, it is achieved by anonymizing the public keys that are used to sign a transaction. So, the public ledger can see someone is sending an amount to someone else, but without information linking the transaction to anyone (similar to tape in a stock exchange). Some linking is still unavoidable with multi-input transactions, which reveals that the inputs originated from the same owner. So if one owner’s key is revealed, linking will allow to discover other transactions associated with that owner. Mathematics of Bitcoin If there were a scenario where an attacker is generating a alternate chain faster than the honest chain, it could still not generate value out of thin air or take someone else’s money because it will be rejected by the corresponding node which owns that token. So all it is capable of doing is to double-spend its own money. Basically the race between the honest chain and the alternate chain can be characterized as a Binomial Random Walk, where success event is honest chain being extended by a block and failure event is the alternate chain extented by a block gaining a lead on the honest chain. In such a case, the probability of an attacker catching up with the honest chain by making up for the deficit can be modeled similar to a Gambler’s Ruin Problem. The probability that the attacker ever reaches a breakeven is given by, where, \\(p\\) is the probability of an honest chain extending \\(q\\) is the probability of a attacker chain extending \\(q_z\\) is the probability of the attacker catching up with the honest chain from \\(z\\) blocks deficit. Given the assumption that the pool of honest compute power is greater than that of attackers, it is safe to assume that \\(p \\gt q\\). So the probability drops exponentially as the number of blocks of deficit for an attacker increases. So after a fair lead, the probability of attacker to catch up is vanishingly small. Time for Establishing Trust Assume that the sender of a transaction is an attacker who wants to make the receiver believe that he paid him for a while, and switch it back to himself after sometime. So initially the receiver will generate a new key pair and give the public key to the attacker for making the transfer. This prevents the attacker from preparing a chain of blocks in advance because of the dynamic nature of the public key generated. So once the transaction is made, the attacker starts working on a alternate chain where the transactions are different. Meanwhile the receiver waits for the transaction to be added to a block and another \\(z\\) blocks to be chained on top of that block. The receiver does not know the progress of the attacker, but assuming the honest nodes took the average time to build a block, the attacker’s progress will be a Poisson Distribution with the expected value, Probability that the attacker could catch up can be calculated by mutliplying the Poisson density for each amount of progress by the probability that he could catch up from that point, given by, Rearranging to avoid infinite summation of the distribution, import math q = 0.1 p = 1-q def factorial(k): fac = 1 for _ in range(1, k+1): fac *= _ return fac def calculate(z): lamda = z * q/p sum = 1 for k in range(z+1): sum -= math.pow(lamda, k) * math.exp(- lamda) / factorial(k) * (1 - math.pow((q/p), z-k)) return sum for _ in range(0, 30, 5): print(_, \"\\t=&gt; \", round(calculate(_),8)) \"\"\" Output ------ 0 =&gt; 1.0 5 =&gt; 0.17735231 10 =&gt; 0.04166048 15 =&gt; 0.01010076 20 =&gt; 0.0024804 25 =&gt; 0.00061323 \"\"\" Calculating the values, it is observed that the probability drops off exponentially with z. Conclusions Bitcoin does not need a third party to enforce trust. Coin framework is based on digital signatures. A peer-to-peer network using proof-of-work records a history of transaction. Nodes can leave or rejoin the system according to their wish. Any needed rules and incentives can be enforced with this consensus mechanism. REFERENCES: Bitcoin: A Peer-to-Peer Electronic Cash System",
    "tags": "concept references papers",
    "url": "/2017/12/21/bitcoin-satoshi-nakamoto/"
  },

  
  
  
  {
    "title": "Congruence and Modular Arithmetic",
    "text": "What is Mathematics Series Index · · · Congruence Two integers \\(a\\) and \\(b\\) are congruent modulo \\(d\\), where \\(d\\) is a fixed integer, if \\(a\\) and \\(b\\) leave same remainder on division by \\(d\\), i.e. It is denoted by, Following defination of congruences are equivalent: \\(a\\) is congruent to \\(b\\) modulo \\(d\\). \\(a = b + nd\\) for some integer n. \\(d\\) divides \\(a-b\\). Properties and Proof Congruence with respect to a fixed modulus has many of the formal properties of ordinary equality. \\(a\\equiv a\\pmod d\\) If \\(a\\equiv b\\pmod d\\), then \\(b\\equiv a\\pmod d\\) If \\(a\\equiv b\\pmod d\\) and \\(b\\equiv c\\pmod d\\), then \\(a\\equiv c\\pmod d\\) If \\(a\\equiv a’\\pmod d\\) and \\(b\\equiv b’\\pmod d\\), then Say, then, Geometric Interpretation Generally, integers are represented geometrically using a number line, where a segment of unit length is chosen and multiplied in either directions to represent negative or positive integers. But, when an integer modulo \\(d\\) is considered, the magnitude is insignificant as long as the behavior on division by \\(d\\) is same (i.e. they leave the same remainder on division by \\(d\\)). This is geometrically represented using a circle divided into d equal parts. This is because any integer divided by \\(d\\) leaves as remainder one of the \\(d\\) numbers \\(0, 1, \\cdots, d-1\\) which are placed at equal distances on the circumference of the circle. Every integer is congruent modulo \\(d\\) to one of these numbers and hence can be represented by one of these points. (Two numbers are congruent if they occur at the same point the circle.) Application of Congruence Properties The test for divisibility, generally taught in elementary school, is a direct result of the properties of congruence operation. For example, since \\(10 = -1 + 11\\). Successively multiplying this congruence, using \\eqref{5}, we obtain, So using \\eqref{3} and \\eqref{5}, it can be shown that any two number, z and t of the form shown below will leave the same remainder when divided by 11. Here, \\(z\\) is the format of any integer to the base 10. Hence, a number is divisible by 11 (i.e. leaves a remainder 0), if and only if \\(t\\) is divisible by 11 (which in \\eqref{7} basically means that the difference of the sum of all the odd digits and even digits together should be divisible by 11, including 0.) It can be observed that while such patterns are easier for numbers like 3, 9, 11, they are not easy to remember for other numbers like 7 and 11, as shown below. From above we reach the result that any number \\(z\\) in \\eqref{6} is divisible by 13 if and only if \\(t\\) of the form \\eqref{8} is divisible by 13 (clearly not an easy one to remember :P). Using a similar approach one can deduce the divisibility rule for any other integer. Other Properties only if either \\(a\\equiv 0 \\pmod d\\) or \\(b\\equiv 0 \\pmod d\\). Property only holds if \\(d\\) is a prime number. If \\(d\\) was a composite, there exist numbers \\(a \\lt d\\) and \\(b \\lt d\\), such that, Where, But, Law of Cancellation: With respect to a prime modulus, if \\(ab \\equiv ac\\) and \\(a \\cancel{\\equiv} 0\\), then \\(b \\equiv c\\). Fermat’s Theorem If \\(p\\) is any prime which does not divide the integer \\(a\\), then Consider multiples of \\(a\\), Let two of these numbers, \\(m_r\\) and \\(m_s\\) be congruent modulo \\(p\\), then, \\(p\\) must be a factor of \\(m_r - m_s = (r-s)a\\) for some \\(r, s\\) such that \\(1 \\leq r \\lt s \\leq (p-1)\\). But since it is assumed that \\(p\\) does not divide \\(a\\) and also \\(p\\) cannot be factor of \\(r-s\\) since it is less than \\(p\\). From \\eqref{9}, it can be concluded that two numbers from \\eqref{11} cannot be congruent modulo \\(p\\). So each of the numbers in \\eqref{11} must be congruent to \\(1, 2, 3, \\cdots , (p-1)\\) in some arrangement. So, For simplicity, let \\(K = 1 \\cdot 2 \\cdots (p-1)\\), then where \\(K\\) is not divisible by \\(p\\), since none of its factors are, hence from \\eqref{9}, \\((a^{p-1} - 1)\\) must be divisible by \\(p\\), i.e. Hence, proving \\eqref{10}. REFERENCES: What is Mathematics? Second Edition - Chapter I: Natural Numbers",
    "tags": "mathematics number-system what-is-mathematics",
    "url": "/2017/12/20/congruence-and-modulo/"
  },

  
  
  
  {
    "title": "The Mysterious Primes",
    "text": "What is Mathematics Series Index · · · Introduction In mathematics, most statements in number theory are concerned not with a single object, but with a whole class of objects that have a common property, such as the class of all even integers etc. Mathematics is the queen of sciences and the theory of numbers is the queen of the mathematics. One such class of number, called the prime numbers (or primes) are of fundamental importance. The Prime Numbers Most numbers can be resolved into smaller factors, but the ones that cannot be resolved are known as prime numbers or primes. A prime is an integer \\(p\\), greater than one, which has no factors other than itself and one. A integer \\(a\\) is a factor or divisor of integer \\(b\\), if there exists an integer \\(c\\) such that \\(b=ac\\). The numbers \\(2, 3, 5, 7, \\cdots \\) are primes. So effectively, every integer can be expressed as a product of primes. And a number which not prime (other than 0 and 1) is called composite. The set of all primes is a infinite set. The infinitude of primes is proved by contradiction. Say there exists only a finite number of primes, \\(n\\) and represented by set \\({p_1, p_2, \\cdots p_n}\\). Then according to the assumption any number greater than numbers in the finite set must be composite. But it is possible to come up with a number, \\(A\\) greater than all these \\(n\\) primes given by, This contracdicts the assumption of finite set of primes and hence its proved that there is a infinite set of primes. Every integer N greater than 1 can be factored into a product of primes in only one way. The proof of this can be achieved by contradiction once again. If there exists a positive prime integer capable of decomposition into two essentially different products of primes, there will be a smallest such integer (using the principle of smallest integer), given by, where \\(p’s\\) and \\(q’s\\) are primes. By rearranging if necessary, its possible to get the following order, Where, \\(p_1 \\ne q_1\\) because then one could cancel them in \\eqref{2} and come up with another number smaller than \\(m\\) with two distinct prime factorization which would contradict the priniciple of smallest integer. So, either \\(p_1 \\lt q_1\\) or \\(q_1 \\lt p_1\\). Let, \\(p_1 \\lt q_1\\) (if \\(q_1 \\lt p_1\\), interchange the sequences in the analysis presented). Let \\(m’\\) be an integer given by, Substituting, for \\(m\\) in \\eqref{4} using \\eqref{2}, Since \\(q_1 \\gt p_1\\), it follows from \\eqref{6} that \\(m’\\) is a positive integer, while from \\eqref{4} it follows that \\(m’ \\lt m\\). Hence the prime factorization of \\(m’\\) must be unique. From \\eqref{5} \\(p_1\\) is a factor of \\(m’\\), so in \\eqref{6}, \\(p_1\\) must be a factor of either \\((q_1 - p_1)\\) or \\(q_2 \\cdots q_s\\). Since \\(p_1 \\lt q_1\\) and \\eqref{3}, \\(p_1\\) cannot be a factor of any of the \\(q’s\\) nor equal to them. So, \\(p_1\\) must be a factor of \\(q_1 - p_1\\). So there exists an integer \\(h\\), such that, But, \\eqref{7} is a contradicts the fact that \\(q_1\\) is a prime number. This points to the fact that the initial assumption of a number having two distinct prime factorizations must be wrong. If a prime \\(p\\), is a factor of the product \\(ab\\), then \\(p\\) must be a factor of either \\(a\\) or \\(b\\). Also, if prime factorization of \\(a\\) can be expressed as, where \\(p’s\\) are distinct primes, each raised to a certain power. The number of different divisors of \\(a\\) (including \\(a\\) and 1) is given by and all the divisors of the number \\(a\\) are given by, where \\(\\beta’s\\) are integers satisfying inequalities, Distribution of Primes In mathematics, the sieve of Eratosthenes is a simple, ancient algorithm for finding all prime numbers up to any given limit. Attempts have been made at finding simple arithmetic formulas that yield only primes, even though they may not give all of them. Fermat’s Conjecture : All number of the form \\(F(n) = 2^{2^n} +1\\) are primes, but was proved incorrect by Euler who discovered that \\(2^{2^5} + = 641 \\cdot 6700417\\) meaning \\(F(5)\\) is not a prime. Any of the numbers \\(F(n)\\) are not prime for \\(n&gt;4\\). There are various other similar expressions which produce primes until a certain limit. \\(f(n) = n^2 - n + 41\\): For limit \\(n &lt; 41\\). \\(f(n) = n^2 - 79n + 1601\\): For limit \\(n &lt; 80\\) It’s been a futile effort to seek equation of a simple type which produces only primes. Even less promising is the attempt to find an algebraic formula that yields all the primes. Primes in Arithmetic Progression In each arithmetic progression, \\(a,\\, a+d,\\, a+2d,\\, \\cdots a+nd,\\, \\cdots\\), where \\(a\\) and \\(d\\) have no common factors, there are infinitely many primes. The proof to this theorem could not be cracked for several years until finally it was presented by Lejeune Dirichlet and even after a hundred years it’s not been simplified any further and would be a series of posts in itself if presented here. Though a generalized Euclid’s proof of infinitude of primes can be used to cover some special arithmetic progression such as \\(4n+3\\) and \\(6n + 5\\). Proof of Infinitude of Arithmetic Progressions: \\(4n+3\\) Any prime greater than 2 must be a odd prime and hence must be of the form \\(4n+1\\) or \\(4n+3\\) for some integer \\(n\\). Also, the product of two numbers of the form \\(4n+1\\) is again of the same form, since Let there be a finite number of primes in this arithmetic progression given by, \\(p_1,\\, p_2,\\, \\cdots p_n\\) of the form \\(4n+3\\). Consider the number, From \\eqref{12}, either N has to be a prime or it can be decomposed into a product of primes, none of which can be \\(p_1,\\, p_2,\\, \\cdots p_n\\) because they leave a remainder \\(-1\\) on dividing \\(N\\). Also, all of the primes must not be of the form \\(4n+1\\) because the form of \\(N\\) is \\(4n+3\\) and \\eqref{11}. Hence, one of the factors must of the form \\(4n+3\\), which is impossible, since none of the \\(p’s\\), which was assumed to be the finite set of primes of the form \\(4n+3\\), can be a factor of \\(N\\) (using \\eqref{12}). So, the initial assumption that number of primes in arithmetic progression of the form \\(4n+3\\) is finite is incorrect because of the contradiction encountered. Proof of Infinitude of Arithmetic Progressions: \\(6n+5\\) Any prime greater than 2 must be a odd prime and hence must be of the form \\(6n+1\\) or \\(6n+3\\) or \\(6n+5\\) for some integer \\(n\\). Let there be a finite number of primes in this arithmetic progression given by, \\(p_1,\\, p_2,\\, \\cdots p_n\\) of the form \\(6n+5\\). From \\eqref{13}, \\eqref{14} and \\eqref{15}, a number of the form \\(6n+5\\) cannot be achieved using only primes of the form \\(6n+1\\) and \\(6n+3\\). Consider the number, From \\eqref{16}, either N has to be a prime or it can be decomposed into a product of primes, none of which can be \\(p_1,\\, p_2,\\, \\cdots p_n\\) because they leave a remainder \\(-1\\) on dividing \\(N\\). Also, all of the primes must not be only of the form \\(6n+1\\) and \\(6n+3\\) because the form of \\(N\\) is \\(6n+5\\) and \\eqref{13}, \\eqref{14} and \\eqref{15} show it cannot be achieved otherwise. Hence, one of the factors must of the form \\(6n+5\\), which is impossible, since none of the \\(p’s\\), which was assumed to be the finite set of primes of the form \\(6n+5\\), can be a factor of \\(N\\) (using \\eqref{16}). So, the initial assumption that number of primes in arithmetic progression of the form \\(6n+5\\) is finite is incorrect because of the contradiction encountered. The Prime Number Theorem Let \\(A_n\\) denote the number of primes among the integers \\(1, 2, 3, \\cdots , n\\). The density of primes among first n integers is given by the ratio \\({A_n \\over n} \\). Prime number theorem describes the asymptotic distribution of the primes among positive integers. It states that, Unsolved Problems Concerning Primes Goldbach Conjecture: Goldbach’s conjecture is one of the oldest and best-known unsolved problems in number theory and all of mathematics. It states: Every even integer greater than 2 can be expressed as the sum of two primes. Polignac’s conjecture: Twin prime conjecture, also known as Polignac’s conjecture, in number theory, assertion that there are infinitely many twin primes, or pairs of primes that differ by 2. These are simple to experiment with, but mathematically some of the most mysterious problems. These are the properties of the number system that shows that a lot is still left to be discovered about the number system. REFERENCES: What is Mathematics? Second Edition - Chapter I: Natural Numbers",
    "tags": "mathematics number-system what-is-mathematics",
    "url": "/2017/11/01/prime-numbers/"
  },

  
  
  
  {
    "title": "Mathematical Induction",
    "text": "What is Mathematics Series Index · · · Why Mathematical Induction? There are infinitely many integers. The step-by-step procedure of passing from \\(n\\) to \\(n+1\\) which generates the infinite sequence of integers forms the basis of one of the most fundamental patterns of mathematical reasoning, the principle of mathematical induction. Mathematical Induction is used to establish the truth of a mathematical theorem for an infinite sequence of cases. There are two steps in proving a theorem, \\(A\\) by mathematical induction: The first statement \\(A_1\\) must be true. If a statement \\(A_r\\) is true then the statement \\(A_{r+1}\\) should be true too. That these two conditions are sufficient to establish the truth of all the statements, \\(A_1, A_2, \\cdots \\) is a logical principle which is as fundamental to mathematics as are the classical rules of Aristotelian logic. Suppose that a) by some mathematical argument it is shown that if \\(r\\) is any integer and if assertion \\(A_r\\) is known to be true then the truth of assertion \\(A_{r+1}\\) will follow, and that b) the first proposition \\(A_1\\) is known to be true. Then all the propositions of the sequence must be true, and A is proved. The principle of mathematical induction rests on the fact that after any integer \\(r\\) there is a next, \\(r+1\\), and that any desired integer \\(n\\) may be reached by a finite number of such steps, starting from the integer 1. Although the principle of mathematical induction suffices to prove a theorem or formula once it is expressed, the proof gives no indication of how this formula was arrived at in the first place. So, it should be more fittingly called a verification. Proof for Arithmetic Progression For every value of n, the sum \\(1 + 2 + \\cdots + n\\) of the first n integers is equal to \\(\\frac {n(n+1)} {2} \\) For any \\(r\\) is given by assertion \\(A_r\\) is given by, Adding \\((r+1)\\) to both LHS and RHS of \\eqref{1}, \\eqref{2} is nothing but assertion, \\(A_{r+1}\\). Also for \\(r=1\\), assertion \\(A_1\\) is true because, from \\eqref{1}, So, using \\eqref{2} and \\eqref{3}, mathematical induction proves that the assertion in \\eqref{1} holds for all positive integers, \\(n\\). Proof for Geometric Progression The theorem states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, Let’s assume \\(G_r\\) is true, i.e. Then, But \\eqref{7} is precisely the assertion \\eqref{4} for the case \\(n=r+1\\). Hence, using \\eqref{5} and \\eqref{7} the assertion \\eqref{4} is proved by mathematical induction. Proof for Sum of First n Squares The theorem states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, Let’s assume \\(A_r\\) is true, i.e., Then, which is precisely the assertion \\eqref{8} for \\(n = r+1\\). So, using \\eqref{9} and \\eqref{11}, mathematical induction proves the assertion. Proof for Sum of First n Cubes The theorem states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, Let’s assume \\(A_r\\) is true, i.e., Then, which is precisely the assertion \\eqref{12} for \\(n = r+1\\). So, using \\eqref{13} and \\eqref{15}, mathematical induction proves the assertion. Proof for Bernoulli’s Inequality The assertion, \\(A_n \\) states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, Let’s assume \\(A_r\\) is true, i.e., Then, which is precisely the assertion \\eqref{16} for \\(n = r+1\\). So, using \\eqref{17} and \\eqref{19}, mathematical induction proves the assertion. If \\(p \\lt -1\\), then \\((1+p)\\) is negative and the inequality in \\eqref{19} would be reversed. So, the restriction introduced in \\eqref{16} is essential. Proof of Binomial Theorem The assertion, \\(C_i^n \\) states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, which is exactly the value for \\(C_0^1 = C_1^1 = 1\\) in Pascal’s Triangle. Let’s assume \\(C_i^r\\) is true, i.e., Then using the relation, \\(C_i^{r+1} = C_{i-1}^{r} + C_i^{r} \\text{, } C_i^{r+1}\\) is given by, which is precisely the assertion \\eqref{20} for \\(n = r+1\\). So, using \\eqref{21} and \\eqref{23}, mathematical induction proves the assertion. Generalized Mathematical Induction If a sequence of statements \\(A_s, A_{s+1}, \\cdots\\) is given where \\(s\\) is a some positive integer, and if a) For every value \\(r \\geq s\\), the truth of \\(A_{r+1}\\) will follow from the truth of \\(A_r\\), b) \\(A_s\\) is known to be true, then all the statements \\(A_s, A_{s+1}, \\cdots\\) are true; i.e. \\(A_n\\) is true for all \\(n \\geq s\\) Proof for Bernoulli’s Inequality (Strict version) The assertion, \\(A_n \\) states that, for every value of \\(n\\), The assertion holds true for \\(n=s=2\\), because, Let’s assume \\(A_r\\) is true, i.e., Then, which is precisely the assertion \\eqref{24} for \\(n = r+1\\). So, using \\eqref{25} and \\eqref{27}, mathematical induction proves the assertion. If \\(p \\lt -1\\), then \\((1+p)\\) is negative and the inequality in \\eqref{19} would be reversed. So, the restriction introduced in \\eqref{16} is essential. Principle of Smallest Integer Every non-empty set \\(C\\) of positive integers has a smallest number. (Set may be finite or infinite.) At first it seems like a trivial principle but it actually does not apply to many sets that are not integers, e.g. the set of positive fractions \\(1, {1 \\over 2} {1 \\over 3} \\cdots \\) does not contain a smallest number. Proof for Mathematical Induction The principle of smallest integer can be used to prove the principle of mathematical induction as a theorem. Let us consider any sequence of statements \\(A_1, A_2, \\cdots \\) such that, For any integer \\(r\\) the truth of \\(A_{r+1}\\) will follow from that of \\(A_r\\) \\(A_1\\) is known to be true. if 1 of the \\(A’s\\) were false, the set \\(C\\) of all positive integers \\(n\\) for which \\(A_n\\) is false would be non-empty. By the principle of smallest integer, \\(C\\) would have a smallest integer \\(p\\), which must be greater than 1 because of condition (2) in the theorem. Hence, \\(A_p\\) would be false, but \\(A_{p-1}\\) true, which contradicts condition (1) of the theorem. So, the assumption in the first place, that any one of the \\(A’s\\) is false is untenable. Mathematical Induction must be applied very carefully to prove an assertion because, it is often fallacious because of a misleading base case. A popular example of this is “All number are equal fallacy”. REFERENCES: What is Mathematics? Second Edition - Chapter I: Natural Numbers",
    "tags": "mathematics number-system what-is-mathematics theorems",
    "url": "/2017/10/28/mathematical-induction/"
  },

  
  
  
  {
    "title": "What is Number System ?",
    "text": "What is Mathematics Series Index · · · The Natural Numbers The natural numbers were created by the human mind out of the necessity to count the objects around us. Most basics of mathematics are associated with association of numbers with tangible objects. But advanced mathematics is built on top of the abstract concept of number system. God created natural numbers; everything else is man’s handiwork. Laws of Arithmetic Natural numbers have only two basic operations, namely, addition and multiplication. The mathematical theory of the natural numbers or positive integers is known as arithmetic. Arithmetic is based on the fact that operations on numbers are governed by certain laws. (a, b, c … symbolically denote integers) The fundamental laws of arithmetic are: Commutative law, i.e. \\(a+b = b+a\\) and \\(ab = ba\\) Associative law, i.e. \\(a + (b + c) = (a + b) + c\\) and \\(a(bc) = (ab)c\\) Distributive law, i.e. \\(a(b+c) = ab + ac\\) Addition and subtraction are inverse operations because, \\((a+d)-d = a\\). Similarly, multiplication and division are inverse because \\({a \\over d} \\cdot d = a\\). Also, 0 and 1 are the identities of operations addition and multiplication respectively. Representation of Integers A number system has a set of digit symbols and numbers where digit symbols are used to denote the larger numbers not available directly in the set of digit symbols. Modern number systems are associated with the place values of individual digits in the number, such as, These are called positional notations. Here, a large number can also be represented using the basic symbols from the set of digit symbols. It is useful to have a way of indicating the result in a general form using a uniform logic, i.e. a general method for representing an integer, \\(z\\) in the decimal system is to express it as follows which would be represented by the symbol \\(a_na_{n-1}\\cdots a_1a_0\\) Specifically, in case of decimal system the base is 10 as can be seen in the equations above. But, in general, for a number system any number greater than 1 can serve as the base of the system. E.g. a septimal system has base 7 and an integer is expressed as, and it would be represented by the symbol \\(b_nb_{n-1}\\cdots b_1b_0\\) As a result 109 in decimal system is represented by 214 in septimal system for the reason shown below As a general rule, in order to convert from base 10 to base B, perform successive divisions of number z by B; the remainders in reverse order would be the number representation in the system with base B. Too small a base base has disadvantages (even a small number like 72 has a lengthy representation 1,001,111 in the dyadic system i.e. the binary system), while a large base requires learning many digit symbols, and an extended multiplication table. The duodecimal system, i.e., choice of base 12 is advocated in many places, because it is exactly divisible by two, three, four, and six. Due to this property, works involving division and fraction are simplified. Early systems of numerations were not positional in nature, but were based on purely additive principle. E.g. in roman symbolism, i.e., the symbol \\(CXVIII\\) represents 118. Same was true about other early number systems like the Egyptian, Hebrew, Greek systems of numeration. Disadvantage of any purely additive notation is that it requires more and more number of new symbols as the number to be represented gets larger. Also, the computation with additive systems is so difficult to perform, that computation was confined to a few adepts in the ancient times. The positional system has the property that all numbers, however large or small, can be represented by the use of a small set of different digit symbols. Also, the major advantage lies in the ease of computation. Computation in Other Number Systems A very curious property of the decimal number system that points to usage of other number systems in the past is the number words used. E.g. the words for 11 and 12 are not constructed as the other teens are, suggesting a linguistic independence from words used for 10. Such peculiarities suggest remnants of use of other bases, notably 12 and 20. Words vingt and quartevingt used for 20 and 80 respectively in French suggest a possibility of number with base 20. There are traces of Babylonian astronomers using a number system called sexagesimal (base 60). The tables for addition and multiplication change with number system while the rules of arithmetic remain the same. Below is the table for addition followed by table for multiplication in duodecimal number system. (10 and 11 in the duodecimal representation below are represented using A and B respectively.) + 1 2 3 4 5 6 7 8 9 A B 1 2 3 4 5 6 7 8 9 A B 10 2 3 4 5 6 7 8 9 A B 10 11 3 4 5 6 7 8 9 A B 10 11 12 4 5 6 7 8 9 A B 10 11 12 13 5 6 7 8 9 A B 10 11 12 13 14 6 7 8 9 A B 10 11 12 13 14 15 7 8 9 A B 10 11 12 13 14 15 16 8 9 A B 10 11 12 13 14 15 16 17 9 A B 10 11 12 13 14 15 16 17 18 A B 10 11 12 13 14 15 16 17 18 19 B 10 11 12 13 14 15 16 17 18 19 1A . 1 2 3 4 5 6 7 8 9 A B 1 1 2 3 4 5 6 7 8 9 A B 2 2 4 6 8 A 10 12 14 16 18 1A 3 3 6 9 10 13 16 19 20 23 26 29 4 4 8 10 14 18 20 24 28 30 34 38 5 5 A 13 18 21 26 2B 34 39 42 47 6 6 10 16 20 26 30 36 40 46 50 56 7 7 12 19 24 2B 36 41 48 53 5A 65 8 8 14 20 28 34 40 48 54 60 68 74 9 9 16 23 30 39 46 53 60 69 76 83 A A 18 26 34 42 50 5A 68 76 84 92 B B 1A 29 38 47 56 65 74 83 92 101 REFERENCES: What is Mathematics? Second Edition - Chapter I: Natural Numbers",
    "tags": "mathematics number-system what-is-mathematics",
    "url": "/2017/10/27/number-system/"
  },

  
  
  
  {
    "title": "Neural Networks: Cost Function and Backpropagation",
    "text": "Basics of Machine Learning Series Index · · · Notation \\({(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots , (x^{(m)}, y^{(m)})}\\) are the \\(m\\) training examples L is the total number of the layers in the network \\(s_l\\) is the number of units (not counting the bias unit) in the layer l K is the number of units in the output layer For example in the network shown above, L = 4 \\(s_1 = 3,\\, s_2 = 3,\\, s_3 = 2,\\, s_4 = 2\\) K = 2 One vs All method is only needed if number of classes is greater than 2, i.e. if \\(K \\gt 2\\), otherwise only one output unit is sufficient to build the model. Cost Function of Neural Networks Cost function of a neural network is a generalization of the cost function of the logistic regression. The L2-Regularized cost function of logistic regression from the post Regularized Logistic Regression is given by, Where \\({\\lambda \\over 2m } \\sum_{j=1}^n \\theta_j^2\\) is the regularization term \\(\\lambda\\) is the regularization factor Extending (1) to then neural networks which can have K units in the output layer the cost function is given by, Where \\(h_\\Theta(x) \\in \\mathbb{R}^K \\) \\((h_\\theta(x))_i\\) is the \\(i^{th}\\) output Here the summation term \\(\\sum_{k=1}^K\\) is to generalize over the K output units of the neural network by calculating the cost function and summing over all the output units in the network. Also following the convention in regularization, the bias term in skipped from the regularization penalty in the cost function defination. Even if one includes the index 0, it would not effect the process in practice. Backpropagation Algorithm Backpropagation algorithm is based on the repeated application of the error calculation used for gradient descent similar to the regression techniques, and since it is repeatedly applied in the reverse order starting from output layer and continuing towards input layer it is termed as backpropagation. For a network with L layers the computation during foward propagation, for an input \\((x, y)\\) would be as follows, The \\(h_\\Theta(x)\\) is the prediction. In order to reduce the error between the prediction and the actual value backpropagation is used. Say, \\(\\delta_j^{(l)}\\) is the error of node j in the layer l is associated with the prediction made at that node given by \\(a_j^{(l)}\\), then backpropagation aims to calculate this error term propating backwards starting from the output unit in the last layer (layer L in the example above). So for each output unit in layer L, the error term is given by, \\(\\delta_j^{(L)} = a_j^{(L)} - y_j\\) which can be vectorized and written as, Where \\(a^{(L)}\\) is \\(h_\\Theta(x)\\). Now the error terms for the previous layers are calculated as follows, Where \\(g’(z^{(l)}) = a^{(l)} .* (1 - a^{(l)}) \\) .* is the element-wise multiplication. This backward propagation stops at l = 2 because l = 1 correponds to the input layer and no weights needs to be calculated there. Now the the gradient for the cost function which is needed for the minimization of the cost function is given by, Where regularization is ignored for the simplicity of expression. Summarizing backpropagation: Given training set \\({(x^{(1)}, y^{(1)}), \\cdots, (x^{(m)}, y^{(m)})}\\) Set \\(\\Delta_{ij}^{l} = 0\\) for all (i, j, l) For i = 1 to m: Set \\(a^{(1)} = x^{(i)} \\) Perform forward propagation to compute \\(a^{(l)}\\) for l = 1, …, L Using \\(y^{(i)}\\) compute \\( \\delta^{(L)} = a^{(L)} - y^{(i)} \\) Compute \\(\\delta^{(L-1)}, \\cdots, \\delta^{(2)}\\) using backpropagation \\(\\Delta_{ij}^{(l)} := \\Delta_{ij}^{(l)} + a_j^{(l)} \\delta_i^{(l+1)} \\) Vectorized implementation of the equation above is given by, \\(D_{ij}^{(l)} := {1 \\over m} \\Delta_{ij}^{(l)} + \\lambda \\, \\Theta_{ij}^{(l)} \\) if \\(j \\ne 0\\) \\(D_{ij}^{(l)} := {1 \\over m} \\Delta_{ij}^{(l)} \\) if \\(j = 0\\) And finally, \\( \\frac {\\partial} {\\partial \\Theta_{ij}^{(l)} } = D_{ij}^{(l)} \\) The capital delta matrix \\(D\\), is used as an accumulator to add up the values as backpropagation proceeds and finally compute the partial derivatives. Backpropagation Intuition Say \\((x^{(i)}, y^{(i)})\\) is a training sample from a set of training examples that the neural network is trying to learn from. If the cost function is applied to this single training sample while setting \\(\\lambda = 0\\) for simplicity, then \\eqref{2} can be reduced to, where, which is very similar to the cost for a logistic regression, which can also be seen analogous to the cost for a linear regression (mean-squared error), i.e. This basically gives a magnitude of how well the network is doing in prediction of the output for a single training sample. Formally, the \\(\\delta\\) terms are the partial derivatives of the cost function given by, where \\(cost(i)\\) is given by \\eqref{7}. So, \\eqref{8} conveys mathematically the intent to change the cost function (by changing the network parameters), in order to effect the intermediate values calculated in \\(z’s\\), so as to minimize the differences in the final output of the network. The basis of backpropagation is benched on the propagating the error term calculated for the final layer using \\eqref{3} and \\eqref{4} backwards to the preceding layers. For more on mathematics of backpropagation, refer Mathematics of Backpropagation. For an approximate implementation of backpropagation using NumPy and checking results using Gradient Checking technique refer Backpropagation Implementation and Gradient Checking REFERENCES: Machine Learning: Coursera - Cost Function",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/10/03/neural-networks-cost-function-and-back-propagation/"
  },

  
  
  
  {
    "title": "Neural Networks Intuition",
    "text": "Basics of Machine Learning Series Index · · · Introduction Neural networks can be used to build all types of function. This post tries to map functions of logical operations using the network. How the parameters are derived in explained in the later posts. AND Gate Using a single neuron, it is possible to achieve the approximation of an and gate. The architecture with the parameters can be seen below. The hypothesis for the above network is given by, Where g is the sigmoid function which asymptotes at 0 and 1. The output table for the hypothesis above is given below. \\(x_1\\) \\(x_2\\) \\(h_\\theta(x)\\) 0 0 \\(g(-30) \\approx 0\\) 0 1 \\(g(-10) \\approx 0\\) 1 0 \\(g(-10) \\approx 0\\) 1 1 \\(g(10) \\approx 1\\) The values of \\(h_\\theta(x)\\) is nothing but the expected value of the AND gate. OR Gate Similarly, using a single neuron, it is possible to achieve the approximation of an or gate. The architecture with the parameters can be seen below. It is same as AND gate but the bias weight is changed. The hypothesis for the above network is given by, Where g is the sigmoid function which asymptotes at 0 and 1. The output table for the hypothesis above is given below. \\(x_1\\) \\(x_2\\) \\(h_\\theta(x)\\) 0 0 \\(g(-10) \\approx 0\\) 0 1 \\(g(10) \\approx 1\\) 1 0 \\(g(10) \\approx 1\\) 1 1 \\(g(30) \\approx 1\\) The values of \\(h_\\theta(x)\\) is nothing but the expected value of the OR gate. NOT Gate Unlike the previous two examples, NOT gate is a unary operator, but still simple weights can give easy implementation of the NOT gate. The architecture and the parameters are shown below. The hypothesis for the above network is given by, Where g is the sigmoid function which asymptotes at 0 and 1. The output table for the hypothesis above is given below. \\(x_1\\) \\(h_\\theta(x)\\) 0 \\(g(10) \\approx 1\\) 1 \\(g(-10) \\approx 0\\) The values of \\(h_\\theta(x)\\) is nothing but the expected value of the NOT gate. (NOT \\(x_1\\)) AND (NOT \\(x_2\\)) Unlike the previous examples, this operation does not look straight forward, but actually it is. Here is an architecture implementation of the above gate. The hypothesis for the above network is given by, Where g is the sigmoid function which asymptotes at 0 and 1. The output table for the hypothesis above is given below. \\(x_1\\) \\(x_2\\) \\(h_\\theta(x)\\) 0 0 \\(g(10) \\approx 1\\) 0 1 \\(g(-10) \\approx 0\\) 1 0 \\(g(-10) \\approx 0\\) 1 1 \\(g(-30) \\approx 0\\) The values of \\(h_\\theta(x)\\) is nothing but the expected value of the (NOT \\(x_1\\)) AND (NOT \\(x_2\\)) operation. Perceptron Limitation All the examples untill this one were linearly seperable and hence were solved using a single neuron. But and XOR Gate is not linearly seperable as was the case with AND and OR gates and this can be clearly seen in the plot below. It is evident that there is no single straight line that can seperate the two classes in plot (c) and hence termed as not linearly seperable. This is a major drawback of perceptron (single layer neural networks). There is a simple proof for concluding that the XOR is not linearly seperable. Say, perceptron were capable of separating the two classes, then it would mean that there exists a set of weights (or parameters), \\(\\theta_0,\\, \\theta_1,\\,\\theta_2\\) such that the hypothesis is given by, Then the above hypothesis should satisfy the following truth table, \\(x_1\\) \\(x_2\\) \\(x_1\\,XOR\\,x_2\\) 0 0 0 0 1 1 1 0 1 1 1 0 Substituting and equating to 0, the hypothesis, following inequalities are generated that would determine the decision boundary. Substituting \\(b = -\\theta_0\\), above inequalities can be written as, It can be seen that the first three inequalities directly contradict the fourth one. Which means that the very first assumption made that there exist such parameters \\(\\theta_0,\\, \\theta_1,\\,\\theta_2\\) was incorrect. Hence, the XOR gate is not linearly seperable. This is where the utility and finesse of multi-layer neural network in deriving intricate features from the input features can be seen in action. XNOR Gate For simplicity, let’s consider a XNOR gate which is nothing but the negation of an XOR gate. So, it would not be wrong to say that if XNOR gate is achieved, XOR gate is not very far. Consider the following neural network with one hidden layer and the given weights. Here if weights are seen carefully, \\(a_1^{(2)}\\) is nothing but the AND gate and \\(a_2^{(2)}\\) is nothing but (NOT \\(x_1\\)) AND (NOT \\(x_2\\)). Similarly the output neuron is nothing but the OR gate. It calculates the following result table, \\(x_1\\) \\(x_2\\) \\(a_1^{(2)}\\) \\(a_2^{(2)}\\) \\(h_\\theta(x)\\) 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 The values of \\(h_\\theta(x)\\) is nothing but the expected value of the XNOR operation. This gives the intuition that the hidden layers are calculating a more complex input which inturn helps to turn the problem into a linearly seperable one using the transformations. This is the main reason the neural networks are fairly powerful classifiers because as the depth (or number of hidden layers) of the neural network increases it can derive more and more complex features for the final layer. REFERENCES: Machine Learning: Coursera - Examples and Intutions I Machine Learning: Coursera - Examples and Intutions II",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/09/27/neural-network-intuition/"
  },

  
  
  
  {
    "title": "Neural Networks Theory",
    "text": "Basics of Machine Learning Series Index · · · Introduction Neural networks were developed to mimic the way neurons in a brain work. Typically, a neuron has input connections and output connections. So basically, neuron is a computational unit which takes a set of inputs and produces output. So the basic functionality of neurons is tried to be replicated in these computational units. For example, neurons in brain interact with each other using electrical signals and are selectively activated based on certain parameters. This behaviour is transferred to a unit in neural networks using activation function which would be explained later in the post. Neuron Model: Logistic Unit Below is the representation of a basic logistic unit neuron model, Here, \\(x_0, \\cdots, x_3\\) are the input units connected to the neuron and can be considered similar to dendrites getting signals from other neurons. \\(h_\\theta(x)\\) is the activation function which basically decides whether or not should the neuron get activated or excited. The term \\(x_0 = 1\\), also called bias unit, plays an important role in the decision made for excitation of neuron. In case of a logistic unit the activation function is a logistic function or sigmoid function i.e. the neuron has a sigmoid (logistic) activation function. Sigmoid function is given by, Parameters of the model, \\(\\theta_0, \\cdots, \\theta_n\\) are also sometimes called weights of the model and represented by \\(w_1, \\cdots, w_n\\). Neural Network A group of these neuron units together forms a neural network. Below is a representation of neural network, Where network has 3 layers layer 1 is called input layer layer 3 is called output layer remaining layers are called hidden layers. In this case there is only one hidden layer, but it is possible to have multiple hidden layers \\(x_0\\) and \\(a_0^{(2)}\\) are the bias terms and equal 1 always. They are generally not counted when the number of units in a layer are calculated. So, layer 1 and layer 2 have 3 units each. Notations \\(a_i^{(j)}\\) is the activation of unit i in layer j \\(\\Theta^{(j)}\\) is the matrix of weights controlling function mapping from layer j to layer j+1. So, the network diagram above depicts the following computations, From the equation above one can generalize, if a network has \\(s_j\\) units in layer j and \\(s_{j+1}\\) units in layer j+1, then \\(\\Theta^{(j)}\\) is a matrix of dimension \\((s_{j+1} * (s_j + 1))\\). Vectorization of Network Computation Consider (2) can be written as, Where \\(z_i^{(j)} = \\Theta_{i0}^{(j-1)}\\,x_0 + \\Theta_{i1}^{(j-1)}\\,x_1 + \\Theta_{i2}^{(j-1)}\\,x_2 + \\Theta_{i3}^{(j-1)}\\,x_3\\) Given \\(x=\\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}\\) and \\(z^{(j)}=\\begin{bmatrix} z_1^{(j)} \\\\ \\vdots \\\\ z_n^{(j)} \\end{bmatrix}\\), then computation for \\(z^{(j)}\\) can be vectorized as follows, Where \\(a^{(1)} = x\\) \\(\\Theta^{(j-1)}\\) is a matrix of dimensions \\((s_j * (s_{j-1}+1))\\) \\(s_j\\) is number of activation nodes After (5), bias unit, \\(a_0^{(j)} = 1\\) is added to the activation vector of layer j and then the process repeats to get activation for the next layer, i.e. (3) is written as, Where \\(\\Theta^{(j)}\\) is a matrix of dimensions \\((s_{j+1} * (s_j + 1)) \\) Process of calculation of activations cascading across layers is called Forward Propagation. Nueral Network and Logistic Regression If one looks closely at the neural network diagram above, it can be easily seen that if one removes the layer 1, the schema looks same as a logistic regression. It is infact a logistic regression model if the hidden layer is the direct features (input layer) fed to the neuron because the activation function of the neuron is logistic (sigmoid) function. So, effectively the neural network shown above is a logistic regression where the features for classification are learnt by the hidden layer and not fed manually by human intervention. Each feature in hidden layer is mapped from the input layer. Because of this architecture there is a chance of learning much better features than started with or one can make using the higher order polynomial terms and hence there is a probability of reaching better hypotheses with neural networks. Architecture of Neural Network It is possible to have different kinds of architecture for the neural network. By architecture, it means to have different schema, i.e. number of neurons per layer can vary, number of layers used can vary, the way the neurons are connected to each other can vary and similarly various other variations are possible in terms of optimization parameters, activation etc. When then number of hidden layers is more than one, they are known as deep neural networks. REFERENCES: Machine Learning: Coursera - Model Representation I Machine Learning: Coursera - Model Representation II",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/09/21/neural-networks/"
  },

  
  
  
  {
    "title": "Non-linear Hypotheses",
    "text": "Basics of Machine Learning Series Index · · · Drawbacks of Logistic Regression Consider a highly non-linear classification task, say something similar to the one shown in the plot below. In order to achieve a decision boundary like the one plotted, one needs to introduce non-linear features in the form of quadratic and other higher order terms, similar to the equation below. As the number of features increase then number of terms in the hypotheses would also increase exponentially to get a good fit which would have high probability of overfitting the data. Hence, when the number of features is really high and the decision boundary is complex, logistic regression would not generalize the solution very well by leveraging the power of polynomial terms. So for highly complex tasks like the ones where one needs to classify objects from images, logistic regression would not perform well. For example, for images of size 100 * 100 pixels if one uses all quadratic features, there would be around 50 million parameter values to learn which is computationally very expensive task and still not a guarantee of good decision boundary. This is where Neural Networks step in to save the day. Neural Networks They are class of very powerful machine learning classifiers that are capable of fitting almost any hypotheses and are motivated by the way a brain functions. Even though the concepts of neural networks were well developed by the 80s, they came into popularity fairly recently because of the advances in the compute power of machines using multiple cores and GPUs. It is mainly because neural networks are a class of very computationally expensive algorithms and earlier systems were just not fast enough to get good results in a feasible time-frame. One Learning Algorithm Hypothesis This hypothesis puts light on the fact that even though human brain learns a variety of tasks involving visual, vocal, or audio inputs, it does not learn them using different algorithms. It has been found that if the optic nerve is re-routed to the auditory cortex (responsible for decoding audio), it would learn to use the input and work with visual input as well i.e. Auditory cortex learns to see. So, extending the result of such experiments suggesting that a single tissue in brain is capable of performing different tasks like analyse visuals, audio, touch etc, it is posited that there must be one algorithm that can approximate any learning task similar to the way brain learns. REFERENCES: Machine Learning: Coursera - Non-Linear Hypotheses Machine Learning: Coursera - Neurons and the Brain",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/09/20/non-linear-hypotheses/"
  },

  
  
  
  {
    "title": "Regularized Logistic Regression",
    "text": "Basics of Machine Learning Series Index · · · Introduction The intuition and implementation of logistic regression is implemented in Classifiction and Logistic Regression and Logistic Regression Model Similar to the linear regression, even logistic regression is prone to overfitting if there are large number of features. If the decision boundary is overfit, the shape might be highly contorted to fit only the training data while failing to generalise for the unseen data. So, the cost function of the logistic regression is updated to penalize high values of the parameters and is given by, Where \\({\\lambda \\over 2m } \\sum_{j=1}^n \\theta_j^2\\) is the regularization term \\(\\lambda\\) is the regularization factor import numpy as np mul = np.matmul \"\"\" X is the design matrix y is the target vector theta is the parameter vector lamda is the regularization parameter \"\"\" def sigmoid(X): return np.power(1 + np.exp(-X), -1) \"\"\" hypothesis function \"\"\" def h(X, theta): return sigmoid(mul(X, theta)) \"\"\" regularized cost function \"\"\" def j(theta, X, y, lamda=None): m = X.shape[0] theta[0] = 0 if lamda: return (-(1/m) * (mul(y.T, np.log(h(X, theta))) + \\ mul((1-y).T, np.log(1 - h(X, theta)))) + \\ (lamda/(2*m))*mul(theta.T, theta))[0][0] return -(1/m) * (mul(y.T, np.log(h(X, theta))) + \\ mul((1-y).T, np.log(1 - h(X, theta))))[0][0] Regularization for Gradient Descent Previously, the gradient descent for logistic regression without regularization was given by, Where \\(j \\in \\{0, 1, \\cdots, n\\} \\) But since the equation for cost function has changed in (1) to include the regularization term, there will be a change in the derivative of cost function that was plugged in the gradient descent algorithm, Because the first term of cost fuction remains the same, so does the first term of the derivative. So taking derivative of second term gives \\(\\frac {\\lambda} {m} \\theta_j\\) as seen above. So, (2) can be updated as, Where \\(j \\in \\{1, 2, \\cdots, n\\} \\) and h is the sigmoid function It can be noticed that, for case j=0, there is no regularization term included which is consistent with the convention followed for regularization. \"\"\" regularized cost gradient \"\"\" def j_prime(theta, X, y, lamda=None): m = X.shape[0] theta[0] = 0 if lamda: return (1/m) * mul(X.T, (h(X, theta) - y)) + (lamda/m) * theta return (1/m) * mul(X.T, (h(X, theta) - y)) \"\"\" Simultaneous update \"\"\" def update_theta(theta, X, y, lamda=None): return theta - alpha * j_prime(theta, X, y, lamda) Link to Rough Working Code. Change the value of lamda in the code to get different decision boundaries for the data as shown below. REFERENCES: Machine Learning: Coursera - Logistic Regression Model",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/09/15/regularized-logistic-regression/"
  },

  
  
  
  {
    "title": "Basics of Language Model",
    "text": "Introduction Given the finite set of all the words in a language, \\(\\nu\\), a sentence in the language is the sequence of words Where \\(n \\ge 1\\) \\(x_1 \\cdots x_{n-1} \\in \\nu\\) \\(x_n\\) is a special symbol STOP \\(\\require{cancel}\\cancel{\\in} \\nu \\) Set of all the words in a language are assumed to be finite. Let \\(\\nu^{\\dagger}\\) be the infinite set of all sentences with the vocabulary \\(\\nu\\). A language model consists of a finite set \\(\\nu\\) as a function \\(p(x_1, x_2, \\cdots, x_n)\\), such that: For any \\(⟨ x_1 \\cdots x_n ⟩ \\in \\nu^\\dagger, \\, p(x_1, x_2, \\cdots, x_n) \\ge 0\\) \\(\\sum_{⟨ x_1 \\cdots x_n ⟩ \\in \\nu^\\dagger} p(x_1, x_2, \\cdots, x_n) = 1\\) So, \\(p(x_1, x_2, \\cdots, x_n)\\) is basically a probability distribution over the sentences \\(\\nu^\\dagger\\). Applications of Language Modeling A distribution of \\(p(x_1 \\cdots x_n)\\) signifies how probable a sentence is in a language. Such a distribution can prove useful in speech recognition or machine translation. Candidates generated by these algorithms can be run against the language model to check how probable the sentences are. Frequency Based Modeling Frequency based modeling is given by, Where \\(c(x_1 \\cdots x_n)\\) is the number of times \\(x_1 \\cdots x_n\\) occurs in the training corpus N is the total number of sentences in the corpus. One major drawback of such a model is that it would assign probability 0 to any sentence not seen in the training corpus. Markov Models for Fixed Length Sequences Consider a sequence of random variables, \\(X_1, X_2, \\cdots, X_n\\), where each random variable can take any value in a finite set \\(\\nu\\). n is assumed to be a fixed number. Language model aims to find the probability of \\(x_1 \\cdots x_n\\), where \\(n\\ge1\\) and \\(x_i \\in \\nu\\) for \\(i = 1 \\cdots n\\), i.e. to model the join probability, If n is a fixed number there are \\(|\\nu|^n\\) possible sequences of the form \\(x_1 \\cdots x_n\\), which makes it impossible to list all the possible sequences for a large value of n and \\(|\\nu|\\). This is where markov models help to build a more compact model. First-Order Markov Process make the assumption that identity of an element in a sequence depends only on the identity of previous element in the sequence, i.e. \\(X_i\\) is conditionally independent of \\( X_1 \\cdots X_{i-2} \\), given the value of \\(X_{i-1}\\). The first step of the equation above is exact using the chain rule of probability. The second step is a result of first-order markov model assumption. Similarly, second-order Markov models assume that identity of an element in a sequence depends only on the identity of previous two elements in the sequence, i.e. \\(X_i\\) is conditionally independent of \\(X_1 \\cdots X_{i-3}\\), given the value of \\(X_{i-1}\\) and \\(X_{i-2}\\). It is assumed that \\(x_0,\\, x_{-1} = *\\), where * is the special start symbol. Markov Sequences for Variable-length Sentences The value n, assumed to be fixed number in previous section, is considered to be a random variable itself and the nth word is always equal to the special symbol STOP. Using the second-order markov assumption, Where \\(n \\ge 1\\) \\(x_n =\\) STOP \\(x_i \\in \\nu\\) for \\(i=1 \\cdots (n-1)\\) Process of generating a sequence using the above distribution would be as follows: Initialize \\(i=1\\), and \\(x_0, x_{-1} = *\\) Generate \\(x_i\\) from the distribution, If \\(x_i = \\) STOP then return the sequence \\(x_1 \\cdots x_i\\), else set i = i+1 and repeat previous step. Trigram Language Models Trigram language models are direct application of second-order markov models to the language modeling problem. Each sentence is modeled as a sequence of n random variables, \\(X_1, \\cdots, X_n\\) where n is itself a random variable. A trigram model consists of finite set \\(\\nu\\), and a parameter, Where u, v, w is a trigram \\(w \\in \\nu \\cup \\{STOP\\}\\) \\(u, v \\in \\nu \\cup \\{*\\}\\) The value of \\(q(w| u, v)\\) can be interpreted as the probability of seeing the word w immediately after bigram u, v. So, for any sequence \\(x_1 \\cdots x_n\\) where \\(x_i \\in \\nu\\) for \\(i = 1 \\cdots (n-1)\\) and \\(x_n = \\) STOP, the probability of the sentence under trigram language model is Where \\(x_0 = x_{-1} = *\\) Trigram assumption: Each word depends on the previous two. This is essentially the second-order markov assumption used over sentences. The only step remaining in the trigram language model is the estimation of language parameters, i.e., \\(q(w|u,v)\\). Since the total number of words are \\(| \\nu |\\) the total number of possible parameters would be \\(| \\nu |^3\\). It is a very big number and hence needs some kind of indirect estimation process. Maximum Likelyhood Estimates This is the most generic solution to the estimation problem shown above. For any w, u, v, Where c(u,v,w) is the number of times the trigram is seen in corpus c(u, v) is the number of times the bigram is seen in the corpus Many of the frequencies c(u,v,w) and c(u,v) would be 0. This would effect the estimation and present the following flaws: \\(q(w|u,v) = 0\\) because c(u,v,w) is 0 which would underestimate many trigram probabilities which is unreasonable If the denominator is 0, then \\(q(w|u,v)\\) would be undefined. Perplexity It is one of the evaluation metrics for the language models and is calculated on a held-out data after the model is trained on some corpus. The held-out data is not used for parameter estimation of the language model. Consider a test dataset consisting of sentences, \\(s_1, \\cdots, s_m\\), then \\(p(s_i)\\) gives probability for sentence \\(s_i\\) in the language model. A basic measure of quality of language model would be the probability it assigns to the entire test set, give by So, higher the quantity is, the better the language model is at modeling unseen sentences. Perplexity is a direct transformation of this basic defination. Let M be the total number of words in the corpus, then average log probability under the model is which is the log probability of the entire corpus, divided by the total number of words in the corpus. Again the higher the value of this, the better the language model. Then, Perplexity is defined as Where The perplexity is a positive number. The smaller the value of perplexity, the better the language model is at modeling unseen data. Perplexity becomes a minimization parameter because of the negative power that is applied to the defination. Intuition for Perplexity Let the vocabulary, \\(\\nu\\) have N words, and the model predicts uniform distribution over the vocabulary, i.e., Then evaluating (3) using (1), Where \\(n_1, n_2, \\cdots, n_m\\) are the number of words in each sentence in the test sample and, Using (4) in (2), So, under a uniform distribution model, the perplexity is equal to the vocabulary size. Perplexity can be considered the effective vocabulary size under the model. Properties of perplexity: If for any trigram u, v, w, the estimated probability \\(q(w|u, v) = 0\\) then the perplexity will be \\(\\infty\\) which is consistent with the rule stating that a good model should not predict probability zero for an unseen dataset and perplexity is low for good models. If perplexity is the measure of language model, then 0 estimates should be avoided at all costs. Linear Interpolation The following trigram, bigram and unigram maximum-likelihood estimates are defined, Where \\(c(u,v,w)\\) is the number of times trigram u, v, w occurs \\(c(v,w)\\) is the number of times bigram v, w occurs \\(c(w)\\) is the number of times unigram w occurs \\(c()\\) is the total number of words seen in the training Properties of these models: The unigram model will never face the issue of number or denominator being 0, so the estimate is always well defined and greater than 0. But it completely ignores the context and hence discards valuable information. The trigram model use the context better than the unigram model, but has the problem of many of its counts being 0 rendering estimate value 0 or undefined. The bigram model falls between the two extremes. Linear Interpolation uses all three of these estimates to define the trigram estimate as follows, Where i.e. (6) is a weighted average of the three estimates. Discounting Methods and Katz Back-off An alternative estimation method commonly used in practice. Consider a bigram language model where the following parameter is to be found, Where \\(w \\in \\nu \\cup \\) {STOP} \\(v \\in \\nu \\cup\\) {*} Discounted Counts are used to reflect the intuition that if the counts are taken from the training corpus, there would be a systematic over-estimation of probability of bigrams seen in the corpus and hence underestimate the bigrams not seen in the corpus. So, discounted count is given by, Where \\(c^*(v, w)\\) is the discounted count \\(c(v,w)\\) is the count of bigrams, such that, \\(c(v,w) \\gt 0\\) 0.5 is the discount value Using the discounted count, (7) can be written as, i.e., use the discounted count in the numberator and regular count in the denominator. For any context \\(v\\), there is a missing mass, defined as, The intuition behind discounted methods is to divide the missing mass among words \\(w\\), such that \\(c(v,w) = 0\\). Formally, for any \\(v\\), there exist sets Then the estimate is given by, i.e if \\(c(v,w) \\gt 0\\) return \\({c^*(v, w) \\over c(v)}\\) else divide the remaining probability mass \\(\\alpha(v)\\) in proportion to the unigram estimates \\(q_{ML}(w)\\). This method can be generalized to trigram language model in a recursive way, i.e., for any bigram (u, v) define, Where 0.5 is the discount value and hence discounted count is given by, Then the trigram model is given by, where \\(\\alpha(u, v)\\) is the missing probability mass of the bigram. It can be noted that the missing probability is distributed in proportion to the bigram estimaes \\(q_{BO}(w|v)\\) given in (8). REFERENCES: Language Modeling - Michael Collins",
    "tags": "NLP machine-learning",
    "url": "/2017/09/12/basics-of-language-modeling/"
  },

  
  
  
  {
    "title": "Regularized Linear Regression",
    "text": "Basics of Machine Learning Series Index · · · The intuition of regularization are explained in the previous post: Overfitting and Regularization. The cost function for a regularized linear equation is given by, Where \\(\\lambda \\sum_{i=1}^n \\theta_j^2\\) is the regularization term \\(\\lambda\\) is called the regularization parameter Regularization for Gradient Descent Previously, the gradient descent for linear regression without regularization was given by, Where \\(j \\in \\{0, 1, \\cdots, n\\} \\) But since the equation for cost function has changed in (1) to include the regularization term, there will be a change in the derivative of cost function that was plugged in the gradient descent algorithm, Because the first term of cost fuction remains the same, so does the first term of the derivative. So taking derivative of second term gives \\(\\frac {\\lambda} {m} \\theta_j\\) as seen above. So, (2) can be updated as, Where \\(j \\in \\{1, 2, \\cdots, n\\} \\) It can be noticed that, for case j=0, there is no regularization term included which is consistent with the convention followed for regularization. The second equation in the gradient descent algorithm above can be written as, Where \\(\\left(1 - \\alpha {\\lambda \\over m} \\right) \\lt 1\\) because \\(\\alpha\\) and \\(\\lambda\\) are both positive. The implementation from Mulivariate Linear Regression can be updated with the following updated regularized functions for cost function, its derivative, and updates. One, can notice that \\(theta_0\\) has not been handled seperately in the code. And as expected it does not affect the regularization much. It can be implemented in the conventional way by adding a couple of logical expressions to the function \"\"\" Regularized Version \"\"\" def reg_j_prime_theta(data, theta, l, order_of_regression, i): result = 0 m = len(data) for x, y in data : x = update_features(x, order_of_regression) result += (h(x, theta) - y) * x[i] result += l*theta[i] return (1/m) * result def reg_j(data, theta, l, order_of_regression): cost = 0 m = len(data) for x, y in data: x = update_features(x, order_of_regression) cost += math.pow(h(x, theta) - y, 2) reg = 0 for j in theta: reg += math.pow(j, 2) reg = reg * l return (1/(2*m)) * (cost + reg) def reg_update_theta(data, alpha, theta, l, order_of_regression): temp = [] for i in range(order_of_regression+1): temp.append(theta[i] - alpha * reg_j_prime_theta(data, theta, l, order_of_regression, i)) theta = np.array(temp) return theta def reg_gradient_descent(data, alpha, l, tolerance, theta=[], order_of_regression = 2): if len(theta) == 0: theta = np.atleast_2d(np.random.random(order_of_regression+1) * 100).T prev_j = 10000 curr_j = reg_j(data, theta, l, order_of_regression) print(curr_j) cost_history = [] theta_history = [] counter = 0 while(abs(curr_j - prev_j) &gt; tolerance): try: cost_history.append(curr_j) theta_history.append(theta) theta = reg_update_theta(data, alpha, theta, l, order_of_regression) prev_j = curr_j curr_j = reg_j(data, theta, l, order_of_regression) if counter % 100 == 0: print(curr_j) counter += 1 except: break print(\"Stopped with Error at %.5f\" % prev_j) return theta, cost_history, theta_history reg_theta, reg_cost_history, reg_theta_history = reg_gradient_descent(data, 0.01, 1, 0.0001, order_of_regression=150) The following plot is obtained on running a random experiment with regression of order 150, which clearly shows how the regularized hypothesis is much better fit to the data. Regularization for Normal Equation The equation and derivation of Normal Equation can be found in the post Normal Equation. It is given by, But after adding the regularization term as shown in (1), making very small changes in the derivation in the post, one can reach the result for regularized normal equation as shown below, Where I is the identity matrix \\(L = \\begin{bmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\end{bmatrix} \\) is a square matrix of size (n+1) If \\(m \\le n\\), then the \\(X^TX\\) was non-invertible in the unregularized case but, \\(X^TX + \\lambda I\\) does not face the same issues and is always invertible. The effect of regularization on regression using normal equation can be seen in the following plot for regression of order 10. No implementation of regularized normal equation presented as it is very straight forward. REFERENCES: Machine Learning: Coursera - Regularized Linear Regression",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/09/11/regularized-linear-regression/"
  },

  
  
  
  {
    "title": "Overfitting and Regularization",
    "text": "Basics of Machine Learning Series Index · · · Overfitting If the number of features is very high, then there is a probability that the hypothesis fill fit all the points in the training data. It might seem like a good thing to happen but has a contradictory results. Suppose a hypothesis of high degree is fit to a set of points such that all the points lie of the hypothesis. Plot below shows a case of overfitting with a regression of order 100. This plot can be created if the code from Multivariate Linear Regression is run with the parameter order of regression set to 100. The curve above shows a case of overfitting where the hypothesis has high variance. While it fits the training data with good accuracy, it will fail to predict the values for unseen cases with same accuracy because of the high variance in the prediction curve. Opposite to this spectrum is the case of underfitting. For example there exists a data set that increased linearly initialy and then saturates after a point. If a univariate linear regression is fit to the data it will give a straight line which might be the best fit for the given training data, but fails to recognize the saturation of the curve. Such a hypothesis is said to be underfit and have high bias i.e. the hypothesis is biased that the prediction should vary linearly over the feature variation. An optimum hypothesis is the trade off between variance and bias. All three cases can be seen in the plots below. Both underfitting and overfitting are undesirable and should be avoided. While overfitting might seem to work well for the training data, it will fail to generalize to new examples. Overfitting and underfitting are not limited to linear regression but also affect other machine learning techniques. Effect of underfitting and overfitting on logistic regression can be seen in the plots below. Detecting Overfitting For lower dimensional datasets, it is possible to plot the hypothesis to check if is overfit or not. But same strategy cannot be applied when the dimensionality of the dataset increases beyond the limit of visualization. So, plotting hypothesis may not always work. So other methods have to utilized to address overfitting. There are two options to avoid the overfitting: Reduce the number of features: Mostly overfitting is observed when the number of features is really high and the number of training examples are less. In such cases reducing the number of features can help avoid the issue of overfitting. Reducing the number of features can be done manually, or using model selection algorithms which help decide which features to eliminate. This also presents a disadvantage as removing features is sometimes equivalent to removing information. Regularization: Keep all the features but reduce magnitude/values of parameters \\(\\theta_j\\). This technique works well, when there exists a lot features and each contributes a bit to the prediction, i.e. Regularization works well when there are a lot of slightly useful features. Regularization Intuition It is mathematically seen that the high variance of a overfit hypothesis is attributed to higher value of parameters corresponding to higher order features i.e. more the dependency is biased on a single feature, greater are the chances of a hypothesis overfitting. This effect can be counter acted by making sure that the values of parameter \\(\\theta_j\\) are small. This is done by penalizing the algorithm proportional to value of \\(\\theta_j\\) which will ensure small values of these parameters and hence would prevent overfitting by attributing small contributions from each features and hence removing high bias or high variance. So the main ideas in regularization are: maintain small values for the parameters \\(\\theta_0, \\theta_1, \\cdots , \\theta_n \\) which keeps the hypothesis simple and less prone to overfitting Mathematically, regularization is acheived by modifying the cost function as follows, Where \\(\\lambda \\sum_{i=1}^n \\theta_j^2\\) is the regularization term and \\(\\lambda\\) is called the regularization parameter. If noticed closely, this term mainly points to the fact that if value of \\(\\theta_j\\) is increased, it would consequently increase the cost which is to be minimized during gradient descent. So it would ensure small values of the parameters as intented to prevent overfitting. By convention \\(\\theta_0\\) is not penalized but in practice it does not affect the algorithm a lot. The regularization parameter \\(\\lambda\\), controls the trade off between the goal of fitting the training set well and the goal of keeping the parameters small and the hypothesis simple. So, if the value of \\(\\lambda\\) is kept very large, it would fail to fit the dataset properly and would give a underfit hypothesis. REFERENCES: Machine Learning: Coursera - The Problem of Overfitting Machine Learning: Coursera - Cost Function",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/09/08/overfitting-and-regularization/"
  },

  
  
  
  {
    "title": "Multiclass Logistic Regression",
    "text": "Basics of Machine Learning Series Index · · · Introduction For intuition and implementation of Binary Logistic Regression refer Classifiction and Logistic Regression and Logistic Regression Model. Multiclass logistic regression is a extension of the binary classification making use of the one-vs-all or one-vs-rest classification strategy. Intuition Given a classification problem with n distinct classes, train n classifiers, where each classifier draws a decision boundary for one class vs all the other classes. Mathematically, Implementation Below is an implementation for multiclass logistic regression with linear decision boundary, where number of classes is 3 and one-vs-all strategy is used. import math import numpy as np import matplotlib.pyplot as plt x_orig = [[0,0], [0,1], [1, 0], [1, 1], [2, 2], [2, 3], [3, 2], [3, 3], [0, 4], [1, 4], [0, 5], [1, 5]] y_orig = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2] x = np.atleast_2d(x_orig) y = np.atleast_2d(y_orig).T def h(X, theta): return 1 / (1 + np.exp(-mul(X, theta))) def j(X, y, theta): return (-1/m) * (mul(y.T, np.log(h(X, theta))) + mul((1-y).T, np.log(1-h(X, theta)))) def update(X, y, theta): return theta - (alpha/m * mul(X.T, (h(X, theta) - y))) theta_all = [] for _ in range(3): theta = np.random.randint(1, 100, size=(3, 1))/ 100 mul = np.matmul alpha = 0.6 m = len(x) x = np.atleast_2d(x_orig) y = np.atleast_2d(y_orig).T idx_0 = np.where(y!=_) idx_1 = np.where(y==_) y[idx_0] = 0 y[idx_1] = 1 X = np.hstack((np.ones((len(x), 1)), x)) prev_j = 10000 curr_j = j(X, y, theta) tolerance = 0.000001 theta_history = [theta] cost_history = [curr_j] while(abs(curr_j - prev_j) &gt; tolerance): theta = update(X, y, theta) theta_history.append(theta) prev_j = curr_j curr_j = j(X, y, theta) cost_history.append(curr_j[0][0]) theta_all.append(theta) print(\"classifier %d stopping with loss: %.5f\" % (_, curr_j[0][0])) def theta_2(theta, x_range): return [(-theta[0]/theta[2] - theta[1]/theta[2]*i) for i in x_range] x_range = np.linspace(-1, 4, 100) x = np.atleast_2d(x_orig) y = np.atleast_2d(y_orig).T fig, ax = plt.subplots() ax.set_xlim(-1, 4) ax.set_ylim(-1, 6) plt.scatter(x[np.where(y == 2), 0], x[np.where(y == 2), 1]) plt.scatter(x[np.where(y == 1), 0], x[np.where(y == 1), 1]) plt.scatter(x[np.where(y == 0), 0], x[np.where(y == 0), 1]) for theta in theta_all: plt.plot(x_range, theta_2(theta, x_range)) plt.title('Multiclass Logistic Regression') plt.show() Below is the plot of all the decision boundaries found by the logistic regression. Value of \\(h_\\theta^{(i)}(x)\\) is the probability of data point belonging to \\(i^{th}\\) class as seen in (1). Keeping this is mind one can decide the precedence of the class based on the values of its corresponding prediction on that data point. So, the predicted class is the one with maximum value of corresponding hypothesis. It shown in the plot below. Similar to the above implementation the classificaiton can be extented to many more classes. REFERENCES: Machine Learning: Coursera - Multiclass Classification: One-vs-All",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/09/06/multiclass-logistic-regression/"
  },

  
  
  
  {
    "title": "Logistic Regression Model",
    "text": "Basics of Machine Learning Series Index · · · Introduction Classifiction and Logistic Regression explains why logistic regression and intuition behind it. This post is about how the model works and some intuitions behind it. Consider a training set having m examples, Where \\(x \\in \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_m \\end{bmatrix} \\in \\mathbb{R}^{n+1} \\) \\(x_0 = 1\\) \\(y \\in {0, 1}\\) And hypothesis is given by, Cost Function It can be seen in Mulivariate Linear Regression that the cost function for the linear regression is given by, Where \\(Cost(h_\\theta(x), y) \\) is the cost the learning algorithm has to pay if it makes an error in the prediction and from (2), it is given by, In case of linear regression the value of cost depends on how off is the prediction of the regression from the expected value which works well for the optimization required in linear regression. But this cost function would not work well for the logistic regression because the hypothesis for logistic regression is the complex sigmoid term shown in (1), gives non-convex curve with many local minima as shown in the plot below. So gradient descent will not work properly for such a case and therefore it would be very difficult to minimize this function. import math import numpy as np import matplotlib.pyplot as plt x = np.array([-20, -5, -1, 10, -50, -10, 2, -3, 4, 1]).T y = np.array([-1, 3, -2, 3, 4, -5, 1, 3, -4, 1]).T mul = np.matmul def j(x, y, theta): h = x*theta h = 1 / (1 + np.exp(-h))\\ d = h - y s = mul(d.T, d) return s/(len(x)*2) theta = (np.array(range(-100, 100))/100).tolist() cost = [j(x, y, i) for i in theta] plt.plot(theta, cost) plt.show() So, cost function for logistic regression is given by, The plots of the functions above can be seen below. It is clear that new cost function can be minimized because its convex. Other useful properties of the chosen cost function are: if y = 1 and h(x) = 1, then Cost = 0 h(x) \\(\\to\\) 0, then Cost \\(\\to \\infty\\) if y = 0 and h(x) = 0, then Cost = 0 h(x) \\(\\to\\) 1, then Cost \\(\\to \\infty\\) Since \\(y \\in \\{0, 1\\} \\), (4) can be written as, So, adding to (2), This cost function is reached at using the principle of maximum likelyhood expectation. So now to get optimal \\(\\theta\\), Which is done using gradient descent given by, And the differential term is given by, Differential of log is given by, And differential of sigmoid function is given by, Using (9) and (10) in (8), Using (11) in (7), Which looks same as the result of gradient descent of linear regression in Mulivariate Linear Regression. But there is a difference which can be seen in the defination of the hypothesis of linear regression and logistic regression. Vectorizing (12), Where X is the design matrix. Note: Feature Scaling is as important for logistic regression as it is for linear regression as it helps the process of gradient descent. Advanced Optimization Given the functions for calculation of \\(J(\\theta)\\) and \\(\\frac {\\partial} {\\partial \\theta} J(\\theta)\\) one can apply one of the many optimization techniques other than gradient descent: Conjugate Descent BFGS L-BFGS Advantage Disadvantages No need to manually pick \\(\\alpha\\) More complex Often faster than gradient descent Harder to debug Most of these algorithms have a clever inner loop like line search algorithm which automatically finds out the best \\(\\alpha\\) value. Implementation Below is an implementation for linear decision boundary, import math import numpy as np import matplotlib.pyplot as plt x = [] y = [] for _ in range(30): i = np.random.rand() x.append(i) y.append(round(i)) x.append(0) y.append(1) x = np.atleast_2d(x).T y = np.atleast_2d(y).T plt.scatter(x[np.where(y==1)], y[np.where(y==1)]) plt.scatter(x[np.where(y==0)], y[np.where(y==0)]) theta = np.random.randint(1, 100, size=(2, 1))/ 1000000 theta = np.array([[0.5], [0]]) mul = np.matmul alpha = 0.6 m = len(x) X = np.hstack((np.ones((len(x), 1)), x)) def h(X, theta): return 1 / (1 + np.exp(-mul(X, theta))) def j(X, y, theta): return (-1/m) * (mul(y.T, np.log(h(X, theta))) + mul((1-y).T, np.log(1-h(X, theta)))) def update(X, y, theta): return theta - (alpha/m * mul(X.T, (h(X, theta) - y))) prev_j = 10000 curr_j = j(X, y, theta) tolerance = 0.000001 theta_history = [theta] cost_history = [curr_j] while(abs(curr_j - prev_j) &gt; tolerance): theta = update(X, y, theta) theta_history.append(theta) prev_j = curr_j curr_j = j(X, y, theta) cost_history.append(curr_j[0][0]) print(curr_j[0][0]) plt.plot(x, mul(X, theta)) plt.show() Plot above shows how the linear decision boundary fits the data over the iterations. The plot below is the contour plot of the cost function. Following is an implementation of non-linear decision boundary. The code is similar to the previous implementation but the data and the dimensions of the design matrix vary because of higher number of features. import math import numpy as np import matplotlib.pyplot as plt x = np.array([ [0,0], [0,1], [0, -1], [1, 0], [-1, 0], [0,2], [0, -2], [2, 0], [-2, 0] ]) y = np.atleast_2d([0, 0, 0, 0, 0, 1, 1, 1, 1]).T plt.scatter(x[np.where(y==1),0], x[np.where(y==1), 1]) plt.scatter(x[np.where(y==0),0], x[np.where(y==0), 1]) theta = np.random.randint(1, 100, size=(5, 1))/ 100 mul = np.matmul alpha = 0.1 m = len(x) X = np.hstack((np.ones((len(x), 1)), x, np.power(x,2))) def h(X, theta): return 1 / (1 + np.exp(-mul(X, theta))) def j(X, y, theta): return (-1/m) * (mul(y.T, np.log(h(X, theta))) + mul((1-y).T, np.log(1-h(X, theta)))) def update(X, y, theta): return theta - (alpha/m * mul(X.T, (h(X, theta) - y))) prev_j = 10000 curr_j = j(X, y, theta) tolerance = 0.000001 theta_history = [theta] cost_history = [curr_j] while(abs(curr_j - prev_j) &gt; tolerance): theta = update(X, y, theta) theta_history.append(theta) prev_j = curr_j curr_j = j(X, y, theta) cost_history.append(curr_j[0][0]) print(\"Regression stopped with error: %.2f\" % curr_j[0][0]) x = np.array(range(-1534,1535))/1000 y1 = [math.sqrt((-theta[0]-(theta[3]*i*i))/theta[4]) for i in x.tolist()] y2 = [-math.sqrt((-theta[0]-(theta[3]*i*i))/theta[4]) for i in x.tolist()] plt.plot(x, y1, 'b') plt.axis('equal') plt.plot(x, y2, 'b') plt.title('Non-Linear Decision Boundary') plt.show() The following plot shows the circular decision boundary. A rough implementation of all these plots and some more can be found here. REFERENCES: Machine Learning: Coursera - Logistic Regression Model Machine Learning: Coursera - Simplified Cost Function and Gradient Descent Machine Learning: Coursera - Advanced Optimization",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/09/02/logistic-regression-model/"
  },

  
  
  
  {
    "title": "Classification and Logistic Regression",
    "text": "Basics of Machine Learning Series Index · · · Introduction Classification is a supervised learning problem wherein the target variable is categorical unlike regression where the target variable is continuous. Classification can be binary i.e. only two possible values of the target variable or multi-class i.e. more than two categories. The most basic step would be to try and fit regression curve to see if one can achieve classification using the same approach. Below is a plot attempting the same. It can be seen below that the attempt to achieve classification using regression curve and thresholding will not always yield conclusive results. In first case say only red data points are in the dataset, then fitting the curve and setting the threshold at 0.5 would work, but say an outlier is present like the blue data point then the same decision boundary D1 would shift to D2 if the threshold is kept constant and the boundary would not be perfect. Hence there is a need of Decision Boundary instead of predictive curve. Applying linear regression to classification problem might work in some cases but is not advisable as it would not scale with complexity. Another issue with application of linear regession to classification would be that even know the categorical variables are discreet say 1s and 0s, the hypothesis would give continuous values which maybe much greater that 1 or much lesser than 0. This issue can be solved by using logistic regression where Logistic Regression Since (1) is to be true, the hypothesis from linear regression given by \\(h_\\theta(x) = \\theta^T\\,x\\) will not work for logistic regression. Hence there is a need of squashing function i.e. a function which limits the output of hypothesis between given range. For logistic regression sigmoid function is used as the squashing function. The hypothesis for logistic regression is give by, Where \\(g(z) = {1 \\over 1 + e^{-z}}\\) and is called sigmoid function or logistic function. Plot of the sigmoid function is given below which shows no matter what the value of x, the function returns a value between 0 and 1 consistent with (1). The value of hypothesis is interpretted as the probability that the input x belongs to class y=1. i.e. probability that y=1, given x, parametrized by \\(\\theta\\). It can be mathematically represented as, The fundamental properties of probability holds here, i.e., Decision Boundary for the given hypothesis of logistic regression in (2), say \\(\\delta=0.5\\) is chosen as the threshold for the binary classification, i.e. From the plot of sigmoid function, it is seen that Using (6), (5) can be rewritten as, Suppose the training data is as show in the plot above where dots and Xs are the two different classes. Let the hypothesis \\(h_\\theta(x)\\) and the optimal value of \\(\\theta\\) be given by, Using the \\(\\theta\\) from (9) and hypothesis from (8) , (7) can be written as, If the line \\(x_1 + x_2 = 12\\) is plotted as shown in the plot above then the region below i.e. the yellow region is where \\(x_1 + x_2 \\lt 12\\) and predicted 0 and similarly the white region above the line \\(x_1 + x_2 = 12\\) is where \\(x_1 + x_2 \\geq 12\\) and hence predicted 1. The line here is called the decision boundary. As the name suggests this line seperates the region with prediction 0 from region with prediction 1. Decision boundary and prediction regions are the property of the hypothesis and not of the dataset. Dataset is only used to fit the parameters, but once the parameters are determined they solely define the decision boundary It is possible to achieve non-linear decision boundaries by using the higher order polynomial terms and can be encorporated in a way similar to how multivariate linear regression handles polynomial regression. The plot above is an example of non-linear decision boundary using higher order polynomial logistic regression. Say, the hypothesis of the logistic regression has higher order polynomial terms, and is given by, The, \\(\\theta\\) given below would form an apt decision boundary, Substituting (12) in (11), So, from (7), the decision boundary is given by, Which the equation of a circle at origin with radius 0, as can be seen in the plot above. And, using the \\(\\theta\\) from (12) and hypothesis from (11) , (7) can be written as, As the order of features is increased more and more complex decision boundaries can be achieved by logistic regression. Gradient Descent is used to fit the parameter values \\(\\theta\\) in (9) and (12). REFERENCES: Machine Learning: Coursera - Classification Machine Learning: Coursera - Hypothesis Representation Machine Learning: Coursera - Decision Boundary",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/08/31/classification-and-representation/"
  },

  
  
  
  {
    "title": "Normal Equation",
    "text": "Basics of Machine Learning Series Index · · · Introduction Gradient descent is an algorithm which is used to reach an optimal solution iteratively using the gradient of the loss function or the cost function. In contrast, normal equation is a method that helps solve for the parameters analytically i.e. instead of reaching the solution iteratively, solution for the parameter \\(\\theta\\) is reached at directly by solving the normal equation. Intuition Consider a one-dimensional equation for the cost function given by, Where \\(\\theta \\in \\mathbb{R} \\) According to calculus, one can find the minimum of this function by calculating the derivative and solving the equation by setting derivative equal to zero, i.e. Similarly, extending (1) to multi-dimensional setup, the cost function is given by, Where \\(\\theta \\in \\mathbb{R}^{n+1} \\) n is the number of features m is the number of training samples And similar to (2), the minimum of (3) can be found by taking partial derivatives w.r.t. individual \\(\\theta_i \\forall i \\in (0, 1, 2, \\cdots, n) \\) and solving the equations by setting them to zero, i.e. where \\(i \\in (0, 1, 2, \\cdots, n)\\) Through derivation one can find that \\(\\theta\\) is given by, Feature scaling is not necessary for the normal equation method. Reason being, the feature scaling was implemented to prevent any skewness in the contour plot of the cost function which affects the gradient descent but the analytical solution using normal equation does not suffer from the same drawback. Comparison between Gradient Descent and Normal Equation Given m training examples, and n features Gradient Descent Normal Equation Proper choice of \\(\\alpha\\) is important \\(\\alpha\\) is not needed Iterative Method Direct Solution Works well with large n. Complexity of algorithm is O(\\(kn^2\\)) Slow for large n. Need to compute \\((X^TX)^{-1}\\). Generally the cost for computing the inverse is O(\\(n^3\\)) Generally if the number of features is less than 10000, one can use normal equation to get the solution beyond which the order of growth of the algorithm will make the computation very slow. Non-invertibility Matrices that do not have an inverse are called singular or degenerate. Reasons for non-invertibility: Linearly dependent features i.e. redundant features. Too many features i.e. \\(m \\leq n\\), then reduce the number of features or use regularization. Calculating psuedo-inverse instead of inverse can also solve the issue of non-invertibility. Implementation import matplotlib.pyplot as plt import numpy as np \"\"\" Dummy Data for Linear Regression \"\"\" data = [(1, 1), (2, 2), (3, 4), (4, 3), (5, 5.5), (6, 8), (7, 6), (8, 8.4), (9, 10), (5, 4)] \"\"\" Matrix Operations \"\"\" inv = np.linalg.inv mul = np.matmul X = [] y = [] for x_i, y_i in data: X.append([1, x_i]) y.append(y_i) X = np.array(X) y = np.atleast_2d(y).T \"\"\" Theta Calculation Using equation (5) \"\"\" theta = mul(mul(inv(mul(X.T, X)), X.T), y) \"\"\" Prediction of y using theta \"\"\" y_pred = np.matmul(X, theta) \"\"\" Plot Graph \"\"\" plt.scatter([i[0] for i in data], [i[1] for i in data]) plt.plot([i[0] for i in data], y_pred, 'r') plt.title('Regression using Normal Equation') plt.xlabel('x') plt.ylabel('y') plt.show() Derivation of Normal Equation Given the hypothesis, \\(\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\) Let X be the design matrix wherein each row corresponds to the features in \\(i^{th}\\) sample of the m samples. Similarly, y is the vector with all the target values for all the m training samples. The cost function for the hypothesis (6) is given by (3). The cost function can be vectorized as follows for replacing the sigma operation with the sum over terms for matrix multiplication, Since \\(X\\theta\\) and \\(y\\) both are vectors, \\((X\\theta)^Ty = y^T(X\\theta)\\). So (7) can be further simplified as, Taking partial derivative w.r.t \\(\\theta\\) and equating to zero, Let, Taking partial derivatives, Vectorizing (10), Similarly, let, Taking partial derivatives, Vectorizing above equations, Substitution (13) and (11) in (8), If \\(X^T X\\) is invertible, then, which is same as (5) REFERENCES: Machine Learning: Coursera - Normal Equation Derivation of the Normal Equation for linear regression Normal Equation and Matrix Calculus",
    "tags": "machine-learning mathematics andrew-ng basics-of-machine-learning",
    "url": "/2017/08/28/normal-equation/"
  },

  
  
  
  {
    "title": "Multivariate Linear Regression",
    "text": "Basics of Machine Learning Series Index · · · Introduction Multivariate linear regression is the generalization of the univariate linear regression seen earlier i.e. Cost Function of Linear Regression. As the name suggests, there are more than one independent variables, \\(x_1, x_2 \\cdots, x_n\\) and a dependent variable \\(y\\). Notation \\(x_1, x_2 \\cdots, x_n\\) denote the n features \\(y\\) denotes the output variable to be predicted \\(n\\) is number of features \\(m\\) is the number of training examples \\(x^{(i)}\\) is the \\(i^{th}\\) training example \\(x_j^{(i)}\\) is the \\(j^{th}\\) feature of the \\(i^{th}\\) training example Hypothesis The hypothesis in case of univariate linear regression was, Extending the above function to multiple features, hypothesis of multivariate linear regression is given by, Where, \\(\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\\\ \\end{bmatrix} \\in \\mathbb{R}^{n+1}\\) and \\(x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} \\in \\mathbb{R}^{n+1} \\) Cost Function The cost function for univariate linear regression was, Extending the above function to multiple features, the cost function for multiple features is given by, Where \\(\\theta\\) is a vector give by \\(\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\\\ \\end{bmatrix} \\) Gradient Descent Note: simultaneous update only Evaluating the partial derivative \\({\\partial \\over \\partial \\theta_j} J(\\theta)\\) gives, It can be easily seen that (4) is generalization of the update equation for univariate linear regression, because if we take only two features \\(\\theta_0\\) and \\(\\theta_1\\) and substitute in (4) the values, it results in the same equations as in Univariate Linear Regression Feature Scaling It is found that during gradient descent if the features are on the same scale then the algorithm tends to work better than when the features are not appropriately scaled in the same range. The plot below shows the effect of feature scaling on the contour plot of the cost function of hypothesis based on these features. As seen above, if the contours are skewed then learning steps would take longer to converge as the steps would be more prone to oscillatory behaviour as shown in the left plot. Whereas if the features are properly scaled, then the plot is evenly distributed and the steps of gradient descent have better profile of convergence. Scaling of features between 0 and 1 is achieved by dividing the features by max. This helps in keeping all the features in appropriate ranges. The aim is to ideally keep the features around the range -1 to 1. Different ways of achieving feature scaling: Normalization: Divide each feature by max of the feature column. Mean Normalization: Replace a feature \\(x_i\\) with \\(x_i - \\mu_i\\) so that the approximate mean of the features is 0 which is then normalized. Not applied to the feature \\(x_0\\) Where \\(x_i\\) is the feature value \\(\\mu_i\\) is the mean \\(S_i\\) is the standard deviation or the range i.e. \\(max - min\\) Learning Rate There are several ways of debugging the gradient descent algorithm. One of the ways is to plot the graph of cost function as a function of number of epochs. If the value is decreasing for every epoch then the descent is working fine. If the curve is plotted as shown one can easily infer that a saturation is reached after 150 iterations. Automatic Convergence Test: Gradient descent can be considered to be converged if the drop in cost function is not more than a preset threshold say \\(10^{-3}\\) Looking at the plot can point out if the algorithm is not working properly. For example, plot A is a proper learning curve but if the plot shows that value of cost function is increasing as in plot C, then this indicates the algorithm is diverging. It generally happens if the value of learning rate \\(\\alpha\\) is too high. Also, if the plot shows that the value is oscillating or fluctuating then the learning rate needs to be reduced as the steps are not small enough to proceed to the minima. For sufficiently small \\(\\alpha\\), gradient descent should decrease on every iteration. Very small learning rate is not advisable as the algorithm will be slow to converge as seen in plot B. In order to choose optimum value of \\(\\alpha\\) run the algorithm with different values like, 1, 0.3, 0.1, 0.03, 0.01 etc and plot the learning curve to understand whether the value should be increased or descreased. Feature Engineering Sometimes it might be fruitful to generate new features by combining the existing ones. For example, given width and length of a property to predict price it might be helpful to use area of the property i.e. width * length as an additional feature. Polynomial Regression The concept of feature engineering can be used to achieve polynomial regression. Say the polynomial hypothesis chosen is, This function can be addressed as multivariate linear regression by substitution and is given by, Where \\(x_n = x^n\\) Note: if using features like this then it is very important to apply feature scaling in order to avert issues related to feature range imbalance. This technique can be very powerful because one can fit all types of features using the substitution model. For example one can get a non-decreasing function as opposed to quadratic function which comes back down by using the following function Implementation import random import matplotlib.pyplot as plt import math import numpy as np \"\"\" Dummy Data for Multivariate Regression \"\"\" data = [(1, 1), (2, 2), (3, 4), (4, 3), (5, 5.5), (6, 8), (7, 6), (8, 8.4), (9, 10), (5, 4)] \"\"\" Plot the line using theta_values \"\"\" def plot_line(formula, x_range, order_of_regression): x = np.array(x_range).tolist() y = [formula(update_features(x_i, order_of_regression)) for x_i in x] plt.plot(x, y) \"\"\" Hypothesis Function \"\"\" def h(x, theta): return np.matmul(theta.T, x)[0][0] \"\"\" Partial Derivative w.r.t. theta_i \"\"\" def j_prime_theta(data, theta, order_of_regression, i): result = 0 m = len(data) for x, y in data : x = update_features(x, order_of_regression) result += (h(x, theta) - y) * x[i] return (1/m) * result \"\"\" Update features by order of the regression \"\"\" def update_features(x, order_of_regression): features = [1] for i in range(order_of_regression): features.append(math.pow(x, i+1)) return np.atleast_2d(features).T \"\"\" Cost Function \"\"\" def j(data, theta, order_of_regression): cost = 0 m = len(data) for x, y in data: x = update_features(x, order_of_regression) cost += math.pow(h(x, theta) - y, 2) return (1/(2*m)) * cost \"\"\" Simultaneous Update \"\"\" def update_theta(data, alpha, theta, order_of_regression): temp = [] for i in range(order_of_regression+1): temp.append(theta[i] - alpha * j_prime_theta(data, theta, order_of_regression, i)) theta = np.array(temp) return theta \"\"\" Gradient Descent For Multivariate Regression \"\"\" def gradient_descent(data, alpha, tolerance, theta=[], order_of_regression = 2): if len(theta) == 0: theta = np.atleast_2d(np.random.random(order_of_regression+1) * 100).T prev_j = 10000 curr_j = j(data, theta, order_of_regression) print(curr_j) cost_history = [] theta_history = [] while(abs(curr_j - prev_j) &gt; tolerance): try: cost_history.append(curr_j) theta_history.append(theta) theta = update_theta(data, alpha, theta, order_of_regression) prev_j = curr_j curr_j = j(data, theta, order_of_regression) print(curr_j) except: break print(\"Stopped with Error at %.5f\" % prev_j) return theta theta = gradient_descent(data, 0.001, 0.001) The above plot shows the working of multivariate linear regression to fit polynomial curve. The higher order terms of the polynomial hypothesis are fed as separate features in the regression. The plot is the shape of a parabola which is consistent with the shape of curves of second order polynomials. Note: The implementation above does not have scaled features. It would be harder to make the algorithm converge if the features are not scaled. But if they are scaled properly, not only does the algorithm converges better but also faster. Below is the plot of the curve fitting by gradient descent when the features are scaled appropriately. A rough implementation of the feature scaling used to get the plot above can be found here. REFERENCES: Machine Learning: Coursera - Multivariate Linear Regression Machine Learning: Coursera - Gradient Descent for Multiple Variables Machine Learning: Coursera - Gradient Descent in Practice I - Feature Scaling Machine Learning: Coursera - Gradient Descent in Practice II - Learning Rate Machine Learning: Coursera - Feature and Polynomial Regerssion",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/08/23/multivariate-linear-regression/"
  },

  
  
  
  {
    "title": "Linear Algebra Review",
    "text": "Basics of Machine Learning Series Index · · · Matrices In mathematics, a matrix is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. For example, A is a matrix below, Dimension of a matrix is given by n * m where n is number of rows and m is number of columns. So the matrix above is a 2*3 matrix. It is also sometimes represented as \\(\\mathbb{R}^{2*3}\\). Matrices are generally represented with uppercase. Also, if A is a matrix, \\(A_{ij}\\) is the i,j entry i.e. the element in \\(i^{th}\\) row and \\(j^{th}\\) column. For example, Vectors In mathematics, vector is a matrix that has only one column. For example, x is a vector below, So effectively vector is a n * 1 matrix where n is the number of rows. It is termed as n-dimensional vector. It is also sometimes represented as \\(\\mathbb{R}^n\\). Vectors are generally represented with lowercase. Also, if x is a matrix, \\(x_{i}\\) is the element in \\(i^{th}\\) row. For example, Vectors can be 0 or 1 indexed i.e. the start of index numbering may begin with 0 or 1. Generally in mathematics, 1-indexed notation is followed while in computer science 0-indexed notation is more popular. Matrix Addition Matrix addition is nothing but adding them element by element. Only matrices of same dimensions can be added The resultant matrix has same dimension. The operation is commutative, associative and distributive. For example, Scalar Multiplication Scalar multiplication is multiplication of a real number with each element of the matrix. The resultant matrix has the same dimension. The operation is commutative, associative and distributive. For example, Matrix operation follow the BODMAS rule for the order of precedence. Matrix Vector Multiplication Consider matrix A and vector x, then the matrix-vector multiplication is given by, Following properties are inferred: Operation is not always commutative. Number of columns in matrix A has to match the number of rows in vector x. The resultant vector of the multiplication is of dimension n as can be seen above. So if A is m*n and x is n-dimensional then, resultant is m-dimensional vector. Application A hypothesis \\(h_\\theta(x) = -40 + 0.25\\,x\\) can be applied to a set of x’s such as (2104, 1416, 1534, 852) then it can be done as follows, Matrix-Matrix Multiplication It is a binary operation that produces a matrix from two matrices. So, if A is \\(n * m\\) matrix and B is a \\(m * p\\) matrix, then their product AB is a \\(n * p\\) matrix. The m enteries along rows of A are multiplied with the m enteries along the column of B and summed to produce elements of AB. When two linear transformations are represented by matrices, then the matrix product represents the composition of the two transformations. Following properties are inferred: Operation is not always commutative. Number of columns in matrix A has to match the number of rows in vector x. Application Consider three hypothesis as follows, All three can be applied to a set of inputs as shown in matrix-vector multiplication. Here, each column corresponds to a specific hypothesis. Matrix Multiplication Properties Commutative Property: Scalar multiplication is commutative while matrix multiplication is not commutative. Associative Property: Scalar and matrix multiplication are both associative. Identity Matrix: Denoted by I (or \\(I_{n*n}\\)) has all elements zero except for the main diagonal elements which are set to 1. It has the following properties. Inverse In the space of real numbers each number is said to have an inverse if the product of the number and the inverse results in the identity i.e. 1. Also not all the real numbers have an inverse, for example, the number 0 does not have an inverse, because 1/0 is undefined. Similarly, a matrix A is said to have an inverse if there exists a \\(A^{-1}\\) such that Only square matrices have inverses. Matrices that do not have an inverse are called singular or degenerate matrices. Transpose Given a matrix A, having dimension m * n and let \\(B = A^T\\) be its transpose, then B is a n * m matrix such that, It is basically the operation where each row is sequentially replaced as a column in the resultant matrix. REFERENCES: Machine Learning: Coursera - Matrices and Vectors Machine Learning: Coursera - Addition and Scalar Multiplication Machine Learning: Coursera - Matrix Vector Multiplication Machine Learning: Coursera - Matrix Matrix Multiplication Machine Learning: Coursera - Matrix Multiplication Properties Machine Learning: Coursera - Inverse and Transpose Matrix Multiplication: Wikipedia",
    "tags": "machine-learning mathematics andrew-ng basics-of-machine-learning",
    "url": "/2017/08/20/linear-algebra/"
  },

  
  
  
  {
    "title": "Gradient Descent for Linear Regression",
    "text": "Basics of Machine Learning Series Index · · · Introduction The posts Cost Function of Linear Regression and Gradient Descent introduced the linear regression cost function and the gradient descent algorithm individually. If the gradient descent is applied to the linear regression cost function would help reach an optimal solution without much manual intervention. Gradient Descent Gradient descent algorithm can be summarized as, Where := is the assignment operator \\(\\alpha\\) is the learning rate which basically defines how big the steps are during the descent \\( \\frac {\\partial} {\\partial \\theta_j} J(\\theta_0, \\theta_1)\\) is the partial derivative term j = 0, 1 represents the feature index number Linear Regression Linear regression hypothesis is given by, And the corresponding cost function is given by, Gradient Descent for Linear Regression Gradient descent is applied to the optimazation problem of the cost function of the linear regression given in (2) in order to find parameters that minimize the cost. In order to apply gradient descent, the derivative term i.e. \\(\\frac {\\partial} {\\partial \\theta_j} J(\\theta_0, \\theta_1)\\) needs to be calculated. Updating the values of the derivative term in the gradient descent algorithm given in (1) where the update must be done simultaneously, The cost function for a linear regression is a convex function i.e. a bow-shaped function and has only one global minimum and no local minima. So it does not face the issue of getting stuck in local minima. It can be understood better with the below plots. The gradient descent technique that uses all the training examples in each step is called Batch Gradient Descent. This is basically the calculation of the derivative term over all the training examples as it can be seen it the equation above. The equation of linear regression can also be solved using Normal Equations method, but it poses a disadvantage that it does not scale very well on larger data while gradient descent does. Implementation import random import matplotlib.pyplot as plt import math import numpy as np \"\"\" Dummy Data for Linear Regression \"\"\" data = [(1, 1), (2, 2), (3, 4), (4, 3), (5, 5.5), (6, 8), (7, 6), (8, 8.4), (9, 10), (5, 4)] \"\"\" Plot the line using theta_values \"\"\" def plot_line(formula, x_range): x = np.array(x_range) y = formula(x) plt.plot(x, y) \"\"\" Hypothesis Function \"\"\" def h(x, theta_0, theta_1): return theta_0 + theta_1 * x \"\"\" Partial Derivative w.r.t. theta_1 \"\"\" def j_prime_theta_1(data, theta_0, theta_1): result = 0 m = len(data) for x, y in data : result += (h(x, theta_0, theta_1) - y) * x return (1/m) * result \"\"\" Partial Derivative w.r.t. theta_0 \"\"\" def j_prime_theta_0(data, theta_0, theta_1): result = 0 m = len(data) for x, y in data : result += (h(x, theta_0, theta_1) - y) return (1/m) * result \"\"\" Cost Function \"\"\" def j(data, theta_0, theta_1): cost = 0 m = len(data) for x, y in data: cost += math.pow(h(x, theta_0, theta_1) - y, 2) return (1/(2*m)) * cost \"\"\" Simultaneous Update \"\"\" def update_theta(data, alpha, theta_0, theta_1): temp_0 = theta_0 - alpha * j_prime_theta_0(data, theta_0, theta_1) temp_1 = theta_1 - alpha * j_prime_theta_1(data, theta_0, theta_1) theta_0 = temp_0 theta_1 = temp_1 return theta_0, theta_1 \"\"\" Gradient Descent For Linear Regression \"\"\" def gradient_descent(data, alpha, tolerance, theta_0=None, theta_1=None): if not theta_0: theta_0 = random.random() * 100 if not theta_1: theta_1 = random.random() * 100 prev_j = 10000 curr_j = j(data, theta_0, theta_1) theta_0_history = [] theta_1_history = [] cost_history = [] while(abs(curr_j - prev_j) &gt; tolerance): try: cost_history.append(curr_j) theta_0_history.append(theta_0) theta_1_history.append(theta_1) theta_0, theta_1 = update_theta(data, alpha, theta_0, theta_1) prev_j = curr_j curr_j = j(data, theta_0, theta_1) except: break print(\"Stopped with Error at %.5f\" % prev_j) return theta_0, theta_1 theta_0, theta_1 = gradient_descent(data, alpha=0.01, tolerance=0.00001) The plot shows the adjustment of the values of \\(\\theta_0\\) and \\(\\theta_1\\) for fitting the best line through the given data points. REFERENCES: Machine Learning: Coursera - Gradient Descent for Linear Regression",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/08/17/gradient-descent-for-linear-regression/"
  },

  
  
  
  {
    "title": "Gradient Descent",
    "text": "Basics of Machine Learning Series Index · · · Gradient Descent Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent is also known as steepest descent. What and How ? In the post Cost Function of Linear Regression, it was deduced that in order to find the best hypothesis, it is required to minimize the cost fuction \\(J(\\theta_0, \\theta_1)\\). And it is not always possible to achieve this goal manually as the complexity and dimensionality of the problem increases. To summarize, given the cost function \\(J(\\theta_0, \\theta_1)\\), the objective is, This is where gradient descent steps in. There are two basic steps involved in this algorithm: Start with some random value of \\(\\theta_0\\), \\(\\theta_1\\). Keep updating the value of \\(\\theta_0\\), \\(\\theta_1\\) to reduce \\(J(\\theta_0, \\theta_1)\\) until minimum is reached. Actually graident descent is a much more robust algorithm capable of solving higher dimensionality problems as well i.e. given a cost fuction \\(J(\\theta_0, \\theta_1, … , \\theta_n)\\), it can help achieve, Depending on initialization gradient descent can end up in different local minimas and a unique solution is not guaranteed. It can be seen in the plot above, if the initialization is at point A then it leads to B while if the initialization is at point C then it would lead to point D after execution. Definition Where := is the assignment operator \\(\\alpha\\) is the learning rate which basically defines how big the steps are during the descent \\( \\frac {\\partial} {\\partial \\theta_j} J(\\theta_0, \\theta_1)\\) is the partial derivative term j = 0, 1 represents the feature index number Also the parameters should be updated simulatenously, i.e. , The notion of simulatenous update is introduced because that is how the gradient descent would work naturally i.e. in nature the path taken at a point would be defined by the gradient along components at a point. But if the update is not simulatenous then the gradient is not computed at the same point because the updated value of one parameter is used in calculating the update of another. In practice this method might also work without any issues or behave strangely in some other cases, so by definition and the intuition of the gradient descent it would be incorrect implementation and would represent some other algorithm with properties different from those of gradient descent. Understanding Gradient Descent Consider a simpler cost function \\(J(\\theta_1)\\) and the objective, As defined earlier, start off by random initialization say at point A as shown in the plot below. Then according to the update definition of the algorithm, the derivative term is represented by the slope at any point also shown in the plot. In this case it would be a positive slope and since \\(\\alpha\\) is a positive real number the overall term \\(- \\alpha \\frac {d} {d \\theta_1} J(\\theta_1)\\) would be negative. This means the value of \\(\\theta_1\\) will be decreased in magnitude as shown by the arrows. Similary, if the initialization was at point B, then the slope of the line would be negative and then update term would be positive leading to increase in the value of \\(\\theta_1\\) as shown in the plot. So, no matter where the \\(\\theta_1\\) is initialized, the algorithm ensures that parameter is updated in the right direction towards the minima, given proper value for \\(\\alpha\\) is chosen. This is intuitive because that is exactly what is needed for minimizing the cost function. The term \\(alpha\\), as it can be seen from the equation will determine the magnitude of the update term, \\(- \\alpha \\frac {d} {d \\theta_1} J(\\theta_1)\\) i.e. if the value is higher the steps of update would be proportionally larger. Learning Rate, \\(\\alpha\\) A proper value of \\(\\alpha\\) plays an important role in gradient descent. Choose an alpha too small and the algorithm will converge very slowly or get stuck in the local minima. Choose an \\(\\alpha\\) too big and the algorithm will never converge either because it will oscillate between around the minima or it will diverge by overshooting the range. All these cases can be adequately understood by the plots below. The plot on the left shows a large learning rate. This leads to overshooting because the steps taken by the algorithm in updating the parameters are so big that the optimization problem can never converge to the minima. Instead it overshoots with every iteration to either start diverging or to oscillate between points that are not the optimum solution. The plot on the right is the case where learning rate is too small. As a result the steps taken i.e. the updates to the parameter \\(\\theta_n\\) are so small that it would take a very long time to converge. More so because as we approach the minima the value of the slope i.e. the value of the differential term will also decrease and as a result the update term i.e. the product of small \\(\\alpha\\) and small differential term would effect only a minute change. The other issue associated with the small learning rate is that of getting stuck in a local minima and hence never reaching the global minima. Gradient descent can converge to a local optimum, even with a fixed learning rate. Because as we approach the local minimum, gradient descent will automatically take smaller steps as the value of slope i.e. derivative decreases around the local minimum. The plot above tries to summarize the effect of \\(\\alpha\\) value on the convergence of the graident descent algorithm. The yellow plot shows the divergence of the algorithm when the learning rate is really high wherein the learning steps overshoot. The green plot shows the case where learning rate is not as large as the previous case but is high enough that the steps keep oscillating at a point which is not the minima. The red plot would be the optimum curve for the cost drop as it drops steeply initially and then saturates very close to the optimum value. The blue plot is the least value of \\(\\alpha\\) and converges very slowly as the steps taken by the algorithm during update steps are very small. Summary Gradient Descent is an optimization algorithm. It can be applied to minimize any cost function J, and not just to linear regression. It is a rather generalized powerful technique widely used in learning problems to reach the optimum parameter sets. It is coupled with various learning techniques and works perfectly well with all of them. For example gradient descent works equally well with linear regression and neural networks. REFERENCES: Machine Learning: Coursera - Gradient Descent",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/08/15/gradient-descent/"
  },

  
  
  
  {
    "title": "Cost Function of Linear Regression",
    "text": "Basics of Machine Learning Series Index · · · Linear Regression Linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression or univariate linear regression. For more than one explanatory variable, the process is called multiple linear regression. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Hypothesis The hypothesis for a univariate linear regression model is given by, Where \\(h_\\theta (x)\\) is the hypothesis function, also denoted as \\(h(x)\\) sometimes \\(x\\) is the independent variable \\(\\theta_0\\) and \\(\\theta_1\\) are the parameters of the linear regression that need to be learnt Parameters of the Hypothesis In the above case of the hypothesis, \\(\\theta_0\\) and \\(\\theta_1\\) are the parameters of the hypothesis. In case of a univariate linear regression, \\(\\theta_0\\) is the y-intercept and \\(\\theta_1\\) is the slope of the line. Different values for these parameters will give different hypothesis function based on the values of slope and intercepts. Cost Function of Linear Regression Assume we are given a dataset as plotted by the ‘x’ marks in the plot above. The aim of the linear regression is to find a line similar to the blue line in the plot above that fits the given set of training example best. Internally this line is a result of the parameters \\(\\theta_0\\) and \\(\\theta_1\\). So the objective of the learning algorithm is to find the best parameters to fit the dataset i.e. choose \\(\\theta_0\\) and \\(\\theta_1\\) so that \\(h_\\theta (x)\\) is close to y for the training examples (x, y). This can be mathematically represented as, Where \\(h_\\theta(x^{(i)}) = \\theta_0 + \\theta_1\\,x^{(i)} \\) \\((x^{(i)},y^{(i)})\\) is the \\(i^{th}\\) training data m is the number of training example \\({1 \\over 2}\\) is a constant that helps cancel 2 in derivative of the function when doing calculations for gradient descent So, cost function is defined as follows, which is basically \\( {1 \\over 2} \\bar{x}\\) where \\(\\bar{x}\\) is the mean of squares of \\(h_\\theta(x^{(i)}) - y^{(i)}\\), or the difference between the predicted value and the actual value. And learning objective is to minimize the cost function i.e. This cost function is also called the squared error function because of obvious reasons. It is the most commonly used cost function for linear regression as it is simple and performs well. Understanding Cost Function Cost function and Hypthesis are two different concepts and are often mixed up. Some of the key differences to remember are, Hypothesis \\(h_\\theta(x)\\) Cost Function \\(J(\\theta_1)\\) For a fixed value of \\(\\theta_1\\), function of x Function of parameter \\(\\theta_1\\) Each value of \\(\\theta_1\\) corresponds to a different hypothesis as it is the slope of the line For any such value of \\(\\theta_1\\), \\(J(\\theta_1)\\) can be calculated using (3) by setting \\(\\theta_0 = 0\\) It is a linear line or a hyperplane Squared error cost function given in (3) is convex in nature Consider a simple case of hypothesis by setting \\(\\theta_0 = 0\\), then (1) becomes which corresponds to different lines passing through the origin as shown in plots below as y-intercept i.e. \\(\\theta_0\\) is nulled out. For the given training data, i.e. x’s marked on the graph, one can calculate cost function at different values of \\(\\theta_1\\) using (3) which can be expressed in the following form using (5), At \\(\\theta_1 = 2\\), At \\(\\theta_1 = 1\\), At \\(\\theta_1 = 0.5\\), On plotting points like this further, one gets the following graph for the cost function which is dependent on parameter \\(\\theta_1\\). In the above plot each value of \\(\\theta_1\\) corresponds to a different hypothesis. The optimization objective was to minimize the value of \\(J(\\theta_1)\\) from (4), and it can be seen that the hypothesis correponding to the minimum \\(J(\\theta_1)\\) would be the best fitting straight line through the dataset. The issue lies in the fact that we cannot always find the optimum global minima of the plot manually because as the number of dimensions increase, these plots would be much more difficult to visualize and interpret. So there is a need of an automated algorithm that can help achieve this objective. REFERENCES: Machine Learning: Coursera - Cost Function Machine Learning: Coursera - Cost Function Intuition I Linear Regression: Wikipedia - Cost Function",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/08/11/cost-function-of-linear-regression/"
  },

  
  
  
  {
    "title": "Model Representation And Hypothesis",
    "text": "Basics of Machine Learning Series Index · · · Notations \\(m\\): Number of training examples \\(x\\)’s: Input variables / features \\(y\\)’s: Output / Target variables \\((x,y)\\): One training example \\((x^{(i)},y^{(i)})\\): \\(i^{th}\\) training example Supervised Learning Formally stated, the aim of the supervised learning algorithm is to use the training Dataset and output a hypothesis function, h where h is a function that takes the input instance and predicts the output based on its learnings from the training dataset. As shown above, say given a dataset where each training instance consists of the area of a house and its price, the job of the learning algorithm would be to come up with a hypothesis, h such that it takes the size of house as input and predicts its price Hypothesis Hypothesis is the function that is to be learnt by the learning algorithm by the training progress for making the predictions about the unseen data. For example, for Linear Regression in One Variable or Univariate Linear Regression, the hypothesis, h is given by Where \\(h_\\theta (x)\\) is the hypothesis function, also denoted as \\(h(x)\\) sometimes \\(x\\) is the independent variable \\(\\theta_0\\) and \\(\\theta_1\\) are the parameters of the linear regression that need to be learnt REFERENCES: Machine Learning: Coursera - Model Representation",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/08/10/model-representation-and-hypothesis/"
  },

  
  
  
  {
    "title": "Supervised and Unsupervised Learning",
    "text": "Basics of Machine Learning Series Index · · · Supervised Learning Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). Regression: When the target variable is continuous. Regression in supervised learning is different from regression in statistics. Also, logistic regression is a classification technique despite its name as its response variable is categorical. Example: Given a picture of a person, predict the age on the basis of the given picture. Classification: When the target variable is categorical. Example: Given a patient with a tumor, predict whether the tumor is malignant or benign. Unsupervised Learning Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data. Clustering: Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). Applications: Organizing large computing clusters Social network analysis Market segmentation Astronomical data analysis Cocktail Party Algorithm: It is an example of source seperation algorithm or Independent Component Analysis (ICA). It can be considered to be opposite of clustering in some sense. REFERENCES: Supervised Learning: Wikipedia Machine Learning: Coursera - Supervised Learning Unsupervised Learning: Mathworks Machine Learning: Coursera - Unsupervised Learning Cluster Analysis: Wikipedia",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/08/09/supervised-learning/"
  },

  
  
  
  {
    "title": "Introduction to Machine Learning",
    "text": "Basics of Machine Learning Series Index · · · What is Machine Learning ? Machine learning is the science of getting computers to learn, without explicitly being programmed. It has developed as a subset of the larger problem of building AI i.e. Artificially Intelligent systems. Machine learning aims at developing new capabilities for computers wherein they can learn the objective intelligently without persistent human intervention, trying to mimic the way human brain learns. Application of Machine Learning Database Mining: Large Datasets are abundantly available which cannot be easily interpretted by human analysis. A machine learning algorithm can give better insights into such datasets such as web search click through data. Similarly, machine learning can help convert medical records to structured data for running various analysis on it such as survival analysis, disease prediction etc. It can also be applied to biology, engineering etc. Non-programmable Applications: For example, one cannot write a program for autonomous driving cars, handwriting recognitions, most of the NLP problems such as word-sense disambiguation, computer vision etc. Self-customizing Programs: For example, the recommender systems from websites like amazon and netflix are machine learning algorithms because it would be impossible to write programs manually to serve each consumer personalized recommendations. Understanding human learning: Understanding how the human brain works will help in the building of AI. Definations Field of study that gives the computers the ability to learn without being explicitly programmed. Well-Posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. Example: Playing chess E = the experience of playing many games of chess T = the task of playing chess P = the probability that the program will win the next game Types of Machine Learning Algorithms Supervised Learning Unsupervised Learning Others: Reinforcement Learning, Recommender Systems REFERENCES: Machine Learning: Coursera - Welcome Machine Learning: Coursera - What is Machine Learning",
    "tags": "machine-learning andrew-ng basics-of-machine-learning",
    "url": "/2017/08/09/introduction-to-machine-learning/"
  },

  
  
  
  {
    "title": "The Normal Distribution",
    "text": "Introduction The concept of normal distribution as explained using the game of darts is easier to understand and explain. Consider a game of dart with aim of throwing the dart at the origin of a cartesian plane. The errors in throwing the dart at the origin will have random errors and produce varying results in different trials. Some of the assumptions one can make in this game are: The errors do not depend on the orientation of the cartesian plane. Errors in perpendicular directions are independent i.e. dart hitting too high does not alter the probability of it being off to the right. Large errors are less likely to occur than the small errors. Determining the Shape of the Distribution The probability of dart falling in a region that lies in the vertical strip from \\(x\\) to \\(x + \\Delta x\\) can be given by, Similarly, the probability of dart landing in horizontal strip from \\(y\\) to \\(y + \\Delta y\\) can be given by, Because the two events are assumed to be independent, the probability of dart falling in the shaded region is given by Also, since orientation does not alter probability of an error, any region r units from the origin and with area \\(\\Delta x \\cdot \\Delta y\\) has the same probability and hence can be expressed as, This results in the inference Differentiating on both the sides, From the figure above, So the derivative in (1) can be expressed as, Using (2) and (3), (4) can be rewritten as, Differential equation can be solved by seperating the variables, so (5) becomes, Differential equation (6) is true for any x and y, and x and y are independent. This leads to the result that the ratio must be a constant, i.e., So, Integrating (7), So, Since, large errors are less likely than smaller errors, C must be negative, so, where k is positive. Determining the Coefficient A If p is the probability density function of a random variable following normal distribution, then the total area under the curve must be 1. So value of A should be such that this property is satisfied. The equation to ve evaluated is, Dividing both sides by A, Since the distribution is symmetric, changing the limits of distribution, Then, Since x and y are independent, (8) can be rewritten as double integral, The double integral in (9) can be evaluated as polar coordinates, Applying u-substitution to (9), differentiating w.r.t. r, So, Substituting (11) and (12) in (10), Using (9), So finally, Substituting A in the \\(p(x)\\) for normal distribution, Determining the value of k Probably k can be calculated using the formulae for mean or variance. The mean, \\(\\mu\\), is defined as the following integral, Since function \\(x\\,p(x)\\) is an odd function, \\(\\mu\\) is zero. The variance, \\(\\sigma^2\\), is given by following integral, Since mean is zero, above equation becomes, Changing the limits of integral, Evaluating the integral on left by parts where u and v are given by, Then v can be evaluated by substitution, So, Substituting (16) and (17) in (15), Applying the parts to (14) , Simplifying it further, Now consider the second part of the integral in (18), Let \\(k_1 = {k \\over 2}\\), and \\(u = x \\sqrt{k}\\) which means \\(du = dx \\sqrt{k} \\), and using gaussian integral. Substituting (19) and (20) in (18), So, Substituting A and k from (13) and (21) in the basic equation, The general equation for the normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is a simple horizontal shift of this basic distribution, REFERENCES: The Normal Distribution: A derivation from basic principles Derivation of univariate normal distribution",
    "tags": "concept mathematics probability",
    "url": "/2017/07/31/normal-distribution/"
  },

  
  
  
  {
    "title": "Random Projection in Dimensionality Reduction",
    "text": "Introduction Random Projections have emerged as a powerful method for dimensionality reduction. Theoretical results indicate that it preserves distances quite nicely but empirical results are sparse. It is often employed in dimensionality reduction in both noisy and noiseless data especially image and text data. Results of projecting on random lower-dimensional subspace yields results comparable to conventional methods like PCA etc but using it is computationally less expensive than the traditional alternatives. Curse of Dimensionality High dimensional data restricts the choice of data processing methods. A statistically optimal way of dimensionality reduction is to project the data onto a lower-dimensional orthogonal subspace that captures as much of the variations of the data as possible. The most widely used method of this sort is PCA (Principal Component Analysis). Drawback of PCA is however that it is computationally expensive to calculate as the dimensions of data increases. Johnson-Lindenstrauss Lemma If points in vector space are projected onto a randomly selected subspace of suitably high dimensions, then the distances between the points are approximately preserved. Random Projection (RP) In RP, a higher dimensional data is projected onto a lower-dimensional subspace using a random matrix whose columns have unit length. RP is computationally efficient, yet accurate enough for this purpose as it does not introduce a significant distortion in the data. It is not sensitive to impulse noise. So RP is promising alternative to some existing methods in noise reduction (like mean filtering) too. The original d-dimensional data is projected to a k-dimensional \\((k \\lt \\lt d)\\) through the origin, using a random \\(k * d\\) matrix R whose columns have unit lengths. It is given by Where \\(X_{k*N}^{RP}\\) is the k-dimensional random projection \\(R_{k*d}\\) is the random matrix used for transformation \\(X_{d*N}\\) are the original set of N d-dimensional observations The key idea of random mapping arises from Johnson-Lindenstrauss Lemma Complexity Forming a random matrix R and projecting d * N data matrix X into k dimensions is of the order O(dkN) If X is a sparse matrix with c non-zero values per column, then the complexity is O(ckN) Theoretically, equation (1) is not a projection because R is generally not orthogonal. A linear mapping like (1) can cause significant distortion in data if R is not orthogonal. Orthogonalizing R is computationally expensive. Instead of orthogonalizing, RP relies on the result presented by Hecht-Neilsen i.e. In a high dimensional space, there exists a much larger number of almost orthogonal than orthogonal directions. Thus the vectors with random directions might be sufficiently close to orthogonal, and equivalently \\(R^TR\\) would approximate an identity matrix. Experimental results show the mean squared difference between \\(R^TR\\) and identity matrix is around 1/k per element. Euclidean distances between \\(x_1\\) and \\(x_2\\) in the original large-dimensional space is given by After RP, this distance can be approximated by scaled Euclidean distance of these vectors in reduced spaces: Where d is the original and k is the reduced dimensionality of the data set and scaling factor \\(\\sqrt{d \\over k}\\) takes into account the decrease in dimensionality of the data set. According to Johnson-Lindenstrauss Lemma, the expected norm of a projection of unit vector onto a random subspace through origin is \\(\\sqrt{k \\over d}\\). The choice of R matrix is generally such that \\(r_{ij}\\) of R are often Gaussian Distributed although many other choices are available. There is a peculiar result which says one can use the following distribution which is much simpler. Practically, all zero mean, unit variance distributions of \\(r_{ij}\\) would give a mapping that satisfies the Johnson-Lindenstrauss Lemma. Equation (2) helps reduce the computational expense even further. Principal Component Analysis (PCA) PCA is eigenvalue decomposition of the data covariance matrix. It is computed as Where columns of E are the eigenvectors of the data covariance matrix \\(E{XX^T}\\) \\(\\Lambda\\) is a the diagonal matrix containing the respective eigenvalues. Dimensionality reduction of data set is obtained by projection on a subspace spanned by most important eigenvectors, given by Where The d * k matrix \\(E_k\\) contains the k eigenvectors corresponding to the k larges eigenvalues. PCA is the optimal way to project data in the mean-square sense, i.e. the squared error introduced in the projection is minimized over all projections onto a k-dimensional space. Eigen value decomposition of the data covariance matrix is very expensive. Computational Complexity : \\(O(d^2N) + O(d^3)\\) Singular Value Decomposition (SVD) Closely related to PCA, singular value decomposition is given by Where Orthogonal matrices U and V contain the left and right singular vectors of X respectively The diagonal matrix S contains the singular values of X. Using SVD, the dimensionality of data can be reduced by projecting data onto the space spanned by the left singular vectors corresponding to the k largest singular values, given by Where \\(U_k\\) is of size d * k and contains k singular vectors. Like PCA, SVD is also expensive to compute. For sparse data matrix \\(X_{d*N}\\) with about c non-zero entries per column, the computational complexity is of the order O(dcN). Latent Semantic Indexing (LSI) It is a dimensionality reduction method for text document data. Using LSI, the document data is represented in a lower-dimensional “topic” space: the documents are characterized by some underlying (latent, hidden) concepts referred to by the terms. LSI can be computed either by PCA or SVD of the data matrix of N d-dimensional document vectors. Discrete Cosine Transform (DCT) Widely used for image compression and can be used for dimensionality reduction of image data. Computational more efficient than PCA and has performance approachable to PCA. DCT is optimal for human eye: the distortions introduced occur at the highest frequencies only, neglected by human eye as noise. DCT can be performed by simple matrix operations: Image is first transformed to DCT space and dimensionality reduction is achieved during inverse transform by discarding the transform coefficients corresponding to highest frequencies. Computing DCT is not data-dependent, unlike PCA that needs the eigenvalue decomposition of data covariance matrix, which is why DCT is orders of magnitude cheaper to compute than PCA. Computational Complexity : \\(O(dN\\,log_2(dN))\\) for data matrix of size d * N. Notes About Random Projection RP does not distort the data significantly more than PCA. Also at smaller dimensions PCA seems to distort data more than RP because of removal of significantly important eigenvectors by elimination. Computational complexity of RP is significantly lesser than other methods like PCA, but more than DCT. So it can be inferred that RP are a better choice given the trade off of DCT accuracy for reduced complexity. At smaller dimension RP outperforms DCT both on accuracy and complexity. Use case can often be a factor in considering the most optimal way of dimensionality reduction. Say, even though RP outperforms DCT, it cannot be used for purposes where aim is to transmit the minimized dataset and re-obtain original data on the other end for human viewing. Psuedoinverse computation of R is expensive to compute but because R is almost orthogonal, \\(R^T\\) would be a good approximation of psuedoinverse. So the image can be computed from RP as, Where \\(X_{k*N}^{RP}\\) is the result of random projection. But the obtained image is visually worse than a DCT compressed image, to a human eye. So RP would serve very well in applications where distance or similarity between data vectors should be preseved under dimensionality reduction as well as possible, but where data is not intended to be visualized for the human eye, eg. machine vision. In case of text dataset, the error in dimensionality reduction is calculated by calculating the inner product among randomly chosen data pairs before and after transform. It is observed that RP is not as accurate as SVD but the error may be neglectable in various use cases. The possible cause could be that the Johnson-Lindenstrauss makes statement about Euclidean distance, but Inner product is a different metric even if Euclidean distance is maintained well. REFERENCES Random projection in dimensionality reduction: Applications to image and text data Various Embeddings on Digits Data - Python Sklearn",
    "tags": "concept machine-learning mathematics",
    "url": "/2017/07/28/random-projection-in-dimensionality-reduction/"
  },

  
  
  
  {
    "title": "Continuous Random Variables",
    "text": "Probability Series Basic Probability Concepts Conditional Probability and Bayes’ Rule Discrete Random Variables Continuous Random Variables · · · Continuous Random Variables A continuous random variable is a function that maps the sample space of a random experiment to an interval in real value space. A random variable is called continuous if there is an underlying function f(x) such that Where f(x) is a non negative function called probability density function (pdf) Probability Density funtion can be considered analogous to Probability Mass function of Discrete random variable but differs in that pdf does give probability directly at a value like in case of pmf. Hence the rules of probability do not apply to f(x). Pdf takes value 0 for values outside range(X). Also the following property can be inferred from rules of probability, Probability of a continuous random variable taking a range of values is given by the area under the curve of f(x) for that range. Cumulative Distribution Function (cdf) Cdf for continuous random variable is same as the one for discrete random variable. It is given by, Properties of cdf: Unlike f(x), cdf(x) is probability and follows the laws and hence, As probability is non-negative, cdf is a non-decreasing function. Differential of cdf is given by Limits of cdf are given by Some Specific Distributions Uniform Distribution \\(X = Uniform(N)\\) is used to model a scenario where all outcomes are equally likely. Uniform([c, d]) is when all the values of \\(x(c \\leq x \\leq d)\\) are equally probable. It is given by Exponential Distribution Defined using a parameter \\(\\lambda\\) and has the pdf given by It is used to model the waiting time for an event to occur eg. waiting time for nuclear decay of radioactive isotope distributed exponentially and \\(\\lambda\\) is known as the half life of the isotope. This distribution exhibits lack of memory i.e. when waiting time is modeled using exponential distributions, the probability of it happening in next N minutes remains same irrespective of the time passed. Proof: According to lack of memory property, prove \\(P(X \\gt n+w | X \\gt w ) = P(X \\gt n)\\). Normal Distribution Most commonly used distribution Also known as Gaussian distribution It is denoted by \\(N(\\mu, \\sigma^2))\\) where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance fo the given distribution. Standard Normal Distribution, denoted by Z is a normal distribution with mean = 0 and variance = 1. It is symmetric about the y-axis and follows the bell-curve. It is given by Summary of Distributions Distribution pdf(x) cdf(x) \\(Uni(c, d)\\) \\({1 \\over d-c}\\) \\({x-c \\over d-c}\\) \\(Exp( \\lambda ), \\, x \\geq 0 \\) \\( \\lambda e^{- \\lambda x} \\) \\(1 - e^{- \\lambda x}\\) \\(N(\\mu, \\sigma^2)\\) \\({1 \\over \\sigma \\sqrt{2 \\pi} } e^{ \\frac {- (x - \\mu)^2} {2 \\sigma^2} }\\) \\({1 \\over 2}[1 + erf({ x - \\mu \\over \\sigma \\sqrt{2} })]\\) Expected Value Gives the average or the mean value over all the possible outcomes of the variable. Used to measure the centrality of a random variable. For a continuous random variable X, whose pdf is \\(f(x)\\), the expected value in the interval [c, d] is given by, Expected value is often denoted by \\(\\mu\\). \\(f(x)dx\\) denotes the probability value with which X can take the infinitesimal range dx. Some other properties of expected value of a random variable: \\[E(X+Y) = E(X) + E(Y) \\tag{3}\\] \\[E(cX + d) = c * E(X) + d \\tag{4}\\] Variance and Standard Deviation For a continuous random variable X, with Expected value \\(\\mu\\), variance is given by, \\[Var(X) = E((X-\\mu)^2) \\tag{5}\\] \\[\\sigma = \\sqrt (Var(X)) \\tag{6}\\] Some other properties of variance of a random variable: \\[Var(X) = E(X^2) - (E(X))^2 \\tag{7}\\] \\[Var(aX + b) = a^2 Var(X) \\tag{8}\\] \\[Var(X+Y) = Var(X) + Var(Y) \\text { iff X and Y are independent } \\tag{9}\\] Quartiles The value of \\(x\\) for which \\(cdf(x) = p\\) is called \\(p^{th}\\) quartile of X. So, median for the continuous random variable is the \\(0.5^{th}\\) quartile REFERENCES: Continuous Random Variables",
    "tags": "mathematics probability",
    "url": "/2017/07/28/continuous-random-variable/"
  },

  
  
  
  {
    "title": "Improvements on Word2Vec",
    "text": "Distributed Vector Representation Series Word2Vec Improvements on Word2Vec · · · Skip-Gram Model Training objective of skip-gram model is to deduce word representations that help in predicting the surrounding words in a sentence or a document, i.e. give a sequence of training words \\(w_1, w_2, w_3, … , w_T\\), the objective is to maximize the average log probability, Where c is the size of the training context Larger c results in more training examples and thus higher accuracy, at the expense of the training time. Basic skip-gram formulation defines \\(p(w_{t+j}|w_t)\\) using softmax function Where \\(v_w\\) and \\(v_w^{'}\\) are the input and output vector representations of \\(w\\) \\(W\\) is the number of words in the vocabulary cost of computing \\(\\nabla log\\,p(w_O| w_I)\\) is proportional to \\(W\\) which is quite large in order of \\(10^5 - 10^7\\) terms Drawbacks of Initial Word2Vec Proposed Training time Indifference to word order and inability to represent idiomatic phrases. Improvements Hierarchical Softmax Computationally efficient approximation of full softmax. Advantageous because instead of evaluating W output nodes in the neural network, only need to evaluate \\(log_2(W)\\) nodes. Uses binary tree representation of the output layer with W nodes as its leaves. Each node represents the relative probability of its child nodes. So, probability is assigned to a leaf node through random walk from root node Each word can be reached by following an appropriate path from the root. Let \\(n(w, j)\\) be the j-th node on the path from the root to w, and let \\(L(w)\\) be the length of this path. So, \\[n(w, 1) = root\\] \\[n(w, L(w)) = w\\] For any inner child n, let \\(ch(n)\\) be arbitrary fixed child of n and let \\(\\unicode{x27E6} x \\unicode{x27E7}\\) be 1 if x is true and -1 otherwise, then hierarchical softmax defines \\(p(w_O|w_I)\\) as Where \\(\\sigma (x) = {1 \\over (1 + exp(-x))}\\) \\(\\sum_{w=1}^W p(w|w_I) = 1\\) is verifiable. Negative Sampling Alternative to Hierarchical softmax. Based on Noise Contrastive Estimation(NCE) which posits that a good model should be able to differentiate data from noise by means of logistic regression. NCE approximately maximizes the log probability of softmax, but skip-gram only aims at learning high quality word representations and hence NCE can be simplified. Negative Sampling (NEG) is defined as It replaces the \\(log\\, p(w_O | w_I)\\) term in skip-gram objective Aiming at distinguishing between \\(w_O\\) from draws from noise distribution \\(P_n(w)\\) using logistic regression, where k is the number of negative samples for each data sample, because this use case does not require maximization of the softmax log probability. k value ranges 5-20 for small datasets and 2-5 for large datasets. NCE differs from NEG in that NCE needs the sample and numerical probabilities of the noise distribution, but NEG uses only samples. Both NCE and NEG have \\(P_n(w)\\) as a free parameter but unigram distribution U(w) raised to 3/4 power i.e. \\(U(w)^{3/4}/Z\\) is found to outperform other options like unigram and uniform distribution. Subsampling of frequent words In a corpus, most frequent words can occur hundreds of millions of time such as the stopwords. These words give little information by co-occuring with other words. Consequently, vector representations of frequent words do not change significantly after training on several million examples. Imbalance between rare and frequent words are thus countered using sub-sampling approach. Each word \\(w_i\\) in the training set is discarded with a probability given by Where \\(f(w_i)\\) is frequency of word \\(w_i\\) t is a chosen threshold, around \\(10^{-5}\\) Subsampling formula is chosen heuristically Learning Phrases Aim is to learn phrases where in individual words meaning is entirely different when compared to the group of words. Start off by finding words that frequently occur together and infrequently in other contexts. Theoretically, skip-gram model can be trained with all n-grams, but would be a very memory intensive operation. A data driven approach is put forward based on unigram and bigram counts, given by Where \\(\\delta\\) is a discounting coefficient and prevents phrases formed by very infrequent words. The bigrams with score above a given threshold only are used as phrases. Often it is needed to run the process 2-4 times changing the threshold values and seeing the quality of phrases formed. Conclusions Subsampling results in faster training and significantly better representations of uncommon words. Negative sampling helps accurately train for frequent words using a simple method. REFERENCES Distributed Representations of Words and Phrases and their Compositionality",
    "tags": "NLP machine-learning papers",
    "url": "/2017/07/26/improvements-on-word2vec/"
  },

  
  
  
  {
    "title": "Discounted Cumulative Gain",
    "text": "Discounted cumulative gain DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks. Measure of ranking quality. Used to measure effectiveness of search algorithms in information retrieval. Underlying Assumptions Highly relevant documents are more useful if appearing earlier in search result. Highly relevant documents are more useful than marginally relevant documents which are better than non-relevant documents. DCG accumulated at a particular rank position \\(p\\) is given by Alternative formulation of DCG that places stronger emphasis on retrieving relevant documents is given by Both the alternatives are same if relevance values are binary i.e \\(rel_i \\in \\{0, 1\\}\\) Various formulations use \\(log_e\\) instead of \\(log_2\\). Logarithmic scale for reduction provides a smooth reduction curve and hence is used. DCG is a successor of Cumulative Gain. Cumulative Gain Does not include the position of a result in the calculation of gain of the result set. CG at a particular rank position \\(p\\) is given by \\[CG_p = \\sum_{i=1}^p rel_i\\] Where \\(rel_i\\) is the graded relevance of result at position \\(i\\). So, CG is unaffected by changes in ordering of search results and hence, DCG is used for more accurate measure. Normalized DCG Comparing a search algorithms performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of \\(p\\) should be normalized across queries. This is done by sorting all relevant documents in the corpus by their relative relevance, producing the maximum possible DCG through position \\(p\\), also called Ideal DCG (IDCG) through that position. Normalized DCG (nDCG) is given by Where Where \\(|REL|\\) is the list of documents ordered by relevance in the corpus up to position p. The average of nDCG for all queries gives a measure of average performance of ranking algorithms in the search process. For a *perfect ranking algorithm, \\[DCG_p = IDCG_p\\] \\[nDCG_p = 1.0\\] Since all nDCG are relative values between 0.0 and 1.0 they are cross-query comparable. The main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial relevance feedback is available. Example and Calculations Given a list of documents, each document is judged on a scale of 0 to 3 where 3 is the most relevant scaling down to 0 which is not relevant. Let a set of documents, S be Where relevance score by user survey is given by Relevance Set, R in the same order, CG is given by Changing the order of documents does not change the score. DCG using Logarithmic scale for reduction is given by i \\(rel_i\\) \\(log_2(i+1)\\) \\(\\frac {rel_i} {log_2(i+1)}\\) 1 3 1 3 2 2 1.585 1.262 3 3 2 1.5 4 0 2.322 0 5 1 2.585 0.387 6 2 2.807 0.712 Changing order of documents say \\(D_3\\) and \\(D_4\\) would decrease the score. The performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized. nDCG calculation using the ideal order and decrease sort of relevance score i \\(rel_i\\) \\(log_2(i+1)\\) \\(\\frac {rel_i} {log_2(i+1)}\\) 1 3 1 3 2 3 1.585 1.893 3 2 2 1 4 2 2.322 0.861 5 1 2.585 0.387 6 0 2.807 0 Drawbacks: Normalized DCG metric does not penalize for bad documents in the result, because results with relevance {1, 1, 1} and {1, 1, 1, 0} both given same nDCG while later is clearly the worse of the two. This can be fixed by adjusting values of relevance. If the relevance scores are 1, 0, -1 instead of 2, 1, 0 for Good, Fair, Bad the issue will be resolved nDCG does not penalize missing documents, because results with relevance {1, 1, 1} has same score as {1, 1, 1, 1, 1}. This can be fixed by considering fixed set size and use minimum score for missing documents which would lead to {1, 1, 1, 0, 0} and {1, 1, 1, 1, 1} where later has a higher nDCG. This would be written as nDCG@5 nDCG does not work well for query comparison when there are several equally good results. This affects the metrics when limited to only first few results. For example, for queries such as “restaurants” nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive. REFERENCES: Discounted cumulative gain",
    "tags": "concept machine-learning mathematics",
    "url": "/2017/07/24/discounted-cumulative-gain/"
  },

  
  
  
  {
    "title": "Merge Sort",
    "text": "Merge Sort Merge sort is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945. Time Complexity: \\(O(n\\,log\\,n)\\) Pseudocode Merge(A, p, q, r): n1 = p - q + 1 n2 = r - q L = arr[n1] R = arr[n2] for k = 1 to n1: L[k] = A[p + k - 1] for k = 1 to n2: R[k] = A[q + k] i = 1 j = 1 for k = p to r: if L[i] &lt;= R[j]: A[k] = L[i] i++ else A[k] = R[j] j++ Java Code public class MergeSort { public static int[] merge(int[] arr_l, int[] arr_r) { int[] arr = new int[arr_l.length + arr_r.length]; int k; int l = 0; int r = 0; for(k = 0; k &lt; arr.length; k++) { if(arr_l[l] &lt;= arr_r[r]) { arr[k] = arr_l[l]; l++; } else { arr[k] = arr_r[r]; r++; } if(l==arr_l.length || r==arr_r.length) { k++; break; } } for(int j = l; j &lt; arr_l.length; j++) { arr[k] = arr_l[j]; k++; } for(int j = r; j &lt; arr_r.length; j++) { arr[k] = arr_r[j]; k++; } return arr; } public static int[] merge_sort(int[] arr) { if(arr.length == 1) return arr; int mid = (int) arr.length/2; int left = mid; int[] arr_l = new int[left]; for(int k = 0; k &lt; left; k++) arr_l[k] = arr[k]; int right = arr.length-mid; int[] arr_r = new int[right]; for(int k = 0; k &lt; right; k++) arr_r[k] = arr[mid+k]; arr_l = merge_sort(arr_l); arr_r = merge_sort(arr_r); arr = merge(arr_l, arr_r); return arr; } public static void print_arr(int[] arr) { for(int i: arr) { System.out.printf(\"%d \", i); } System.out.println(\"\\n\"); } public static void main(String[] args) { int[] arr = {1, 9, 8, 7, 6, 10, 5, 9}; MergeSort.print_arr(MergeSort.merge_sort(arr)); } } Java Code for Inplace Merge Sort public class MergeSortInplace { static int[] arr; public static void merge(int start, int mid, int end) { int[] aux_arr = new int[end-start+1]; int k; for(k = 0; k &lt; end-start+1; k++) { aux_arr[k] = arr[start + k]; } k=start; int i = 0; int j = mid-start+1; while(i &lt;= mid-start &amp;&amp; j &lt;= end-start) { if(aux_arr[i] &lt;= aux_arr[j]) { arr[k] = aux_arr[i]; i++; } else { arr[k] = aux_arr[j]; j++; } k++; } while(i&lt;= mid-start) { arr[k] = aux_arr[i]; i++; k++; } while(j&lt;=end-start) { arr[k] = aux_arr[j]; j++; k++; } } public static void merge_sort(int start, int end) { if(start &gt;= end) return; int mid = start + (end-start)/2; merge_sort(start, mid); merge_sort(mid+1, end); merge(start, mid, end); } public static void print_arr(int[] arr) { for(int i: arr) System.out.printf(\"%d \", i); System.out.println(); } public static void main(String[] args) { int[] arr = {1, 3, 2, 5, 4, 7, 6, 10, 5, 8, 9}; MergeSortInplace.arr = arr; merge_sort(0, MergeSortInplace.arr.length-1); print_arr(MergeSortInplace.arr); } } REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 2 Merge Sort - Wikipedia",
    "tags": "algorithms DSA",
    "url": "/2017/07/21/merge-sort/"
  },

  
  
  
  {
    "title": "UMLS Knowledge Sources",
    "text": "UMLS The Unified Medical Language System (UMLS) is a compendium of many controlled vocabularies in the biomedical sciences (created 1986). It provides a mapping structure among these vocabularies and thus allows one to translate among the various terminology systems. It may also be viewed as a comprehensive thesaurus and ontology of biomedical concepts. UMLS further provides facilities for natural language processing. It is intended to be used mainly by developers of systems in medical informatics. UMLS consists of Knowledge Sources (databases) and a set of software tools. UMLS Knowledge Sources Metathesaurus: The Metathesaurus forms the base of the UMLS and comprises over 1 million biomedical concepts and 5 million concept names, all of which stem from the over 100 incorporated controlled vocabularies and classification systems. Some examples of the incorporated controlled vocabularies are ICD-10, MeSH, SNOMED CT, DSM-IV, LOINC, WHO Adverse Drug Reaction Terminology, UK Clinical Terms, RxNorm, Gene Ontology, and OMIM etc. Semantic Network: Each concept in the Metathesaurus is assigned one or more semantic types (categories), which are linked with one another through semantic relationships. The semantic network is a catalog of these semantic types and relationships. This is a rather broad classification; there are 127 semantic types and 54 relationships in total. Specialist Lexicon: The SPECIALIST Lexicon contains information about common English vocabulary, biomedical terms, terms found in MEDLINE and terms found in the UMLS Metathesaurus. Each entry contains syntactic (how words are put together to create meaning), morphological (form and structure) and orthographic (spelling) information. A set of Java programs use the lexicon to work through the variations in biomedical texts by relating words by their parts of speech, which can be helpful in web searches or searches through an electronic medical record. UMLS Metathesaurus to MySQL The setup procedure can be broken down into two steps mainly: Creating the MySQL Script Loading the MySQL Script to SQL Server. Creating MySQL Script (Current Version - 2017AA): Download the Full Release from UMLS Knowledge Sources which would be over 4.5GB in size and requires licence to login and download. Extract umls-2017AA-full.zip which requires over 30GB free space cd 2017AA-full Extract mmsys.zip Copy contents of folder mmsys to parent folder 2017AA-full Go to Terminal and run .\\run_linux.sh or .\\run_mac.command based on platform After MetaMorphosys starts click Install UMLS Select Source : &lt;path_to_folder&gt;/2017AA-full, Destination, and Metathesaurus only from Knowledge Source List. Click New Configuration and Accept Licence Agreement Notice Select Level 0 among options in Default Subset Configuration Input Data Format should be NLM Data File Format in Input Options tab Select Database to MySQL in Write Database Load Script section of Output Options tab Select all ENG souces from Source List after selecting INCLUDE in subset option Click Done in the Action Bar and let the process finish. Shortcut: Can skip the above step by downloading the 2017AA.tar.gz directly Loading From MySQL Script: Go to Destination folder provided in previous section cd 2017AA/META Edit populate_mysql_db.sh to provide details of MYSQL_HOME, user, password and db_name Save and run .\\populate_mysql_db.sh Run tail -f mysql.log to follow the logs. REFERENCES: Unified Medical Language System Unified Medical Language System - Knowledge Sources Downloads",
    "tags": "setup UMLS",
    "url": "/2017/07/19/umls-knowledge-sources/"
  },

  
  
  
  {
    "title": "Divide and Conquer",
    "text": "Recursion The process in which a function calls itself directly or indirectly is called recursion and the corresponding function is called as recursive function. Base Condition: In a recursive problem, solution to the base case is provided and solution to bigger problem is expressed in terms of smaller problems. Stack Overflow: In a recursion if the base condition is not reached and the function call stack reaches its limit, a stack overflow error is thrown. Direct vs Indirect Recursion: A function is direct recursive if it calls the same function, but a indirect recursive function calls a different method which inturn calls the original function. Tail Recursion: If the recursive call is the last thing executed by the function. Disadvantage: Greater space requirements and additional overhead for function calls and return values. Divide and Conquer Approach The divide and conquer paradigm involves three steps: Divide: Divide a problem into a number of smaller subproblems. Conquer: Solve the subproblems recursively. Combine: Combine solution to subproblems to get the solution of original problem. For example merge sort is based on divide and conquer paradigm. It follows the following steps: Divide array of size n to be sorted into two of size n/2. Sort the sub-arrays recursively. Combine the two sorted sub-arrays to get the sorted array. Base case is when the sub-array has only a single element and is trivially sorted. Auxiliary procedure Merge(A, p, q, r) where A is the array and p, q and r are indices into the array such that p &lt;= q &lt;= r. Merge procedure assumes that the sequence A[p .. q] and A[q+1 .. r] are in sorted order and return A[p .. r] in sorted order. Time Complexity of the Merge(A, p, q, r) procedure is O(n). REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 2 Recursion - GeeksforGeeks",
    "tags": "algorithms DSA",
    "url": "/2017/07/18/recursion-and-divide-and-conquer/"
  },

  
  
  
  {
    "title": "Basics of Linguistics",
    "text": "Dimensions of human language: Discrete infinity: A discrete set, a set with finite number of elements which can be used to make infinitely many combinations is called a discrete infinity. For example, the alphabets in english are 26 and form a discrete set but can be combined to form infinitely many sentences and hence is a discrete infinity. Displacement: It is the ability to communicate about time, space or any other abstract notion. For example, conversation about future or about good vs bad. Joint Attention: Human anguages can express shared goals. For example, conversation about voting for a leader. Sign language is also a human language as it serves the same purposes as a spoken language. Aphasia Aphasia is an inability to comprehend and formulate language because of damage to specific brain regions. This damage is typically caused by a cerebral vascular accident (stroke), or head trauma, however these are not the only possible causes. To be diagnosed with aphasia, a person’s speech or language must be significantly impaired in one (or several) of the four communication modalities following acquired brain injury or have significant decline over a short time period (progressive aphasia). The four communication modalities are auditory comprehension, verbal expression, reading and writing, and functional communication. Evolution of Language There are two different types of theories prevailing among linguistic communities, namely Continuity Based Theories: Based on the premise that human language is a complicated form of animal languages i.e. language exhibits so much complexity that one cannot imagine it simply appearing from nothing in its final form; therefore it must have evolved from earlier pre-linguistic systems among our primate ancestors. Discontinuity Based Theories: Based on theory that there is no connection among two but some drastic development occured that led to human langugages i.e. language, as a unique trait which cannot be compared to anything found among non-humans, must have appeared fairly suddenly during the course of human evolution. Ethnologue Ethnologue: Languages of the World is a web-based publication that contains information about the 7,099 living languages in its 20th edition, which was released in 2017. Ethnologue provides information on the number of speakers, location, dialects, linguistic affiliations, autonym of the language, availability of the Bible in each language and dialect described, a cursory description of revitalization efforts where reported, and an estimate of language viability using the Expanded Graded Intergenerational Disruption Scale (EGIDS). Phonetics vs Phonology Phonetics is about the physical aspect of sounds, it studies the production and the perception of sounds, called phones. Phonetics has some subcategories, but if not specified, we usually mean articulatory phonetics: that is, the study of the production of speech sounds by the articulatory and vocal tract by the speaker. Phonology is about the abstract aspect of sounds and it studies the phonemes. Phonology is about establishing what are the phonemes in a given language, i.e. those sounds that can bring a difference in meaning between two words. A phoneme is a phonic segment with a meaning value. International Phonetic Alphabet The International Phonetic Alphabet (IPA) is an alphabetic system of phonetic notation based primarily on the Latin alphabet. It was devised by the International Phonetic Association in the late 19th century as a standardized representation of the sounds of spoken language. Vowels vs Consonants A vowel is a speech sound made with your mouth fairly open, the nucleus of a spoken syllable. A consonant is a sound made with your mouth fairly closed. Dimensions of Consonants Major Dimensions of the consonants are: Place of articulation - Where in the vocal tract the obstruction of the consonant occurs, and which speech organs are involved. Places include bilabial (both lips), alveolar (tongue against the gum ridge), and velar (tongue against soft palate). Manner of articulation - How air escapes from the vocal tract when the consonant or approximant (vowel-like) sound is made. Manners include stops, fricatives, and nasals Voicing or Phonation - How the vocal cords vibrate during the articulation. When the vocal cords vibrate fully, the consonant is called voiced; when they do not vibrate at all, it is voiceless. REFERENCES: Miracles of Human Language Aphasia - Wikipedia Origin of Language Ethnologue What’s the difference between phonetics and phonology? International Phonetic Alphabet The difference between consonants and vowels Consonants",
    "tags": "concept references",
    "url": "/2017/07/14/introduction-to-linguistics/"
  },

  
  
  
  {
    "title": "Pseudocode",
    "text": "Pseudocode Logic of algorithms is often expressed as pseudocode which is similar in many respects to languages like C, C++, Java etc. Major difference lies in that pseudocode uses the most concise and meaningful way of expressing the algorithm which can be sometimes as simple as plain english. Also pseudocode does not get into the issues of software engineering like abstraction, modularity or error handling. Pseudocode for insertion sort can be written as: InsertionSort(A): for i = 1 to A.length-1 key = A[i] j = i - 1 while j &gt; 0 and a[j] &gt; key: A[j+1] = A[j] j-- A[j+1] = key Loop Invariant and Correctness of Algorithm A loop invariant is a condition [among program variables] that is necessarily true immediately before and immediately after each iteration of a loop. (Note that this says nothing about its truth or falsity part way through an iteration.) Loop invariants are used to prove the correctness of an algorithm. There are three properties of loop invariant that must hold for the correctness of algorithm. Initialization: It is true before the first iteration of the loop. Maintenance: If it is true before an iteration of the loop, it remains true before the next iteration. Termination: It is true when the loop terminates. Loop invariant works in a way similar to mathematical induction. So to prove a algorithm works, invariant must work before first iteration (base step), and it must hold between consecutive iterations (inductive step). General Pseudocode Conventions Indentation indicates block stucture similar to python language. General looping constructs used are while, for, if-else etc and loop counter retains its value after loop terminates. Variables are local to the given procedures. Parameters are passed by value. Boolean operators and and or are short-circuiting. REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 2 Algorithm - What is loop invariant?",
    "tags": "algorithms DSA",
    "url": "/2017/07/13/pseudocode/"
  },

  
  
  
  {
    "title": "Insertion Sort",
    "text": "Insertion Sort Given an array A[1…n] of length n to be sorted. The algorithm sorts the list inplace. It picks an element i as key and places it in the correct position in the sorted A[1..(i-1)]. Time Complexity: \\(O(n^2)\\) Space Complexity: \\(O(n)\\) Pseudocode InsertionSort(A): for i = 1 to A.length-1 key = A[i] j = i - 1 while j &gt; 0 and a[j] &gt; key: A[j+1] = A[j] j-- A[j+1] = key Java Code public class InsertionSort { public int[] insertionSort(int[] arr) { int length = arr.length; int key, j, i; for(j = 1; j &lt; length; j++) { key = arr[j]; i = j-1; while(i &gt;= 0 &amp;&amp; arr[i] &gt; key) { arr[i+1] = arr[i]; i--; } arr[i+1] = key; } return arr; } public static void print_arr(int[] arr) { for(int i: arr) { System.out.printf(\"%d \", i); } } public static void main(String[] args) { InsertionSort i = new InsertionSort(); int[] arr = {9, 8, 7, 6, 5}; int[] sorted = i.insertionSort(arr); InsertionSort.print_arr(sorted); } } REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 2",
    "tags": "algorithms DSA",
    "url": "/2017/07/13/insertion-sort/"
  },

  
  
  
  {
    "title": "Algorithms and Data Structures",
    "text": "Algorithm Any well-defined computational procedure that takes some value, or set of values, as input and produces some value, or set of values, as output. So, algorithm is a sequence of computational steps that transform the input into the output. An algorithm is correct if, for every input instance of a problem, it halts with the correct output. Incorrect algorithms can sometimes be useful, if the error rate is controlled. Measures of a good algorithm Time Complexity: Time taken to run. Space Complexity: Auxilliary space needed to run the algorithm. Data Structures A data structure is a way to store and organize data in order to facilitate access and modifications. No single data structure works well for all purposes, and all of them have their strengths and weeknesses. Hard Problems General measure of efficiency is speed for all algorithms. But some problems there are no efficient solutions known. A subset of these problems are also known as NP-complete problems. These problems have the 3 interesting properties: Although no efficient algorithm for an NP-complete problem has been found, its neither been proved that an efficient algorithm for one cannot exist i.e. no one knows whether or not efficient algorithm exist for NP-complete problems. If an efficient algorithm exists for any one of them, then efficient algorithms exist for the rest of them. Several NP-complete problems are similar, but not identical, to problems for which efficient algorithms are known i.e. small changes in problem statements cause a big change in efficiency of the best known algorithm. Travelling Salesman Problem is a classic example of a NP-complete problem encountered in everyday application. REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 1",
    "tags": "algorithms DSA",
    "url": "/2017/07/12/algorithms-in-computing/"
  },

  
  
  
  {
    "title": "Word2Vec",
    "text": "Distributed Vector Representation Series Word2Vec Improvements on Word2Vec · · · Introduction Computing the continuous vector representations of words from very large data sets. Current state-of-the-art performance on semantic and syntactic word similarities. Classical techniques treat words as atomic units without any notion of similarities between them because they are represented using indices in a vocabulary (bag-of-words). Advantages of classical techniques lie in simplicity, robustness and accuracy of simple model when trained on large data sets over complex models trained on less data. Disadvantage of these methods is observed when the amount of data available to train is limited in certain fields like say, automatic speech recognition and machine translations. Previous Works Neural Network Language Model (NNLM): Consists of input, projection, hidden and output layers. Input layer has N previous words encoded using 1-in-V coding, where V is the size of Vocabulary. Projection layer, P has a projection of input layer has a dimensionality of \\(N * D\\) and uses a projection matrix. High complexity between projection and hidden layer due to dimensions of the dense projection layer. Computational complexity of NNLM per training example is given by \\[Q = N * D + N * D * H + H * V\\] Where Q is the computational cost N is the number of previous words used for learning D is the dimensionality of the projection layer H is the size of hidden layer V is the size of the vocabulary and output layer. \\(H * V\\) is the dominating term above which was proposed to be reduced to as less as \\(H * log_2(V)\\) using Hierarchical softmax Avoiding normalized models for training Binary tree representations of the vocabulary using Huffman Trees So, the major complexity is dominated by \\(N * D * H\\) Recurrent Neural Network Language Model (RNNLM): Overcome the limitations of NNLM such as need to specify the context length, N (order of the model N) Theoretically RNNs can efficiently represent more complex patterns than shallow neural networks. No projection layer Consists of Input, hidden and output layers. Develops a short term memory of seen data in the self-fed time delayed hidden layer. Computational complexity of NNLM per training example is given by \\[Q = H * H + H * V\\] Where Q is the computational cost H is the size of hidden layer V is the size of the vocabulary and output layer. Word representations D have the same dimensionality as the hidden layer H. Again, \\(H * V\\) will be reduced to \\(H * log_2(V)\\) using Hierarchical softmax. So, the major complexity is dominated by \\(H * H\\) It’s observed that most complexity is contributed by the non-linearity of the hidden layer in the networks. Continuous Bag-of-Words Model (CBOW) Similar to feedforward NNLM, but the non-linear hidden layer is removed. Projection layer is shared for all the words. So all words are projected into the same position and their vectors are averaged. Model is called bag-of-words model because the order of words in the history or future does not influence the projections. Unlike NNLM, words from future are used to with the best result found with 4 history and 4 future words in context. Training criterion is the correct classification of the current(middle) word. Training complexity is given by \\[Q = N * D + D * log_2(V)\\] Model is continuous bag-of-words because unlike standard bag-of-words it uses continuous distributed representations of the context. Weights between the input and the projection layer is shared for all words positions in the same way as in NNLM. Continuous Skip-Gram Model Similar to CBOW but slight changes in training criterion. Instead of predicting current word from the surrounding words in the window, current word is used to predict the words surrounding the current word. Accuracy and quality of vector is found to increase as the number of context words predicted is increased, but that increased the computational complexity as well. Training complexity is given by \\[Q = C * (D + D * log_2(V))\\] Where C is the maximum distance of the words. Say, C=5 is chosen then a number \\(R \\in [1, C]\\) is selected randomly and then R words from history and R from future are correct labels of the current word. Model Architectures Results Algebraic operations on the vector representations actually give meaningful results like cosine similary of \\(vector(X)\\) is closest to \\(vector(‘smallest’)\\) where \\[vector(X) = vector(‘biggest’) - vector(‘big’) + vector(‘small’)\\] Subtle relationships are learnt when accurate data is used. For example, France is to Paris as Germany is to Berlin. After a certain point adding more dimensionality to the word vectors or adding more training data provides diminishing improvements. NNLM vectors work better than RNNLM because word vectors in RNNLM are directly connected to non-linear hidden layer. CBOW is better than NNLM on syntactic tasks and about the same on semantic tasks. Skip-Gram works slightly worse than CBOW but better than NNLM on syntactic tasks and much better on semantic tasks. Training time for Skip-Gram model is greater than CBOW model. REFERENCES: Efficient Estimation of Word Representations in Vector Space",
    "tags": "NLP machine-learning papers",
    "url": "/2017/07/11/word-to-vector-word-representations/"
  },

  
  
  
  {
    "title": "Building Stopword List for Information Retrieval System",
    "text": "What are stopwords ? Words in a document that are frequently occuring but meaningless in terms of Information Retrieval (IR) are called stopwords. Use of a fixed set of stopwords across various documents of different kinds is not suggested because as the context changes so does the utility of a word. For example, a word like economy might not be a stopword in context of automobiles but would be a stopword in an Economic Times Newspaper. Also the pattern of words changes over time as the trends change, so the list of stopwords used should keep up with the trends in word usages. They are also called noise words or the negative dictionary. Zipf’s law The law states that given some corpus of natural language, the frequency of any word is inversely proportional to its rank in the frequency table i.e. the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc. This law can be seen in action in the Brown Corpus of English text, where the is the most frequently occuring word and accounts for 7% of the word occurences and of is the second most occuring word which is approximately 3.5% of the corpus, followed by and. And only 135 words in the vocabulary account for half the Brown Corpus. The same relationship can be seen in other rankings unrelated to language, such as population rank of cities etc. It is an empirical law formulated using mathematical statistics that states that many types of data in physical and social sciences can be approximated with a Zipfian Distribution (ZD). ZD belongs to a family of discrete power law probabiliy distributions. Observations It can be seen from Zipf’s Law that a relatively small number of words account for a very significant fraction of all text’s size. These terms make very poor index terms because of their low discriminative value. Kullback–Leibler Divergence It is the measure of how on probability distribution diverges from a second expected probability distribution. Applications lie in finding the relative (Shannon) Entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. In summary it can help find the amount of information a word provides in a corpus. And the lesser the information a word has the more likely it is to be a stopword. Classical Methods Zipf’s Law can be mathematically represented by \\[F(r) = \\frac{C}{r_\\alpha}\\] Where \\(\\alpha \\approx 1\\) \\(C \\approx 0.1\\) \\(r\\) is the rank frequency Four different classical methods exist by replacing the term frequency \\(r\\) above with one of the four refinements given below. Term Frequency (TF): The number of times a term occurs in a specific collection. Normalized TF: Generated by normalizing the term frequency by the total number of tokens in the collection i.e. the size of the lexicon file given by Where TF is the term frequency \\(v\\) is total number of tokens in the lexicon file Inverse Document Frequency (IDF): Calculated using the TF distribution where IDF of term k is given by \\[idf_k = log (\\frac{N_{Doc}}{D_k})\\] Where \\(N_{Doc}\\) is the total number of documents in the corpus \\(D_k\\) is the number of documents containing term k So infrequently occuring terms have a greater probability of occuring in relevant documents and hence are more informative. Normalized IDF: Many normalizing techniques are used in this case but one of the most frequently used ones is given by Robertson and Sparck-Jones which normalizes with respect to number of documents not containing the term and adds a constant to both numerator and denominator to moderate extreme values. Where \\(N_{Doc}\\) is the total number of documents in the corpus \\(D_k\\) is the number of documents containing term k A threshold value is to be determined to produce the best average precision. It cannont be chosen at random. It is needed to check the difference between the frequency of consecutive ranks say \\(F(r)\\) and \\(F(r+1)\\) because if the difference is big enough threshold can be set as \\(frequency \\geq F(r)\\) for choosing the stopwords. # imports import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer # documents is the list of documents in the collection vocab = list(set([i for sub_doc in documents for i in sub_doc.strip().split()])) # TfidfVectorizer from sklearn vectorizer = TfidfVectorizer( stop_words=None, norm=None, min_df=0, sublinear_tf=True, token_pattern=r'(?u)\\b\\w+\\b', vocabulary=vocab ) # calculate tf-idf def tf_idf(documents): return vectorizer.fit_transform(documents) # create dataframe from pandas tf_idf = pd.DataFrame({'word': vectorizer.vocabulary, 'idf': vectorizer.idf_}, index=None) tf_idf.sort_values('idf') def get_word_idf(word): return tf_idf.loc[tf_idf.word==word] Term Based Random Sampling Approach Based on how informative a particular term is. Importance of term is determined using Kullback–Leibler divergence measure. Approach is similar to idea of query expansion in which a query is expanded based on a particular query term. The idea is to find terms that complement the initially chosen query terms which follows from the idea that a individual term might be inadequate to express a concept accurately. It differs from the standard approach in that instead of finding the similar terms, find all the documents containing the current term and use them as the new sample and then extract the least informative term from the sample by measuring divergence of a given term distribution within the sampled document set from its distribution in the collection. After this Kullback–Leibler divergence can be used to measure the importance of each term. Weight of a term t in the sampled document set is given by Where \\(P_x = \\frac{tf_x}{l_x}\\) \\(P_c = \\frac{F}{token_c}\\) \\(tf_x\\) is the frequency of the query term in the sampled documents \\(l_x\\) is the sum of the length of the sampled document set F is the term frequency of the query term in the collection \\(token_c\\) is the total number of tokens in the whole collection Issues and Solutions Selecting a random term has the possibility of finding only one document containing that term which would result in a relatively small sample. This problem can be solved by repeating selection step Y times which would theoretically result in a better sample, creating a better view of term distribution and their importance. Advantages: Easier to implement than the classical methods even though algorithm looks complex. Because all steps here are automatic and do not require manual interventions like in the classical techniques for choosing proper threshold. Classical techniques need to check \\(F(r)-F(r+1)\\) listing one by one and tf-idf graph is needed for zipf’s law. # imports import pandas as pd from collections import Counter, defaultdict import re from math import log # documents is the list of documents in the collection # Collection Analysis tokens = Counter(re.findall(r'\\w+', \" \".join(documents))) def F(word): return tokens.get(word) TOKEN_C = len(tokens) def P_c(word): return float(F(word))/TOKEN_C # creating inverted index def create_index(data): index = defaultdict(list) for i, document in enumerate(data): for token in document.strip().split(): index[token].append(i) return index inv_index = create_index(documents) # sample analysis def P_x(word): sample = [documents[i] for i in inv_index[word]] tokens_sample = Counter(re.findall(r'\\w+', ' '.join(sample))) L_x = 0 for k, v in tokens_sample.items(): L_x += v return float(tokens_sample[word])/L_x # kullback leibler divergence def kl_div(word): p_x = P_x(word) p_c = P_c(word) return p_x * log(p_x/p_c, 2) # collection analysis if the dataset is not huge terms = list(tokens.keys()) kl_div_val = [] for t in terms: kl_div_val.append(kl_div(t)) df_kl_div = pd.DataFrame({'term': terms, 'kl_div': kl_div_val}) df_kl_div.sort_values('kl_div') def get_word_kl_metric(word): return df_kl_div.loc[df_kl_div.term == word] REFERENCES: Automatically Building a Stopword List for an Information Retrieval System Zipf’s law Kullback–Leibler Divergence",
    "tags": "NLP machine-learning papers",
    "url": "/2017/07/04/building-stopword-list-for-information-retrieval-system/"
  },

  
  
  
  {
    "title": "Zero Sum Subarrays",
    "text": "Q: Given an array find all sub arrays that have sum 0. Input: a = [0, 1, -1, 4, 2, -3, -1, 0, 4] Output: [0] [0, 1, -1] [1, -1] [0] [-3, -1, 0, 4] Algorithms: Logic: Hash sums and ending indices. If a sum is seen previously then the elements in between sum to 0. Complexity: Time: O(n) Space: O(n) a = [0, 1, -1, 4, 2, -3, -1, 0, 4] sum_idx = {} s = 0 for i in range(len(a)): s = s + a[i] l = sum_idx.get(s, []) if s == 0: [print(a[: i+1])] [print(a[j+1: i+1]) for j in l] l.append(i) sum_idx[s] = l REFERENCES: 500 Data Structures and Algorithms practice problems and their solutions Find sub-array with 0 sum",
    "tags": "array-algorithms references algorithms",
    "url": "/2017/07/03/zero-sum-subarray/"
  },

  
  
  
  {
    "title": "Sort Binary Array",
    "text": "Q: Sort a given binary array. Input: a = [1, 0, 1, 0, 1, 0, 0, 1] Output: [0, 0, 0, 0, 1, 1, 1, 1] Algorithms: Logic: Iterate and fill available position with 0 if 0 found in list. Fill remaining positions with 1s. Complexity: Time: O(n) Space: O(1) a = [1, 0, 1, 0, 1, 0, 0, 1] k = 0 for i in a: if i==0: a[k] = 0 k += 1 for i in range(k, len(a)): a[i] = 1 Quicksort Logic: Complexity: Time: O(n) Space: O(1) a = [1, 0, 1, 0, 1, 0, 0, 1] pivot = 1 start = 0 end = len(a) - 1 j = 0 while start &lt; end: if a[j] &lt; pivot: a[j], a[start] = a[start], a[j] start += 1 j += 1 elif a[j] &gt;= pivot: a[j], a[end] = a[end], a[j] end -= 1 REFERENCES: 500 Data Structures and Algorithms practice problems and their solutions Sort binary array in linear time",
    "tags": "array-algorithms references algorithms",
    "url": "/2017/07/03/sort-binary-array/"
  },

  
  
  
  {
    "title": "Pair Sums In An Array",
    "text": "Q: Given an array find a pair of element with the given sum. Input: a = [8, 7, 2, 5, 3, 1] s = 10 Output: (8, 2) (7, 3) Algorithms: Hashing: Complexity: Time: O(n) Space: O(n) a = [8, 7, 2, 5, 3, 1] s = 10 h = {} for idx, elem in enumerate(a): h[elem] = idx diff = h.get(s-elem, None) if diff != None and diff != idx: print((elem, s-elem)) Sorting: Complexity: Time: O(nlogn) Space: O(1) a = [8, 7, 2, 5, 3, 1] s = 10 a.sort() l = 0 h = len(a) - 1 while (l&lt;h): temp = a[l] + a[h] if temp == s: print((a[l], a[h])) l = l+1 h = h-1 elif temp &gt; s: h = h-1 else: l = l+1 REFERENCES: 500 Data Structures and Algorithms practice problems and their solutions Find pair with given sum in the array",
    "tags": "array-algorithms references algorithms",
    "url": "/2017/07/03/pair-sums-in-an-array/"
  },

  
  
  
  {
    "title": "Duplicate In Limited Range Array (XOR)",
    "text": "Q: Find duplicate in limited range (1 to n-1) array of size n using XOR operation. Input: a = [1, 2, 3, 4, 4] Output: 4 Algorithms: Logic: XOR all the numbers XOR with all the numbers between 1 to n-1. a ^ a = 0 0 ^ 0 = 0 a ^ 0 = a Complexity: Time: O(n) Space: O(1) a = [1, 2, 3, 4, 4] xor = 0 for elem in a: xor = xor ^ elem for i in range(len(a)): xor = xor ^ i print(xor) REFERENCES: 500 Data Structures and Algorithms practice problems and their solutions Find a duplicate element in a limited range array",
    "tags": "array-algorithms references algorithms",
    "url": "/2017/07/03/duplicate-in-limited-range-array-using-xor/"
  },

  
  
  
  {
    "title": "Blockchain",
    "text": "Cryptocurrency Series Blockchain Bitcoin B-Money · · · What is blockchain ? Originally developed as a part of digital currency Bitcoin. Blockchain can support a wide variety of applications such as peer-to-peer payment services, supply chain tracking etc. Digital Record: A blockchain is a record of transactions like a traditional ledger, where a transaction can be any movement of money, goods or data. Secure: It stores data in a way that it is virtually impossible to tamper the data without being detected by other users. Decentralized: Blockchain uses decentralized verification systems that uses consensus of multiple users instead of traditional centralized ones regulated by a verified authority like a government or a credit card clearinghouse. How does it work ? Blockchain Steps: First, gather and order data into blocks. Second, chain them together securely using cryptography. Recording a Transaction: Say A sells car to B. Transaction information is recorded and shared with other systems on the blockchain network. Building transactions into Blocks: On the network, the record is combined with other transactions to form a block and each transaction is time stamped. Upon completion, a block also gets a time stamp. All the data is sequential which helps to avoid duplicate records. Connecting blocks into Chains: Completed block is sent out across the network and appended to the chain. Other participants may also be sending out their blocks but the time stamp ensures the correct order of blocks and participants have the latest versions. Securing the chain: Security is maintained using a hash and the cryptographic math makes the links between blocks made using these hashes virtually unbreakable. A hash function takes the information in the block to create the hash which is a unique string of characters easy to generate but almost impossible to back trace to original data. Locking it down: The hash from one block is added to the data in the next block. So, when the next block goes through the hash function a trace of hash from previous block is woven into the new hash. The same is repeated further down the chain. Raising the Alarm: So if there is any tampering with a previously created block the hash encoded in the next block will not match up anymore. The mismatch will cascade through all subsequent blocks denoting an alteration in the chain. Establishing Trust: Since all the participants have a copy of the block chain they can detect any data tampering. If hashes match up across the chain, all parties know they can trust their records. Blockchain in Action (Examples) Enormous potential. Because they establish trust, they provide simple, paperless way to establish ownership of money, information and objects - like concert tickets. Trusted Concert Tickets: Trusted Seller: It’s hard to tell a real ticket from a counterfeit, especially if bought through a third-party website or a private individual. Going Straight to the Source: Blockchain can help buyers quickly establish that a ticket (and its seller) can be trusted. The event venue will register all tickets to a blockchain which would be accessible online. When a ticket is sold it will be assigned an address - a string of data publically viewable on the blockchain. Owner is given a private key which is a hash of the address data. The key can be used to unlock the address. So by producing the correct key the buyer can prove that the item is theirs without checking with the event venue. If they choose to sell it, it is assigned a new address, and new owner gets a new private key and the transaction is added to the blockchain. The ticket can be sold multiple number of times and when a seller unlocks the ticket with their private key, the buyer knows that the ticket they are getting is authentic. More Efficient Markets: Removing the Bottlenecks: In the financial markets the trade happens in a fraction of second, but the actual exchanging of assets and payments can take days, involving multiple banks and clearinghouses which can cause errors, delays and other unnecessary risks. Smart Contract: A piece of computer code that describes a transaction step by step. It can connect to multiple blockchains, tracking multiple assets so it can swap those assets as needed to execute transactions. A broker only needs to buy stock on behalf of a client. The order will be placed with the private keys of both the buyer and seller. This will trigger the execution of a smart contract. It connects to multiple blockchains, verifies the availability of the stock and the payment and then makes the transfers between the seller and the buyer. Digital ID: Digitally issued IDs via a blockchain would be more secure mechanism than the traditional ones issued by governments. Internation ID Blockchain, accessible anywhere in the world, allows people to prove identity, connect with family member or receive money without a bank account. A person is a fingerprint. Fingerprint is digitized and added to blockchain with other data like name etc. To prove identity they need to give their fingerprint which can be used to unlock and verify their ID. Improvements Needed: Early stage of the technology. Has various hurdles to overcome: Departure from manual work for businesses would add new costs and new risks which leads to reluctance to adopt the new tech. Current blockchain technologies like bitcoin can support only 5-8 transactions per second which cannot keep up with applications like credit card transactions which amount to be around 10000 times what is supported. Even though its transparent in ledgering there are no real standardization of implementation, which is required for relaibility and other legal issues. Even though it uses business grade cryptography, it is not 100% secure. Large sums of money transaction therefore would be reluctant to adopt this technology. REFERENCES: Blockchain by Goldman Sachs",
    "tags": "concept references",
    "url": "/2017/07/02/blockchain/"
  },

  
  
  
  {
    "title": "Discrete Random Variables",
    "text": "Probability Series Basic Probability Concepts Conditional Probability and Bayes’ Rule Discrete Random Variables Continuous Random Variables · · · Discrete Random Variables Even though it is named variable, discrete random variable is actually a function that maps the sample space to a set of discrete real values. Where X is the random variable S is the sample space R is the set of real numbers Probability Mass Function Probability mass function is the probability defined over a given random variable, i.e., it gives the probability that a discrete random variable is exactly equal to some value in the sample space, S. For a random variable X, \\(p(X=c)\\) is denoted as \\(p(c)\\) and the mapping of each value in sample space to their respective probabilities is known as pmf. For all values c that are not is sample space \\(p(c) = 0\\) because it is pointing to an empty set. Commutative Distribution Function Probability defined over an inequality such as \\(X \\leq c\\) gives probabilities of all the events that satisfy the condition from \\(-\\infty\\) to c i.e. the probability value estimate for X less than or equal to c. Mathematically, Explaination Set has 5 boys and 5 girls. 3 kids selected at random but gender not known. X is the random variable that denotes number of girls selected. Event c such that \\(X(c) = 3\\) is given by set \\(c = \\{(GGG)\\}\\) i.e. all three girls selected are girls. The pmf for random variable X will be as follows: a p(a) CD(a) 0 1/12 1/12 1 5/12 6/12 2 5/12 11/12 3 1/12 1 Calculations: Number of ways of selecting k kids from total of N kids is given by \\(C_k^N = \\frac{N!}{k! (N-k)!}\\) implies \\(C_3^{10} = \\frac{10!}{3! (10-3)!} = 120\\) for N = 10 and k = 3. value 1 : a = 0 means that no girl is selected which implies that k boys are selected out of the total B boys. The number of ways this can be accomplished is given by \\(C_k^B = \\frac{B!}{k! (B-k)!}\\) implies \\(C_3^5 = \\frac{5!}{3! (5-3)!} = 10\\) for B = 5 and k = 3. For commutative distribution \\(CD(2) = p(0) + p(1) + p(2) = 11/12\\). Similarly the value of \\(p(a)\\) and \\(CD(a)\\) at other values of \\(a\\) can be calculated. Random Variable Metrics Expected Value The average or mean value calculated over all the possible outcomes of the random variable. \\[E(X) = \\sum_n v_i * p(v_i) \\tag{3}\\] X is the random variable. \\(v_i\\) is the value the random variable takes with probability \\(p(v_i)\\). sample space size is n. often represented by \\(\\mu\\). it is a measure of central tendency of the random variable. Some other properties of expected value of a random variable: Variance and Standard Deviation Variance gives the dispersion of probability mass around the mean value (i.e. E(X), expected value) of the random variable. \\[Var(X) = E((X-\\mu)^2) \\tag{6}\\] \\[\\sigma = \\sqrt (Var(X)) \\tag{7}\\] \\(\\sigma\\) is the standard deviation. Some other properties of variance of a random variable: \\[Var(X) = E(X^2) - (E(X))^2 \\label{8} \\tag{8}\\] \\[Var(aX + b) = a^2 Var(X) \\label{9} \\tag{9}\\] \\[Var(X+Y) = Var(X) + Var(Y) \\text{ iff X and Y are independent } \\tag{10}\\] Derivation of equation \\eqref{8} Derivation of equation \\eqref{9} using equations \\eqref{5}, \\eqref{8} Some Specific Distributions Uniform Distribution All outcomes are eqully possible. Eg: Probability of getting a heads or tails for a fair coin. Uniform(N) implies N outcomes and each has probability 1/N. Bernoulli Distribution Used to model the random experiment where each trail has exactly 2 possible outcomes. One possible outcome is termed as success and other as failure. Parameter \\(p\\) is the probability of success in an experiment. Random variable \\(X \\in \\{0, 1\\}\\). X = 1 has probability \\(p\\) and X = 0 has probability \\(1-p\\) where 0 is denoting failure and 1 denoting success. Binomial Distribution Used to model \\(n\\) independent traits of Bernoulli Distribution. If X follows Binomial(n, p) then, X = k implies, the event of k successes in n independent Bernoulli trials. \\[P(X=k) = C_k^n * (p^k) * ((1-p)^{n-k})\\] \\(C_k^n\\) is the number of ways of picking k success events in n total trials. \\((p^k) * ((1-p)^{n-k})\\) gives the combined probability of the n Bernoulli trails. Geometric Distribution Also defined over Bernoulli distribution. Models the event of k failures before first success. \\(geometric(p)\\) is given by \\[P(X=k) = (1-p)^k * p\\] where \\(X = k\\) is the event where first success occured after k failures. If X and Y follows geometric distribution with same probability p, then \\(X + Y\\) is also a geometric distribution. Expected Value and Variance for Distributions Distribution E(X) Var(X) Uniform \\(\\frac{n+1}{2}\\) \\(\\frac{n^2-1}{12}\\) Bernoulli \\(p\\) \\(p(1-p)\\) Binomial \\(np\\) \\(np(1-p)\\) Geometric \\(\\frac{1-p}{p}\\) \\(\\frac{1-p}{p^2}\\) REFERENCES: Discrete Random Variables Expectation and Variance Variance",
    "tags": "mathematics probability",
    "url": "/2017/06/23/discrete-random-variables/"
  },

  
  
  
  {
    "title": "Conditional Probability and Bayes’ Rule",
    "text": "Probability Series Basic Probability Concepts Conditional Probability and Bayes’ Rule Discrete Random Variables Continuous Random Variables · · · Conditional Probability The probability of event X given that Y has already occurred is denoted by \\(P(X|Y)\\) If X and Y are independent: \\(P(X|Y) = P(X)\\) because event X is not dependent on event Y. If X and Y are mutually exclusive: \\(P(X|Y) = 0\\) because X and Y are disjoint events. Product Rule From \\eqref{1}, following can be concluded, \\(X \\subseteq Y\\) implies \\(P(X|Y) = P(X)/P(Y)\\) because \\(X \\cap Y = X\\) \\(Y \\subseteq X\\) implies \\(P(X|Y) = 1\\) because \\(X \\cap Y = Y\\) The distributive, associative and De Morgan’s laws are valid for conditional probability. \\[P(X \\cup Y|Z) = P(X|Z) + P(Y|Z) - P(X \\cap Y|Z)\\] \\[P(X^{c}|Z) = 1-P(X|Z)\\] Chain Rule Bayes’ Theorem Where \\(P(X) = P(X \\cap Y) + P(X \\cap Y^{c})\\) from the sum rule. Derivation of Bayes’ Theorem From \\eqref{1}, Using the commutative law, From \\eqref{2}, \\eqref{3} and, \\eqref{4}, Hence, EXAMPLE \\(p_ot\\) is probability of reaching on time when no car trouble. \\(p_ct\\) is probability of car trouble. Commute by train if car trouble occurs. N is the number of trains available. Only 2 of the N trains would reach on time. What is the probability of reaching on time. Explaination: \\(O\\): reach on time \\(C^c\\): car not working \\(P(O) = P(O \\cap C) + P(O \\cap C^c)\\) \\(P(O \\cap C) = P(O|C) * P(C) \\text{ where } P(C) = 1 - P(C^c)\\) \\(P(O \\cap C^c) = P(O|C^c) * P(C^c) \\text{ where } P(O|C^c) = 2/N\\) p_ct = float(input()) # P(Car Trouble) p_ot = float(input()) # P(On Time | No Car Trouble) N = float(input()) # Number of trains p_rt = 2.0/N # P(Correct Train) p_o = p_ct * p_rt + (1-p_ct) * p_ot # P(On Time) print(\"%.6f\" % p_o) REFERENCES: Bayes’ rules, Conditional probability, Chain rule",
    "tags": "mathematics probability",
    "url": "/2017/06/22/bayes-rule-conditional-probability-chain-rule/"
  },

  
  
  
  {
    "title": "The One With Github Pages",
    "text": "GitHub Pages is a web hosting service offered by GitHub for hosting static web pages for GitHub users, user blogs, project documentation, or even whole books. It is integrated with the Jekyll software for static web site and blog generation. The Jekyll source pages for a web site can be stored on GitHub as a Git repository, and when the repository is updated, github servers will automatically regenerate the site. GitHub Pages was launched in late 2008. As with the rest of GitHub, it includes both free and paid tiers of service, instead of being supported by web advertising. Web sites generated through this service are hosted either as subdomains of the github.io domain, or as custom domains bought through a third-party domain name registrars. Steps for setup: Fork this repository. Update repository name to &lt;user_name&gt;.github.io. Customize _config.yml to match user credentials. In _includes/comments.html edit disqus_shortname. Update index.html bio. Commit the changes. Done! This site is build on top of the customizations by psteadman on lanyon theme which derives from poole. Head over to the post Using Jekyll, Poole and Lanyon to setup my github user page for further references. Additions: Added MathJax for adding equations. Added Facebook Sidebar Icon using font-awesome. Added Search bar to search through the posts using Tipue Search. Added Social sharing buttons for posts sharing on platforms like Reddit, Facebook etc. Added Typed.js developed by Matt Boldt for text typing using javascript. Added Travis CI Build and Deployment as it helps debug issues in deployment easier. REFERENCES: Github Pages - Wikipedia Using Jekyll, Poole and Lanyon to setup my github user page",
    "tags": "setup github-pages jekyll",
    "url": "/2017/06/16/initial-site-setup/"
  },

  
  
  
  {
    "title": "Basic Probability Concepts",
    "text": "Probability Series Basic Probability Concepts Conditional Probability and Bayes’ Rule Discrete Random Variables Continuous Random Variables · · · Introduction Trail or Experiment - The act that leads to a result with certain possibility. Sample Space - Set of all possible outcomes of an experiment. Event - Non empty subset of a sample space. Basic Probability Formula \\[P(A) = \\sum_{i=1}^n P(E_i) \\label{1} \\tag{1} \\] Where A is an event, S is the sample space, \\(E_1 … E_n\\) are the n outcomes in A. If \\(E_1 … E_n\\) are equally likely to occur, then \\eqref{1} can be written as, \\[P(A) = {\\text{number of outcomes in A} \\over \\text{total number of possible outcomes}} \\label{2} \\tag{2}\\] From \\eqref{2}, following results can be inferred, \\(0 \\leq P(A) \\leq 1\\), \\(P(S) = 1\\) Complement of an Event Compliment of an event A is defined as all the outcomes of the sample space, S that are not in A, i.e. \\[P(A^c) = 1 - P(A) \\tag{3} \\] Where \\(A^c\\) is used to denote the compliment of A. Union and Intersection of Events \\[P(A \\cup B) = P(A) + P(B) + P(A \\cap B) \\label{4} \\tag{4}\\] Mutually Exclusive Events Two events A and B are mutually exclusive if there are no overlapping outcomes, i.e., the intersection of the two experiments is a null set. \\[P(A \\cap B) = 0 \\label{5} \\tag{5}\\] Using \\eqref{4} and \\eqref{5}, Independent Events Two events A and B are independent if occurence of one does not affect the probability of the other occuring and is mathematically given by, \\[P(A \\cap B) = P(A) * P(B)\\] Sum Rule or Marginal Probability \\[P(A) = \\sum_{B} P(\\text{A and B})\\] EXAMPLE M wants to go fishing this weekend to nearby lake. His neighbour A is also planing to go to the same spot for fishing this weekend. The probability that it will rain this weekend is \\(p_1\\). There are two possible ways to reach the fishing spot (bus or train). The probability that M will take the bus is \\(p_{mb}\\) A will take the bus is \\(p_{ab}\\). Travel plans of both are independent of each other and rain. What is the probability \\(p_{rs}\\) that M and A meet each other only (should not meet in bus or train) on a lake in rain ? p_mb = float(input()) # P(M taking Bus) p_ab = float(input()) # P(A taking Bus) p_1 = float(input()) # P(Rain) p_rs = p_1 * (1 - p_ab*p_mb - (1-p_ab)*(1-p_mb)) # P(Meet at lake only) print(\"%.6f\" % p_rs) REFERENCES: Basic Probability Models and Rules",
    "tags": "mathematics probability",
    "url": "/2017/06/16/basic-probability-models-and-rules/"
  }

]};
