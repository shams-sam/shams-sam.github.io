







var tipuesearch = {"pages": [

  
  
  
  {
    "title": "Mathematical Induction",
    "text": "What is Mathematics Series What is Number System ? Mathematical Induction · · · Why Mathematical Induction? There are infinitely many integers. The step-by-step procedure of passing from \\(n\\) to \\(n+1\\) which generates the infinite sequence of integers forms the basis of one of the most fundamental patterns of mathematical reasoning, the principle of mathematical induction. Mathematical Induction is used to establish the truth of a mathematical theorem for an infinite sequence of cases. There are two steps in proving a theorem, \\(A\\) by mathematical induction: The first statement \\(A_1\\) must be true. If a statement \\(A_r\\) is true then the statement \\(A_{r+1}\\) should be true too. That these two conditions are sufficient to establish the truth of all the statements, \\(A_1, A_2, \\cdots \\) is a logical principle which is as fundamental to mathematics as are the classical rules of Aristotelian logic. Suppose that a) by some mathematical argument it is shown that if \\(r\\) is any integer and if assertion \\(A_r\\) is known to be true then the truth of assertion \\(A_{r+1}\\) will follow, and that b) the first proposition \\(A_1\\) is known to be true. Then all the propositions of the sequence must be true, and A is proved. The principle of mathematical induction rests on the fact that after any integer \\(r\\) there is a next, \\(r+1\\), and that any desired integer \\(n\\) may be reached by a finite number of such steps, starting from the integer 1. Although the principle of mathematical induction suffices to prove a theorem or formula once it is expressed, the proof gives no indication of how this formula was arrived at in the first place. So, it should be more fittingly called a verification. Proof for Arithmetic Progression For every value of n, the sum \\(1 + 2 + \\cdots + n\\) of the first n integers is equal to \\(\\frac {n(n+1)} {2} \\) For any \\(r\\) is given by assertion \\(A_r\\) is given by, Adding \\((r+1)\\) to both LHS and RHS of \\eqref{1}, \\eqref{2} is nothing but assertion, \\(A_{r+1}\\). Also for \\(r=1\\), assertion \\(A_1\\) is true because, from \\eqref{1}, So, using \\eqref{2} and \\eqref{3}, mathematical induction proves that the assertion in \\eqref{1} holds for all positive integers, \\(n\\). Proof for Geometric Progression The theorem states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, Let’s assume \\(G_r\\) is true, i.e. Then, But \\eqref{7} is precisely the assertion \\eqref{4} for the case \\(n=r+1\\). Hence, using \\eqref{5} and \\eqref{7} the assertion \\eqref{4} is proved by mathematical induction. Proof for Sum of First n Squares The theorem states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, Let’s assume \\(A_r\\) is true, i.e., Then, which is precisely the assertion \\eqref{8} for \\(n = r+1\\). So, using \\eqref{9} and \\eqref{11}, mathematical induction proves the assertion. Proof for Sum of First n Cubes The theorem states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, Let’s assume \\(A_r\\) is true, i.e., Then, which is precisely the assertion \\eqref{12} for \\(n = r+1\\). So, using \\eqref{13} and \\eqref{15}, mathematical induction proves the assertion. Proof for Bernoulli’s Inequality The assertion, \\(A_n \\) states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, Let’s assume \\(A_r\\) is true, i.e., Then, which is precisely the assertion \\eqref{16} for \\(n = r+1\\). So, using \\eqref{17} and \\eqref{19}, mathematical induction proves the assertion. If \\(p \\lt -1\\), then \\((1+p)\\) is negative and the inequality in \\eqref{19} would be reversed. So, the restriction introduced in \\eqref{16} is essential. Proof of Binomial Theorem The assertion, \\(C_i^n \\) states that, for every value of \\(n\\), The assertion holds true for \\(n=1\\), because, which is exactly the value for \\(C_0^1 = C_1^1 = 1\\) in Pascal’s Triangle. Let’s assume \\(C_i^r\\) is true, i.e., Then using the relation, \\(C_i^{r+1} = C_{i-1}^{r} + C_i^{r} \\text{, } C_i^{r+1}\\) is given by, which is precisely the assertion \\eqref{20} for \\(n = r+1\\). So, using \\eqref{21} and \\eqref{23}, mathematical induction proves the assertion. Generalized Mathematical Induction If a sequence of statements \\(A_s, A_{s+1}, \\cdots\\) is given where \\(s\\) is a some positive integer, and if a) For every value \\(r \\geq s\\), the truth of \\(A_{r+1}\\) will follow from the truth of \\(A_r\\), b) \\(A_s\\) is known to be true, then all the statements \\(A_s, A_{s+1}, \\cdots\\) are true; i.e. \\(A_n\\) is true for all \\(n \\geq s\\) Proof for Bernoulli’s Inequality (Strict version) The assertion, \\(A_n \\) states that, for every value of \\(n\\), The assertion holds true for \\(n=s=2\\), because, Let’s assume \\(A_r\\) is true, i.e., Then, which is precisely the assertion \\eqref{24} for \\(n = r+1\\). So, using \\eqref{25} and \\eqref{27}, mathematical induction proves the assertion. If \\(p \\lt -1\\), then \\((1+p)\\) is negative and the inequality in \\eqref{19} would be reversed. So, the restriction introduced in \\eqref{16} is essential. Principle of Smallest Integer Every non-empty set \\(C\\) of positive integers has a smallest number. (Set may be finite or infinite.) At first it seems like a trivial principle but it actually does not apply to many sets that are not integers, e.g. the set of positive fractions \\(1, {1 \\over 2} {1 \\over 3} \\cdots \\) does not contain a smallest number. Proof for Mathematical Induction The principle of smallest integer can be used to prove the principle of mathematical induction as a theorem. Let us consider any sequence of statements \\(A_1, A_2, \\cdots \\) such that, For any integer \\(r\\) the truth of \\(A_{r+1}\\) will follow from that of \\(A_r\\) \\(A_1\\) is known to be true. if 1 of the \\(A’s\\) were false, the set \\(C\\) of all positive integers \\(n\\) for which \\(A_n\\) is false would be non-empty. By the principle of smallest integer, \\(C\\) would have a smallest integer \\(p\\), which must be greater than 1 because of condition (2) in the theorem. Hence, \\(A_p\\) would be false, but \\(A_{p-1}\\) true, which contradicts condition (1) of the theorem. So, the assumption in the first place, that any one of the \\(A’s\\) is false is untenable. Mathematical Induction must be applied very carefully to prove an assertion because, it is often fallacious because of a misleading base case. A popular example of this is “All number are equal fallacy”. REFERENCES: What is Mathematics? Second Edition - Chapter I: Natural Numbers",
    "tags": "mathematics number system what is mathematics theorems",
    "url": "/2017/10/28/mathematical-induction/"
  },

  
  
  
  {
    "title": "What is Number System ?",
    "text": "What is Mathematics Series What is Number System ? Mathematical Induction · · · The Natural Numbers The natural numbers were created by the human mind out of the necessity to count the objects around us. Most basics of mathematics are associated with association of numbers with tangible objects. But advanced mathematics is built on top of the abstract concept of number system. God created natural numbers; everything else is man’s handiwork. Laws of Arithmetic Natural numbers have only two basic operations, namely, addition and multiplication. The mathematical theory of the natural numbers or positive integers is known as arithmetic. Arithmetic is based on the fact that operations on numbers are governed by certain laws. (a, b, c … symbolically denote integers) The fundamental laws of arithmetic are: Commutative law, i.e. \\(a+b = b+a\\) and \\(ab = ba\\) Associative law, i.e. \\(a + (b + c) = (a + b) + c\\) and \\(a(bc) = (ab)c\\) Distributive law, i.e. \\(a(b+c) = ab + ac\\) Addition and subtraction are inverse operations because, \\((a+d)-d = a\\). Similarly, multiplication and division are inverse because \\({a \\over d} \\cdot d = a\\). Also, 0 and 1 are the identities of operations addition and multiplication respectively. Representation of Integers A number system has a set of digit symbols and numbers where digit symbols are used to denote the larger numbers not available directly in the set of digit symbols. Modern number systems are associated with the place values of individual digits in the number, such as, These are called positional notations. Here, a large number can also be represented using the basic symbols from the set of digit symbols. It is useful to have a way of indicating the result in a general form using a uniform logic, i.e. a general method for representing an integer, \\(z\\) in the decimal system is to express it as follows which would be represented by the symbol \\(a_na_{n-1}\\cdots a_1a_0\\) Specifically, in case of decimal system the base is 10 as can be seen in the equations above. But, in general, for a number system any number greater than 1 can serve as the base of the system. E.g. a septimal system has base 7 and an integer is expressed as, and it would be represented by the symbol \\(b_nb_{n-1}\\cdots b_1b_0\\) As a result 109 in decimal system is represented by 214 in septimal system for the reason shown below As a general rule, in order to convert from base 10 to base B, perform successive divisions of number z by B; the remainders in reverse order would be the number representation in the system with base B. Too small a base base has disadvantages (even a small number like 72 has a lengthy representation 1,001,111 in the dyadic system i.e. the binary system), while a large base requires learning many digit symbols, and an extended multiplication table. The duodecimal system, i.e., choice of base 12 is advocated in many places, because it is exactly divisible by two, three, four, and six. Due to this property, works involving division and fraction are simplified. Early systems of numerations were not positional in nature, but were based on purely additive principle. E.g. in roman symbolism, i.e., the symbol \\(CXVIII\\) represents 118. Same was true about other early number systems like the Egyptian, Hebrew, Greek systems of numeration. Disadvantage of any purely additive notation is that it requires more and more number of new symbols as the number to be represented gets larger. Also, the computation with additive systems is so difficult to perform, that computation was confined to a few adepts in the ancient times. The positional system has the property that all numbers, however large or small, can be represented by the use of a small set of different digit symbols. Also, the major advantage lies in the ease of computation. Computation in Other Number Systems A very curious property of the decimal number system that points to usage of other number systems in the past is the number words used. E.g. the words for 11 and 12 are not constructed as the other teens are, suggesting a linguistic independence from words used for 10. Such peculiarities suggest remnants of use of other bases, notably 12 and 20. Words vingt and quartevingt used for 20 and 80 respectively in French suggest a possibility of number with base 20. There are traces of Babylonian astronomers using a number system called sexagesimal (base 60). The tables for addition and multiplication change with number system while the rules of arithmetic remain the same. Below is the table for addition followed by table for multiplication in duodecimal number system. (10 and 11 in the duodecimal representation below are represented using A and B respectively.) + 1 2 3 4 5 6 7 8 9 A B 1 2 3 4 5 6 7 8 9 A B 10 2 3 4 5 6 7 8 9 A B 10 11 3 4 5 6 7 8 9 A B 10 11 12 4 5 6 7 8 9 A B 10 11 12 13 5 6 7 8 9 A B 10 11 12 13 14 6 7 8 9 A B 10 11 12 13 14 15 7 8 9 A B 10 11 12 13 14 15 16 8 9 A B 10 11 12 13 14 15 16 17 9 A B 10 11 12 13 14 15 16 17 18 A B 10 11 12 13 14 15 16 17 18 19 B 10 11 12 13 14 15 16 17 18 19 1A . 1 2 3 4 5 6 7 8 9 A B 1 1 2 3 4 5 6 7 8 9 A B 2 2 4 6 8 A 10 12 14 16 18 1A 3 3 6 9 10 13 16 19 20 23 26 29 4 4 8 10 14 18 20 24 28 30 34 38 5 5 A 13 18 21 26 2B 34 39 42 47 6 6 10 16 20 26 30 36 40 46 50 56 7 7 12 19 24 2B 36 41 48 53 5A 65 8 8 14 20 28 34 40 48 54 60 68 74 9 9 16 23 30 39 46 53 60 69 76 83 A A 18 26 34 42 50 5A 68 76 84 92 B B 1A 29 38 47 56 65 74 83 92 101 REFERENCES: What is Mathematics? Second Edition - Chapter I: Natural Numbers",
    "tags": "mathematics number system what is mathematics",
    "url": "/2017/10/27/number-system/"
  },

  
  
  
  {
    "title": "Neural Networks: Cost Function and Backpropagation",
    "text": "Notation \\({(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots , (x^{(m)}, y^{(m)})}\\) are the \\(m\\) training examples L is the total number of the layers in the network \\(s_l\\) is the number of units (not counting the bias unit) in the layer l K is the number of units in the output layer For example in the network shown above, L = 4 \\(s_1 = 3,\\, s_2 = 3,\\, s_3 = 2,\\, s_4 = 2\\) K = 2 One vs All method is only needed if number of classes is greater than 2, i.e. if \\(K \\gt 2\\), otherwise only one output unit is sufficient to build the model. Cost Function of Neural Networks Cost function of a neural network is a generalization of the cost function of the logistic regression. The L2-Regularized cost function of logistic regression from the post Regularized Logistic Regression is given by, Where \\({\\lambda \\over 2m } \\sum_{j=1}^n \\theta_j^2\\) is the regularization term \\(\\lambda\\) is the regularization factor Extending (1) to then neural networks which can have K units in the output layer the cost function is given by, Where \\(h_\\Theta(x) \\in \\mathbb{R}^K \\) \\((h_\\theta(x))_i\\) is the \\(i^{th}\\) output Here the summation term \\(\\sum_{k=1}^K\\) is to generalize over the K output units of the neural network by calculating the cost function and summing over all the output units in the network. Also following the convention in regularization, the bias term in skipped from the regularization penalty in the cost function defination. Even if one includes the index 0, it would not effect the process in practice. Backpropagation Algorithm Backpropagation algorithm is based on the repeated application of the error calculation used for gradient descent similar to the regression techniques, and since it is repeatedly applied in the reverse order starting from output layer and continuing towards input layer it is termed as backpropagation. For a network with L layers the computation during foward propagation, for an input \\((x, y)\\) would be as follows, The \\(h_\\Theta(x)\\) is the prediction. In order to reduce the error between the prediction and the actual value backpropagation is used. Say, \\(\\delta_j^{(l)}\\) is the error of node j in the layer l is associated with the prediction made at that node given by \\(a_j^{(l)}\\), then backpropagation aims to calculate this error term propating backwards starting from the output unit in the last layer (layer L in the example above). So for each output unit in layer L, the error term is given by, \\(\\delta_j^{(L)} = a_j^{(L)} - y_j\\) which can be vectorized and written as, Where \\(a^{(L)}\\) is \\(h_\\Theta(x)\\). Now the error terms for the previous layers are calculated as follows, Where \\(g’(z^{(l)}) = a^{(l)} .* (1 - a^{(l)}) \\) .* is the element-wise multiplication. This backward propagation stops at l = 2 because l = 1 correponds to the input layer and no weights needs to be calculated there. Now the the gradient for the cost function which is needed for the minimization of the cost function is given by, Where regularization is ignored for the simplicity of expression. Summarizing backpropagation: Given training set \\({(x^{(1)}, y^{(1)}), \\cdots, (x^{(m)}, y^{(m)})}\\) Set \\(\\Delta_{ij}^{l} = 0\\) for all (i, j, l) For i = 1 to m: Set \\(a^{(1)} = x^{(i)} \\) Perform forward propagation to compute \\(a^{(l)}\\) for l = 1, …, L Using \\(y^{(i)}\\) compute \\( \\delta^{(L)} = a^{(L)} - y^{(i)} \\) Compute \\(\\delta^{(L-1)}, \\cdots, \\delta^{(2)}\\) using backpropagation \\(\\Delta_{ij}^{(l)} := \\Delta_{ij}^{(l)} + a_j^{(l)} \\delta_i^{(l+1)} \\) Vectorized implementation of the equation above is given by, \\(D_{ij}^{(l)} := {1 \\over m} \\Delta_{ij}^{(l)} + \\lambda \\, \\Theta_{ij}^{(l)} \\) if \\(j \\ne 0\\) \\(D_{ij}^{(l)} := {1 \\over m} \\Delta_{ij}^{(l)} \\) if \\(j = 0\\) And finally, \\( \\frac {\\partial} {\\partial \\Theta_{ij}^{(l)} } = D_{ij}^{(l)} \\) Formally, the \\(\\delta\\) terms are the partial derivatives of the cost function given by, REFERENCES: Machine Learning: Coursera - Cost Function",
    "tags": "machine learning andrew ng",
    "url": "/2017/10/03/neural-networks-cost-function-and-back-propagation/"
  },

  
  
  
  {
    "title": "Neural Networks Intuition",
    "text": "Introduction Neural networks can be used to build all types of function. This post tries to map functions of logical operations using the network. How the parameters are derived in explained in the later posts. AND Gate Using a single neuron, it is possible to achieve the approximation of an and gate. The architecture with the parameters can be seen below. The hypothesis for the above network is given by, Where g is the sigmoid function which asymptotes at 0 and 1. The output table for the hypothesis above is given below. \\(x_1\\) \\(x_2\\) \\(h_\\theta(x)\\) 0 0 \\(g(-30) \\approx 0\\) 0 1 \\(g(-10) \\approx 0\\) 1 0 \\(g(-10) \\approx 0\\) 1 1 \\(g(10) \\approx 1\\) The values of \\(h_\\theta(x)\\) is nothing but the expected value of the AND gate. OR Gate Similarly, using a single neuron, it is possible to achieve the approximation of an or gate. The architecture with the parameters can be seen below. It is same as AND gate but the bias weight is changed. The hypothesis for the above network is given by, Where g is the sigmoid function which asymptotes at 0 and 1. The output table for the hypothesis above is given below. \\(x_1\\) \\(x_2\\) \\(h_\\theta(x)\\) 0 0 \\(g(-10) \\approx 0\\) 0 1 \\(g(10) \\approx 1\\) 1 0 \\(g(10) \\approx 1\\) 1 1 \\(g(30) \\approx 1\\) The values of \\(h_\\theta(x)\\) is nothing but the expected value of the OR gate. NOT Gate Unlike the previous two examples, NOT gate is a unary operator, but still simple weights can give easy implementation of the NOT gate. The architecture and the parameters are shown below. The hypothesis for the above network is given by, Where g is the sigmoid function which asymptotes at 0 and 1. The output table for the hypothesis above is given below. \\(x_1\\) \\(h_\\theta(x)\\) 0 \\(g(10) \\approx 1\\) 1 \\(g(-10) \\approx 0\\) The values of \\(h_\\theta(x)\\) is nothing but the expected value of the NOT gate. (NOT \\(x_1\\)) AND (NOT \\(x_2\\)) Unlike the previous examples, this operation does not look straight forward, but actually it is. Here is an architecture implementation of the above gate. The hypothesis for the above network is given by, Where g is the sigmoid function which asymptotes at 0 and 1. The output table for the hypothesis above is given below. \\(x_1\\) \\(x_2\\) \\(h_\\theta(x)\\) 0 0 \\(g(10) \\approx 1\\) 0 1 \\(g(-10) \\approx 0\\) 1 0 \\(g(-10) \\approx 0\\) 1 1 \\(g(-30) \\approx 0\\) The values of \\(h_\\theta(x)\\) is nothing but the expected value of the (NOT \\(x_1\\)) AND (NOT \\(x_2\\)) operation. Perceptron Limitation All the examples untill this one were linearly seperable and hence were solved using a single neuron. But and XOR Gate is not linearly seperable as was the case with AND and OR gates and this can be clearly seen in the plot below. It is evident that there is no single straight line that can seperate the two classes in plot (c) and hence termed as not linearly seperable. This is a major drawback of perceptron (single layer neural networks). There is a simple proof for concluding that the XOR is not linearly seperable. Say, perceptron were capable of separating the two classes, then it would mean that there exists a set of weights (or parameters), \\(\\theta_0,\\, \\theta_1,\\,\\theta_2\\) such that the hypothesis is given by, Then the above hypothesis should satisfy the following truth table, \\(x_1\\) \\(x_2\\) \\(x_1\\,XOR\\,x_2\\) 0 0 0 0 1 1 1 0 1 1 1 0 Substituting and equating to 0, the hypothesis, following inequalities are generated that would determine the decision boundary. Substituting \\(b = -\\theta_0\\), above inequalities can be written as, It can be seen that the first three inequalities directly contradict the fourth one. Which means that the very first assumption made that there exist such parameters \\(\\theta_0,\\, \\theta_1,\\,\\theta_2\\) was incorrect. Hence, the XOR gate is not linearly seperable. This is where the utility and finesse of multi-layer neural network in deriving intricate features from the input features can be seen in action. XNOR Gate For simplicity, let’s consider a XNOR gate which is nothing but the negation of an XOR gate. So, it would not be wrong to say that if XNOR gate is achieved, XOR gate is not very far. Consider the following neural network with one hidden layer and the given weights. Here if weights are seen carefully, \\(a_1^{(2)}\\) is nothing but the AND gate and \\(a_2^{(2)}\\) is nothing but (NOT \\(x_1\\)) AND (NOT \\(x_2\\)). Similarly the output neuron is nothing but the OR gate. It calculates the following result table, \\(x_1\\) \\(x_2\\) \\(a_1^{(2)}\\) \\(a_2^{(2)}\\) \\(h_\\theta(x)\\) 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 The values of \\(h_\\theta(x)\\) is nothing but the expected value of the XNOR operation. This gives the intuition that the hidden layers are calculating a more complex input which inturn helps to turn the problem into a linearly seperable one using the transformations. This is the main reason the neural networks are fairly powerful classifiers because as the depth (or number of hidden layers) of the neural network increases it can derive more and more complex features for the final layer. REFERENCES: Machine Learning: Coursera - Examples and Intutions I Machine Learning: Coursera - Examples and Intutions II",
    "tags": "machine learning andrew ng",
    "url": "/2017/09/27/neural-network-intuition/"
  },

  
  
  
  {
    "title": "Neural Networks Theory",
    "text": "Introduction Neural networks were developed to mimic the way neurons in a brain work. Typically, a neuron has input connections and output connections. So basically, neuron is a computational unit which takes a set of inputs and produces output. So the basic functionality of neurons is tried to be replicated in these computational units. For example, neurons in brain interact with each other using electrical signals and are selectively activated based on certain parameters. This behaviour is transferred to a unit in neural networks using activation function which would be explained later in the post. Neuron Model: Logistic Unit Below is the representation of a basic logistic unit neuron model, Here, \\(x_0, \\cdots, x_3\\) are the input units connected to the neuron and can be considered similar to dendrites getting signals from other neurons. \\(h_\\theta(x)\\) is the activation function which basically decides whether or not should the neuron get activated or excited. The term \\(x_0 = 1\\), also called bias unit, plays an important role in the decision made for excitation of neuron. In case of a logistic unit the activation function is a logistic function or sigmoid function i.e. the neuron has a sigmoid (logistic) activation function. Sigmoid function is given by, Parameters of the model, \\(\\theta_0, \\cdots, \\theta_n\\) are also sometimes called weights of the model and represented by \\(w_1, \\cdots, w_n\\). Neural Network A group of these neuron units together forms a neural network. Below is a representation of neural network, Where network has 3 layers layer 1 is called input layer layer 3 is called output layer remaining layers are called hidden layers. In this case there is only one hidden layer, but it is possible to have multiple hidden layers \\(x_0\\) and \\(a_0^{(2)}\\) are the bias terms and equal 1 always. They are generally not counted when the number of units in a layer are calculated. So, layer 1 and layer 2 have 3 units each. Notations \\(a_i^{(j)}\\) is the activation of unit i in layer j \\(\\Theta^{(j)}\\) is the matrix of weights controlling function mapping from layer j to layer j+1. So, the network diagram above depicts the following computations, From the equation above one can generalize, if a network has \\(s_j\\) units in layer j and \\(s_{j+1}\\) units in layer j+1, then \\(\\Theta^{(j)}\\) is a matrix of dimension \\((s_{j+1} * (s_j + 1))\\). Vectorization of Network Computation Consider (2) can be written as, Where \\(z_i^{(j)} = \\Theta_{i0}^{(j-1)}\\,x_0 + \\Theta_{i1}^{(j-1)}\\,x_1 + \\Theta_{i2}^{(j-1)}\\,x_2 + \\Theta_{i3}^{(j-1)}\\,x_3\\) Given \\(x=\\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}\\) and \\(z^{(j)}=\\begin{bmatrix} z_1^{(j)} \\\\ \\vdots \\\\ z_n^{(j)} \\end{bmatrix}\\), then computation for \\(z^{(j)}\\) can be vectorized as follows, Where \\(a^{(1)} = x\\) \\(\\Theta^{(j-1)}\\) is a matrix of dimensions \\((s_j * (s_{j-1}+1))\\) \\(s_j\\) is number of activation nodes After (5), bias unit, \\(a_0^{(j)} = 1\\) is added to the activation vector of layer j and then the process repeats to get activation for the next layer, i.e. (3) is written as, Where \\(\\Theta^{(j)}\\) is a matrix of dimensions \\((s_{j+1} * (s_j + 1)) \\) Process of calculation of activations cascading across layers is called Forward Propagation. Nueral Network and Logistic Regression If one looks closely at the neural network diagram above, it can be easily seen that if one removes the layer 1, the schema looks same as a logistic regression. It is infact a logistic regression model if the hidden layer is the direct features (input layer) fed to the neuron because the activation function of the neuron is logistic (sigmoid) function. So, effectively the neural network shown above is a logistic regression where the features for classification are learnt by the hidden layer and not fed manually by human intervention. Each feature in hidden layer is mapped from the input layer. Because of this architecture there is a chance of learning much better features than started with or one can make using the higher order polynomial terms and hence there is a probability of reaching better hypotheses with neural networks. Architecture of Neural Network It is possible to have different kinds of architecture for the neural network. By architecture, it means to have different schema, i.e. number of neurons per layer can vary, number of layers used can vary, the way the neurons are connected to each other can vary and similarly various other variations are possible in terms of optimization parameters, activation etc. When then number of hidden layers is more than one, they are known as deep neural networks. REFERENCES: Machine Learning: Coursera - Model Representation I Machine Learning: Coursera - Model Representation II",
    "tags": "machine learning andrew ng",
    "url": "/2017/09/21/neural-networks/"
  },

  
  
  
  {
    "title": "Non-linear Hypotheses",
    "text": "Drawbacks of Logistic Regression Consider a highly non-linear classification task, say something similar to the one shown in the plot below. In order to achieve a decision boundary like the one plotted, one needs to introduce non-linear features in the form of quadratic and other higher order terms, similar to the equation below. As the number of features increase then number of terms in the hypotheses would also increase exponentially to get a good fit which would have high probability of overfitting the data. Hence, when the number of features is really high and the decision boundary is complex, logistic regression would not generalize the solution very well by leveraging the power of polynomial terms. So for highly complex tasks like the ones where one needs to classify objects from images, logistic regression would not perform well. For example, for images of size 100 * 100 pixels if one uses all quadratic features, there would be around 50 million parameter values to learn which is computationally very expensive task and still not a guarantee of good decision boundary. This is where Neural Networks step in to save the day. Neural Networks They are class of very powerful machine learning classifiers that are capable of fitting almost any hypotheses and are motivated by the way a brain functions. Even though the concepts of neural networks were well developed by the 80s, they came into popularity fairly recently because of the advances in the compute power of machines using multiple cores and GPUs. It is mainly because neural networks are a class of very computationally expensive algorithms and earlier systems were just not fast enough to get good results in a feasible time-frame. One Learning Algorithm Hypothesis This hypothesis puts light on the fact that even though human brain learns a variety of tasks involving visual, vocal, or audio inputs, it does not learn them using different algorithms. It has been found that if the optic nerve is re-routed to the auditory cortex (responsible for decoding audio), it would learn to use the input and work with visual input as well i.e. Auditory cortex learns to see. So, extending the result of such experiments suggesting that a single tissue in brain is capable of performing different tasks like analyse visuals, audio, touch etc, it is posited that there must be one algorithm that can approximate any learning task similar to the way brain learns. REFERENCES: Machine Learning: Coursera - Non-Linear Hypotheses Machine Learning: Coursera - Neurons and the Brain",
    "tags": "machine learning andrew ng",
    "url": "/2017/09/20/non-linear-hypotheses/"
  },

  
  
  
  {
    "title": "Regularized Logistic Regression",
    "text": "Introduction The intuition and implementation of logistic regression is implemented in Classifiction and Logistic Regression and Logistic Regression Model Similar to the linear regression, even logistic regression is prone to overfitting if there are large number of features. If the decision boundary is overfit, the shape might be highly contorted to fit only the training data while failing to generalise for the unseen data. So, the cost function of the logistic regression is updated to penalize high values of the parameters and is given by, Where \\({\\lambda \\over 2m } \\sum_{j=1}^n \\theta_j^2\\) is the regularization term \\(\\lambda\\) is the regularization factor import numpy as np mul = np.matmul \"\"\" X is the design matrix y is the target vector theta is the parameter vector lamda is the regularization parameter \"\"\" def sigmoid(X): return np.power(1 + np.exp(-X), -1) \"\"\" hypothesis function \"\"\" def h(X, theta): return sigmoid(mul(X, theta)) \"\"\" regularized cost function \"\"\" def j(theta, X, y, lamda=None): m = X.shape[0] theta[0] = 0 if lamda: return (-(1/m) * (mul(y.T, np.log(h(X, theta))) + \\ mul((1-y).T, np.log(1 - h(X, theta)))) + \\ (lamda/(2*m))*mul(theta.T, theta))[0][0] return -(1/m) * (mul(y.T, np.log(h(X, theta))) + \\ mul((1-y).T, np.log(1 - h(X, theta))))[0][0] Regularization for Gradient Descent Previously, the gradient descent for logistic regression without regularization was given by, Where \\(j \\in \\{0, 1, \\cdots, n\\} \\) But since the equation for cost function has changed in (1) to include the regularization term, there will be a change in the derivative of cost function that was plugged in the gradient descent algorithm, Because the first term of cost fuction remains the same, so does the first term of the derivative. So taking derivative of second term gives \\(\\frac {\\lambda} {m} \\theta_j\\) as seen above. So, (2) can be updated as, Where \\(j \\in \\{1, 2, \\cdots, n\\} \\) and h is the sigmoid function It can be noticed that, for case j=0, there is no regularization term included which is consistent with the convention followed for regularization. \"\"\" regularized cost gradient \"\"\" def j_prime(theta, X, y, lamda=None): m = X.shape[0] theta[0] = 0 if lamda: return (1/m) * mul(X.T, (h(X, theta) - y)) + (lamda/m) * theta return (1/m) * mul(X.T, (h(X, theta) - y)) \"\"\" Simultaneous update \"\"\" def update_theta(theta, X, y, lamda=None): return theta - alpha * j_prime(theta, X, y, lamda) Link to Rough Working Code. Change the value of lamda in the code to get different decision boundaries for the data as shown below. REFERENCES: Machine Learning: Coursera - Logistic Regression Model",
    "tags": "machine learning andrew ng",
    "url": "/2017/09/15/regularized-logistic-regression/"
  },

  
  
  
  {
    "title": "Basics of Language Model",
    "text": "Introduction Given the finite set of all the words in a language, \\(\\nu\\), a sentence in the language is the sequence of words Where \\(n \\ge 1\\) \\(x_1 \\cdots x_{n-1} \\in \\nu\\) \\(x_n\\) is a special symbol STOP \\(\\require{cancel}\\cancel{\\in} \\nu \\) Set of all the words in a language are assumed to be finite. Let \\(\\nu^{\\dagger}\\) be the infinite set of all sentences with the vocabulary \\(\\nu\\). A language model consists of a finite set \\(\\nu\\) as a function \\(p(x_1, x_2, \\cdots, x_n)\\), such that: For any \\(⟨ x_1 \\cdots x_n ⟩ \\in \\nu^\\dagger, \\, p(x_1, x_2, \\cdots, x_n) \\ge 0\\) \\(\\sum_{⟨ x_1 \\cdots x_n ⟩ \\in \\nu^\\dagger} p(x_1, x_2, \\cdots, x_n) = 1\\) So, \\(p(x_1, x_2, \\cdots, x_n)\\) is basically a probability distribution over the sentences \\(\\nu^\\dagger\\). Applications of Language Modeling A distribution of \\(p(x_1 \\cdots x_n)\\) signifies how probable a sentence is in a language. Such a distribution can prove useful in speech recognition or machine translation. Candidates generated by these algorithms can be run against the language model to check how probable the sentences are. Frequency Based Modeling Frequency based modeling is given by, Where \\(c(x_1 \\cdots x_n)\\) is the number of times \\(x_1 \\cdots x_n\\) occurs in the training corpus N is the total number of sentences in the corpus. One major drawback of such a model is that it would assign probability 0 to any sentence not seen in the training corpus. Markov Models for Fixed Length Sequences Consider a sequence of random variables, \\(X_1, X_2, \\cdots, X_n\\), where each random variable can take any value in a finite set \\(\\nu\\). n is assumed to be a fixed number. Language model aims to find the probability of \\(x_1 \\cdots x_n\\), where \\(n\\ge1\\) and \\(x_i \\in \\nu\\) for \\(i = 1 \\cdots n\\), i.e. to model the join probability, If n is a fixed number there are \\(|\\nu|^n\\) possible sequences of the form \\(x_1 \\cdots x_n\\), which makes it impossible to list all the possible sequences for a large value of n and \\(|\\nu|\\). This is where markov models help to build a more compact model. First-Order Markov Process make the assumption that identity of an element in a sequence depends only on the identity of previous element in the sequence, i.e. \\(X_i\\) is conditionally independent of \\( X_1 \\cdots X_{i-2} \\), given the value of \\(X_{i-1}\\). The first step of the equation above is exact using the chain rule of probability. The second step is a result of first-order markov model assumption. Similarly, second-order Markov models assume that identity of an element in a sequence depends only on the identity of previous two elements in the sequence, i.e. \\(X_i\\) is conditionally independent of \\(X_1 \\cdots X_{i-3}\\), given the value of \\(X_{i-1}\\) and \\(X_{i-2}\\). It is assumed that \\(x_0,\\, x_{-1} = *\\), where * is the special start symbol. Markov Sequences for Variable-length Sentences The value n, assumed to be fixed number in previous section, is considered to be a random variable itself and the nth word is always equal to the special symbol STOP. Using the second-order markov assumption, Where \\(n \\ge 1\\) \\(x_n =\\) STOP \\(x_i \\in \\nu\\) for \\(i=1 \\cdots (n-1)\\) Process of generating a sequence using the above distribution would be as follows: Initialize \\(i=1\\), and \\(x_0, x_{-1} = *\\) Generate \\(x_i\\) from the distribution, If \\(x_i = \\) STOP then return the sequence \\(x_1 \\cdots x_i\\), else set i = i+1 and repeat previous step. Trigram Language Models Trigram language models are direct application of second-order markov models to the language modeling problem. Each sentence is modeled as a sequence of n random variables, \\(X_1, \\cdots, X_n\\) where n is itself a random variable. A trigram model consists of finite set \\(\\nu\\), and a parameter, Where u, v, w is a trigram \\(w \\in \\nu \\cup \\{STOP\\}\\) \\(u, v \\in \\nu \\cup \\{*\\}\\) The value of \\(q(w| u, v)\\) can be interpreted as the probability of seeing the word w immediately after bigram u, v. So, for any sequence \\(x_1 \\cdots x_n\\) where \\(x_i \\in \\nu\\) for \\(i = 1 \\cdots (n-1)\\) and \\(x_n = \\) STOP, the probability of the sentence under trigram language model is Where \\(x_0 = x_{-1} = *\\) Trigram assumption: Each word depends on the previous two. This is essentially the second-order markov assumption used over sentences. The only step remaining in the trigram language model is the estimation of language parameters, i.e., \\(q(w|u,v)\\). Since the total number of words are \\(| \\nu |\\) the total number of possible parameters would be \\(| \\nu |^3\\). It is a very big number and hence needs some kind of indirect estimation process. Maximum Likelyhood Estimates This is the most generic solution to the estimation problem shown above. For any w, u, v, Where c(u,v,w) is the number of times the trigram is seen in corpus c(u, v) is the number of times the bigram is seen in the corpus Many of the frequencies c(u,v,w) and c(u,v) would be 0. This would effect the estimation and present the following flaws: \\(q(w|u,v) = 0\\) because c(u,v,w) is 0 which would underestimate many trigram probabilities which is unreasonable If the denominator is 0, then \\(q(w|u,v)\\) would be undefined. Perplexity It is one of the evaluation metrics for the language models and is calculated on a held-out data after the model is trained on some corpus. The held-out data is not used for parameter estimation of the language model. Consider a test dataset consisting of sentences, \\(s_1, \\cdots, s_m\\), then \\(p(s_i)\\) gives probability for sentence \\(s_i\\) in the language model. A basic measure of quality of language model would be the probability it assigns to the entire test set, give by So, higher the quantity is, the better the language model is at modeling unseen sentences. Perplexity is a direct transformation of this basic defination. Let M be the total number of words in the corpus, then average log probability under the model is which is the log probability of the entire corpus, divided by the total number of words in the corpus. Again the higher the value of this, the better the language model. Then, Perplexity is defined as Where The perplexity is a positive number. The smaller the value of perplexity, the better the language model is at modeling unseen data. Perplexity becomes a minimization parameter because of the negative power that is applied to the defination. Intuition for Perplexity Let the vocabulary, \\(\\nu\\) have N words, and the model predicts uniform distribution over the vocabulary, i.e., Then evaluating (3) using (1), Where \\(n_1, n_2, \\cdots, n_m\\) are the number of words in each sentence in the test sample and, Using (4) in (2), So, under a uniform distribution model, the perplexity is equal to the vocabulary size. Perplexity can be considered the effective vocabulary size under the model. Properties of perplexity: If for any trigram u, v, w, the estimated probability \\(q(w|u, v) = 0\\) then the perplexity will be \\(\\infty\\) which is consistent with the rule stating that a good model should not predict probability zero for an unseen dataset and perplexity is low for good models. If perplexity is the measure of language model, then 0 estimates should be avoided at all costs. Linear Interpolation The following trigram, bigram and unigram maximum-likelihood estimates are defined, Where \\(c(u,v,w)\\) is the number of times trigram u, v, w occurs \\(c(v,w)\\) is the number of times bigram v, w occurs \\(c(w)\\) is the number of times unigram w occurs \\(c()\\) is the total number of words seen in the training Properties of these models: The unigram model will never face the issue of number or denominator being 0, so the estimate is always well defined and greater than 0. But it completely ignores the context and hence discards valuable information. The trigram model use the context better than the unigram model, but has the problem of many of its counts being 0 rendering estimate value 0 or undefined. The bigram model falls between the two extremes. Linear Interpolation uses all three of these estimates to define the trigram estimate as follows, Where i.e. (6) is a weighted average of the three estimates. Discounting Methods and Katz Back-off An alternative estimation method commonly used in practice. Consider a bigram language model where the following parameter is to be found, Where \\(w \\in \\nu \\cup \\) {STOP} \\(v \\in \\nu \\cup\\) {*} Discounted Counts are used to reflect the intuition that if the counts are taken from the training corpus, there would be a systematic over-estimation of probability of bigrams seen in the corpus and hence underestimate the bigrams not seen in the corpus. So, discounted count is given by, Where \\(c^*(v, w)\\) is the discounted count \\(c(v,w)\\) is the count of bigrams, such that, \\(c(v,w) \\gt 0\\) 0.5 is the discount value Using the discounted count, (7) can be written as, i.e., use the discounted count in the numberator and regular count in the denominator. For any context \\(v\\), there is a missing mass, defined as, The intuition behind discounted methods is to divide the missing mass among words \\(w\\), such that \\(c(v,w) = 0\\). Formally, for any \\(v\\), there exist sets Then the estimate is given by, i.e if \\(c(v,w) \\gt 0\\) return \\({c^*(v, w) \\over c(v)}\\) else divide the remaining probability mass \\(\\alpha(v)\\) in proportion to the unigram estimates \\(q_{ML}(w)\\). This method can be generalized to trigram language model in a recursive way, i.e., for any bigram (u, v) define, Where 0.5 is the discount value and hence discounted count is given by, Then the trigram model is given by, where \\(\\alpha(u, v)\\) is the missing probability mass of the bigram. It can be noted that the missing probability is distributed in proportion to the bigram estimaes \\(q_{BO}(w|v)\\) given in (8). REFERENCES: Language Modeling - Michael Collins",
    "tags": "NLP machine learning",
    "url": "/2017/09/12/basics-of-language-modeling/"
  },

  
  
  
  {
    "title": "Regularized Linear Regression",
    "text": "The intuition of regularization are explained in the previous post: Overfitting and Regularization. The cost function for a regularized linear equation is given by, Where \\(\\lambda \\sum_{i=1}^n \\theta_j^2\\) is the regularization term \\(\\lambda\\) is called the regularization parameter Regularization for Gradient Descent Previously, the gradient descent for linear regression without regularization was given by, Where \\(j \\in \\{0, 1, \\cdots, n\\} \\) But since the equation for cost function has changed in (1) to include the regularization term, there will be a change in the derivative of cost function that was plugged in the gradient descent algorithm, Because the first term of cost fuction remains the same, so does the first term of the derivative. So taking derivative of second term gives \\(\\frac {\\lambda} {m} \\theta_j\\) as seen above. So, (2) can be updated as, Where \\(j \\in \\{1, 2, \\cdots, n\\} \\) It can be noticed that, for case j=0, there is no regularization term included which is consistent with the convention followed for regularization. The second equation in the gradient descent algorithm above can be written as, Where \\(\\left(1 - \\alpha {\\lambda \\over m} \\right) \\lt 1\\) because \\(\\alpha\\) and \\(\\lambda\\) are both positive. The implementation from Mulivariate Linear Regression can be updated with the following updated regularized functions for cost function, its derivative, and updates. One, can notice that \\(theta_0\\) has not been handled seperately in the code. And as expected it does not affect the regularization much. It can be implemented in the conventional way by adding a couple of logical expressions to the function \"\"\" Regularized Version \"\"\" def reg_j_prime_theta(data, theta, l, order_of_regression, i): result = 0 m = len(data) for x, y in data : x = update_features(x, order_of_regression) result += (h(x, theta) - y) * x[i] result += l*theta[i] return (1/m) * result def reg_j(data, theta, l, order_of_regression): cost = 0 m = len(data) for x, y in data: x = update_features(x, order_of_regression) cost += math.pow(h(x, theta) - y, 2) reg = 0 for j in theta: reg += math.pow(j, 2) reg = reg * l return (1/(2*m)) * (cost + reg) def reg_update_theta(data, alpha, theta, l, order_of_regression): temp = [] for i in range(order_of_regression+1): temp.append(theta[i] - alpha * reg_j_prime_theta(data, theta, l, order_of_regression, i)) theta = np.array(temp) return theta def reg_gradient_descent(data, alpha, l, tolerance, theta=[], order_of_regression = 2): if len(theta) == 0: theta = np.atleast_2d(np.random.random(order_of_regression+1) * 100).T prev_j = 10000 curr_j = reg_j(data, theta, l, order_of_regression) print(curr_j) cost_history = [] theta_history = [] counter = 0 while(abs(curr_j - prev_j) &gt; tolerance): try: cost_history.append(curr_j) theta_history.append(theta) theta = reg_update_theta(data, alpha, theta, l, order_of_regression) prev_j = curr_j curr_j = reg_j(data, theta, l, order_of_regression) if counter % 100 == 0: print(curr_j) counter += 1 except: break print(\"Stopped with Error at %.5f\" % prev_j) return theta, cost_history, theta_history reg_theta, reg_cost_history, reg_theta_history = reg_gradient_descent(data, 0.01, 1, 0.0001, order_of_regression=150) The following plot is obtained on running a random experiment with regression of order 150, which clearly shows how the regularized hypothesis is much better fit to the data. Regularization for Normal Equation The equation and derivation of Normal Equation can be found in the post Normal Equation. It is given by, But after adding the regularization term as shown in (1), making very small changes in the derivation in the post, one can reach the result for regularized normal equation as shown below, Where I is the identity matrix \\(L = \\begin{bmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\\\ \\end{bmatrix} \\) is a square matrix of size (n+1) If \\(m \\le n\\), then the \\(X^TX\\) was non-invertible in the unregularized case but, \\(X^TX + \\lambda I\\) does not face the same issues and is always invertible. The effect of regularization on regression using normal equation can be seen in the following plot for regression of order 10. No implementation of regularized normal equation presented as it is very straight forward. REFERENCES: Machine Learning: Coursera - Regularized Linear Regression",
    "tags": "machine learning andrew ng",
    "url": "/2017/09/11/regularized-linear-regression/"
  },

  
  
  
  {
    "title": "Overfitting and Regularization",
    "text": "Overfitting If the number of features is very high, then there is a probability that the hypothesis fill fit all the points in the training data. It might seem like a good thing to happen but has a contradictory results. Suppose a hypothesis of high degree is fit to a set of points such that all the points lie of the hypothesis. Plot below shows a case of overfitting with a regression of order 100. This plot can be created if the code from Multivariate Linear Regression is run with the parameter order of regression set to 100. The curve above shows a case of overfitting where the hypothesis has high variance. While it fits the training data with good accuracy, it will fail to predict the values for unseen cases with same accuracy because of the high variance in the prediction curve. Opposite to this spectrum is the case of underfitting. For example there exists a data set that increased linearly initialy and then saturates after a point. If a univariate linear regression is fit to the data it will give a straight line which might be the best fit for the given training data, but fails to recognize the saturation of the curve. Such a hypothesis is said to be underfit and have high bias i.e. the hypothesis is biased that the prediction should vary linearly over the feature variation. An optimum hypothesis is the trade off between variance and bias. All three cases can be seen in the plots below. Both underfitting and overfitting are undesirable and should be avoided. While overfitting might seem to work well for the training data, it will fail to generalize to new examples. Overfitting and underfitting are not limited to linear regression but also affect other machine learning techniques. Effect of underfitting and overfitting on logistic regression can be seen in the plots below. Detecting Overfitting For lower dimensional datasets, it is possible to plot the hypothesis to check if is overfit or not. But same strategy cannot be applied when the dimensionality of the dataset increases beyond the limit of visualization. So, plotting hypothesis may not always work. So other methods have to utilized to address overfitting. There are two options to avoid the overfitting: Reduce the number of features: Mostly overfitting is observed when the number of features is really high and the number of training examples are less. In such cases reducing the number of features can help avoid the issue of overfitting. Reducing the number of features can be done manually, or using model selection algorithms which help decide which features to eliminate. This also presents a disadvantage as removing features is sometimes equivalent to removing information. Regularization: Keep all the features but reduce magnitude/values of parameters \\(\\theta_j\\). This technique works well, when there exists a lot features and each contributes a bit to the prediction, i.e. Regularization works well when there are a lot of slightly useful features. Regularization Intuition It is mathematically seen that the high variance of a overfit hypothesis is attributed to higher value of parameters corresponding to higher order features i.e. more the dependency is biased on a single feature, greater are the chances of a hypothesis overfitting. This effect can be counter acted by making sure that the values of parameter \\(\\theta_j\\) are small. This is done by penalizing the algorithm proportional to value of \\(\\theta_j\\) which will ensure small values of these parameters and hence would prevent overfitting by attributing small contributions from each features and hence removing high bias or high variance. So the main ideas in regularization are: maintain small values for the parameters \\(\\theta_0, \\theta_1, \\cdots , \\theta_n \\) which keeps the hypothesis simple and less prone to overfitting Mathematically, regularization is acheived by modifying the cost function as follows, Where \\(\\lambda \\sum_{i=1}^n \\theta_j^2\\) is the regularization term and \\(\\lambda\\) is called the regularization parameter. If noticed closely, this term mainly points to the fact that if value of \\(\\theta_j\\) is increased, it would consequently increase the cost which is to be minimized during gradient descent. So it would ensure small values of the parameters as intented to prevent overfitting. By convention \\(\\theta_0\\) is not penalized but in practice it does not affect the algorithm a lot. The regularization parameter \\(\\lambda\\), controls the trade off between the goal of fitting the training set well and the goal of keeping the parameters small and the hypothesis simple. So, if the value of \\(\\lambda\\) is kept very large, it would fail to fit the dataset properly and would give a underfit hypothesis. REFERENCES: Machine Learning: Coursera - The Problem of Overfitting Machine Learning: Coursera - Cost Function",
    "tags": "machine learning andrew ng",
    "url": "/2017/09/08/overfitting-and-regularization/"
  },

  
  
  
  {
    "title": "Multiclass Logistic Regression",
    "text": "Introduction For intuition and implementation of Binary Logistic Regression refer Classifiction and Logistic Regression and Logistic Regression Model. Multiclass logistic regression is a extension of the binary classification making use of the one-vs-all or one-vs-rest classification strategy. Intuition Given a classification problem with n distinct classes, train n classifiers, where each classifier draws a decision boundary for one class vs all the other classes. Mathematically, Implementation Below is an implementation for multiclass logistic regression with linear decision boundary, where number of classes is 3 and one-vs-all strategy is used. import math import numpy as np import matplotlib.pyplot as plt x_orig = [[0,0], [0,1], [1, 0], [1, 1], [2, 2], [2, 3], [3, 2], [3, 3], [0, 4], [1, 4], [0, 5], [1, 5]] y_orig = [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2] x = np.atleast_2d(x_orig) y = np.atleast_2d(y_orig).T def h(X, theta): return 1 / (1 + np.exp(-mul(X, theta))) def j(X, y, theta): return (-1/m) * (mul(y.T, np.log(h(X, theta))) + mul((1-y).T, np.log(1-h(X, theta)))) def update(X, y, theta): return theta - (alpha/m * mul(X.T, (h(X, theta) - y))) theta_all = [] for _ in range(3): theta = np.random.randint(1, 100, size=(3, 1))/ 100 mul = np.matmul alpha = 0.6 m = len(x) x = np.atleast_2d(x_orig) y = np.atleast_2d(y_orig).T idx_0 = np.where(y!=_) idx_1 = np.where(y==_) y[idx_0] = 0 y[idx_1] = 1 X = np.hstack((np.ones((len(x), 1)), x)) prev_j = 10000 curr_j = j(X, y, theta) tolerance = 0.000001 theta_history = [theta] cost_history = [curr_j] while(abs(curr_j - prev_j) &gt; tolerance): theta = update(X, y, theta) theta_history.append(theta) prev_j = curr_j curr_j = j(X, y, theta) cost_history.append(curr_j[0][0]) theta_all.append(theta) print(\"classifier %d stopping with loss: %.5f\" % (_, curr_j[0][0])) def theta_2(theta, x_range): return [(-theta[0]/theta[2] - theta[1]/theta[2]*i) for i in x_range] x_range = np.linspace(-1, 4, 100) x = np.atleast_2d(x_orig) y = np.atleast_2d(y_orig).T fig, ax = plt.subplots() ax.set_xlim(-1, 4) ax.set_ylim(-1, 6) plt.scatter(x[np.where(y == 2), 0], x[np.where(y == 2), 1]) plt.scatter(x[np.where(y == 1), 0], x[np.where(y == 1), 1]) plt.scatter(x[np.where(y == 0), 0], x[np.where(y == 0), 1]) for theta in theta_all: plt.plot(x_range, theta_2(theta, x_range)) plt.title('Multiclass Logistic Regression') plt.show() Below is the plot of all the decision boundaries found by the logistic regression. Value of \\(h_\\theta^{(i)}(x)\\) is the probability of data point belonging to \\(i^{th}\\) class as seen in (1). Keeping this is mind one can decide the precedence of the class based on the values of its corresponding prediction on that data point. So, the predicted class is the one with maximum value of corresponding hypothesis. It shown in the plot below. Similar to the above implementation the classificaiton can be extented to many more classes. REFERENCES: Machine Learning: Coursera - Multiclass Classification: One-vs-All",
    "tags": "machine learning andrew ng",
    "url": "/2017/09/06/multiclass-logistic-regression/"
  },

  
  
  
  {
    "title": "Logistic Regression Model",
    "text": "Introduction Classifiction and Logistic Regression explains why logistic regression and intuition behind it. This post is about how the model works and some intuitions behind it. Consider a training set having m examples, Where \\(x \\in \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_m \\end{bmatrix} \\in \\mathbb{R}^{n+1} \\) \\(x_0 = 1\\) \\(y \\in {0, 1}\\) And hypothesis is given by, Cost Function It can be seen in Mulivariate Linear Regression that the cost function for the linear regression is given by, Where \\(Cost(h_\\theta(x), y) \\) is the cost the learning algorithm has to pay if it makes an error in the prediction and from (2), it is given by, In case of linear regression the value of cost depends on how off is the prediction of the regression from the expected value which works well for the optimization required in linear regression. But this cost function would not work well for the logistic regression because the hypothesis for logistic regression is the complex sigmoid term shown in (1), gives non-convex curve with many local minima as shown in the plot below. So gradient descent will not work properly for such a case and therefore it would be very difficult to minimize this function. import math import numpy as np import matplotlib.pyplot as plt x = np.array([-20, -5, -1, 10, -50, -10, 2, -3, 4, 1]).T y = np.array([-1, 3, -2, 3, 4, -5, 1, 3, -4, 1]).T mul = np.matmul def j(x, y, theta): h = x*theta h = 1 / (1 + np.exp(-h))\\ d = h - y s = mul(d.T, d) return s/(len(x)*2) theta = (np.array(range(-100, 100))/100).tolist() cost = [j(x, y, i) for i in theta] plt.plot(theta, cost) plt.show() So, cost function for logistic regression is given by, The plots of the functions above can be seen below. It is clear that new cost function can be minimized because its convex. Other useful properties of the chosen cost function are: if y = 1 and h(x) = 1, then Cost = 0 h(x) \\(\\to\\) 0, then Cost \\(\\to \\infty\\) if y = 0 and h(x) = 0, then Cost = 0 h(x) \\(\\to\\) 1, then Cost \\(\\to \\infty\\) Since \\(y \\in \\{0, 1\\} \\), (4) can be written as, So, adding to (2), This cost function is reached at using the principle of maximum likelyhood expectation. So now to get optimal \\(\\theta\\), Which is done using gradient descent given by, And the differential term is given by, Differential of log is given by, And differential of sigmoid function is given by, Using (9) and (10) in (8), Using (11) in (7), Which looks same as the result of gradient descent of linear regression in Mulivariate Linear Regression. But there is a difference which can be seen in the defination of the hypothesis of linear regression and logistic regression. Vectorizing (12), Where X is the design matrix. Note: Feature Scaling is as important for logistic regression as it is for linear regression as it helps the process of gradient descent. Advanced Optimization Given the functions for calculation of \\(J(\\theta)\\) and \\(\\frac {\\partial} {\\partial \\theta} J(\\theta)\\) one can apply one of the many optimization techniques other than gradient descent: Conjugate Descent BFGS L-BFGS Advantage Disadvantages No need to manually pick \\(\\alpha\\) More complex Often faster than gradient descent Harder to debug Most of these algorithms have a clever inner loop like line search algorithm which automatically finds out the best \\(\\alpha\\) value. Implementation Below is an implementation for linear decision boundary, import math import numpy as np import matplotlib.pyplot as plt x = [] y = [] for _ in range(30): i = np.random.rand() x.append(i) y.append(round(i)) x.append(0) y.append(1) x = np.atleast_2d(x).T y = np.atleast_2d(y).T plt.scatter(x[np.where(y==1)], y[np.where(y==1)]) plt.scatter(x[np.where(y==0)], y[np.where(y==0)]) theta = np.random.randint(1, 100, size=(2, 1))/ 1000000 theta = np.array([[0.5], [0]]) mul = np.matmul alpha = 0.6 m = len(x) X = np.hstack((np.ones((len(x), 1)), x)) def h(X, theta): return 1 / (1 + np.exp(-mul(X, theta))) def j(X, y, theta): return (-1/m) * (mul(y.T, np.log(h(X, theta))) + mul((1-y).T, np.log(1-h(X, theta)))) def update(X, y, theta): return theta - (alpha/m * mul(X.T, (h(X, theta) - y))) prev_j = 10000 curr_j = j(X, y, theta) tolerance = 0.000001 theta_history = [theta] cost_history = [curr_j] while(abs(curr_j - prev_j) &gt; tolerance): theta = update(X, y, theta) theta_history.append(theta) prev_j = curr_j curr_j = j(X, y, theta) cost_history.append(curr_j[0][0]) print(curr_j[0][0]) plt.plot(x, mul(X, theta)) plt.show() Plot above shows how the linear decision boundary fits the data over the iterations. The plot below is the contour plot of the cost function. Following is an implementation of non-linear decision boundary. The code is similar to the previous implementation but the data and the dimensions of the design matrix vary because of higher number of features. import math import numpy as np import matplotlib.pyplot as plt x = np.array([ [0,0], [0,1], [0, -1], [1, 0], [-1, 0], [0,2], [0, -2], [2, 0], [-2, 0] ]) y = np.atleast_2d([0, 0, 0, 0, 0, 1, 1, 1, 1]).T plt.scatter(x[np.where(y==1),0], x[np.where(y==1), 1]) plt.scatter(x[np.where(y==0),0], x[np.where(y==0), 1]) theta = np.random.randint(1, 100, size=(5, 1))/ 100 mul = np.matmul alpha = 0.1 m = len(x) X = np.hstack((np.ones((len(x), 1)), x, np.power(x,2))) def h(X, theta): return 1 / (1 + np.exp(-mul(X, theta))) def j(X, y, theta): return (-1/m) * (mul(y.T, np.log(h(X, theta))) + mul((1-y).T, np.log(1-h(X, theta)))) def update(X, y, theta): return theta - (alpha/m * mul(X.T, (h(X, theta) - y))) prev_j = 10000 curr_j = j(X, y, theta) tolerance = 0.000001 theta_history = [theta] cost_history = [curr_j] while(abs(curr_j - prev_j) &gt; tolerance): theta = update(X, y, theta) theta_history.append(theta) prev_j = curr_j curr_j = j(X, y, theta) cost_history.append(curr_j[0][0]) print(\"Regression stopped with error: %.2f\" % curr_j[0][0]) x = np.array(range(-1534,1535))/1000 y1 = [math.sqrt((-theta[0]-(theta[3]*i*i))/theta[4]) for i in x.tolist()] y2 = [-math.sqrt((-theta[0]-(theta[3]*i*i))/theta[4]) for i in x.tolist()] plt.plot(x, y1, 'b') plt.axis('equal') plt.plot(x, y2, 'b') plt.title('Non-Linear Decision Boundary') plt.show() The following plot shows the circular decision boundary. A rough implementation of all these plots and some more can be found here. REFERENCES: Machine Learning: Coursera - Logistic Regression Model Machine Learning: Coursera - Simplified Cost Function and Gradient Descent Machine Learning: Coursera - Advanced Optimization",
    "tags": "machine learning andrew ng",
    "url": "/2017/09/02/logistic-regression-model/"
  },

  
  
  
  {
    "title": "Classification and Logistic Regression",
    "text": "Introduction Classification is a supervised learning problem wherein the target variable is categorical unlike regression where the target variable is continuous. Classification can be binary i.e. only two possible values of the target variable or multi-class i.e. more than two categories. The most basic step would be to try and fit regression curve to see if one can achieve classification using the same approach. Below is a plot attempting the same. It can be seen below that the attempt to achieve classification using regression curve and thresholding will not always yield conclusive results. In first case say only red data points are in the dataset, then fitting the curve and setting the threshold at 0.5 would work, but say an outlier is present like the blue data point then the same decision boundary D1 would shift to D2 if the threshold is kept constant and the boundary would not be perfect. Hence there is a need of Decision Boundary instead of predictive curve. Applying linear regression to classification problem might work in some cases but is not advisable as it would not scale with complexity. Another issue with application of linear regession to classification would be that even know the categorical variables are discreet say 1s and 0s, the hypothesis would give continuous values which maybe much greater that 1 or much lesser than 0. This issue can be solved by using logistic regression where Logistic Regression Since (1) is to be true, the hypothesis from linear regression given by \\(h_\\theta(x) = \\theta^T\\,x\\) will not work for logistic regression. Hence there is a need of squashing function i.e. a function which limits the output of hypothesis between given range. For logistic regression sigmoid function is used as the squashing function. The hypothesis for logistic regression is give by, Where \\(g(z) = {1 \\over 1 + e^{-z}}\\) and is called sigmoid function or logistic function. Plot of the sigmoid function is given below which shows no matter what the value of x, the function returns a value between 0 and 1 consistent with (1). The value of hypothesis is interpretted as the probability that the input x belongs to class y=1. i.e. probability that y=1, given x, parametrized by \\(\\theta\\). It can be mathematically represented as, The fundamental properties of probability holds here, i.e., Decision Boundary for the given hypothesis of logistic regression in (2), say \\(\\delta=0.5\\) is chosen as the threshold for the binary classification, i.e. From the plot of sigmoid function, it is seen that Using (6), (5) can be rewritten as, Suppose the training data is as show in the plot above where dots and Xs are the two different classes. Let the hypothesis \\(h_\\theta(x)\\) and the optimal value of \\(\\theta\\) be given by, Using the \\(\\theta\\) from (9) and hypothesis from (8) , (7) can be written as, If the line \\(x_1 + x_2 = 12\\) is plotted as shown in the plot above then the region below i.e. the yellow region is where \\(x_1 + x_2 \\lt 12\\) and predicted 0 and similarly the white region above the line \\(x_1 + x_2 = 12\\) is where \\(x_1 + x_2 \\geq 12\\) and hence predicted 1. The line here is called the decision boundary. As the name suggests this line seperates the region with prediction 0 from region with prediction 1. Decision boundary and prediction regions are the property of the hypothesis and not of the dataset. Dataset is only used to fit the parameters, but once the parameters are determined they solely define the decision boundary It is possible to achieve non-linear decision boundaries by using the higher order polynomial terms and can be encorporated in a way similar to how multivariate linear regression handles polynomial regression. The plot above is an example of non-linear decision boundary using higher order polynomial logistic regression. Say, the hypothesis of the logistic regression has higher order polynomial terms, and is given by, The, \\(\\theta\\) given below would form an apt decision boundary, Substituting (12) in (11), So, from (7), the decision boundary is given by, Which the equation of a circle at origin with radius 0, as can be seen in the plot above. And, using the \\(\\theta\\) from (12) and hypothesis from (11) , (7) can be written as, As the order of features is increased more and more complex decision boundaries can be achieved by logistic regression. Gradient Descent is used to fit the parameter values \\(\\theta\\) in (9) and (12). REFERENCES: Machine Learning: Coursera - Classification Machine Learning: Coursera - Hypothesis Representation Machine Learning: Coursera - Decision Boundary",
    "tags": "machine learning andrew ng",
    "url": "/2017/08/31/classification-and-representation/"
  },

  
  
  
  {
    "title": "Normal Equation",
    "text": "Introduction Gradient descent is an algorithm which is used to reach an optimal solution iteratively using the gradient of the loss function or the cost function. In contrast, normal equation is a method that helps solve for the parameters analytically i.e. instead of reaching the solution iteratively, solution for the parameter \\(\\theta\\) is reached at directly by solving the normal equation. Intuition Consider a one-dimensional equation for the cost function given by, Where \\(\\theta \\in \\mathbb{R} \\) According to calculus, one can find the minimum of this function by calculating the derivative and solving the equation by setting derivative equal to zero, i.e. Similarly, extending (1) to multi-dimensional setup, the cost function is given by, Where \\(\\theta \\in \\mathbb{R}^{n+1} \\) n is the number of features m is the number of training samples And similar to (2), the minimum of (3) can be found by taking partial derivatives w.r.t. individual \\(\\theta_i \\forall i \\in (0, 1, 2, \\cdots, n) \\) and solving the equations by setting them to zero, i.e. where \\(i \\in (0, 1, 2, \\cdots, n)\\) Through derivation one can find that \\(\\theta\\) is given by, Feature scaling is not necessary for the normal equation method. Reason being, the feature scaling was implemented to prevent any skewness in the contour plot of the cost function which affects the gradient descent but the analytical solution using normal equation does not suffer from the same drawback. Comparison between Gradient Descent and Normal Equation Given m training examples, and n features Gradient Descent Normal Equation Proper choice of \\(\\alpha\\) is important \\(\\alpha\\) is not needed Iterative Method Direct Solution Works well with large n. Complexity of algorithm is O(\\(kn^2\\)) Slow for large n. Need to compute \\((X^TX)^{-1}\\). Generally the cost for computing the inverse is O(\\(n^3\\)) Generally if the number of features is less than 10000, one can use normal equation to get the solution beyond which the order of growth of the algorithm will make the computation very slow. Non-invertibility Matrices that do not have an inverse are called singular or degenerate. Reasons for non-invertibility: Linearly dependent features i.e. redundant features. Too many features i.e. \\(m \\leq n\\), then reduce the number of features or use regularization. Calculating psuedo-inverse instead of inverse can also solve the issue of non-invertibility. Implementation import matplotlib.pyplot as plt import numpy as np \"\"\" Dummy Data for Linear Regression \"\"\" data = [(1, 1), (2, 2), (3, 4), (4, 3), (5, 5.5), (6, 8), (7, 6), (8, 8.4), (9, 10), (5, 4)] \"\"\" Matrix Operations \"\"\" inv = np.linalg.inv mul = np.matmul X = [] y = [] for x_i, y_i in data: X.append([1, x_i]) y.append(y_i) X = np.array(X) y = np.atleast_2d(y).T \"\"\" Theta Calculation Using equation (5) \"\"\" theta = mul(mul(inv(mul(X.T, X)), X.T), y) \"\"\" Prediction of y using theta \"\"\" y_pred = np.matmul(X, theta) \"\"\" Plot Graph \"\"\" plt.scatter([i[0] for i in data], [i[1] for i in data]) plt.plot([i[0] for i in data], y_pred, 'r') plt.title('Regression using Normal Equation') plt.xlabel('x') plt.ylabel('y') plt.show() Derivation of Normal Equation Given the hypothesis, \\(\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\) Let X be the design matrix wherein each row corresponds to the features in \\(i^{th}\\) sample of the m samples. Similarly, y is the vector with all the target values for all the m training samples. The cost function for the hypothesis (6) is given by (3). The cost function can be vectorized as follows for replacing the sigma operation with the sum over terms for matrix multiplication, Since \\(X\\theta\\) and \\(y\\) both are vectors, \\((X\\theta)^Ty = y^T(X\\theta)\\). So (7) can be further simplified as, Taking partial derivative w.r.t \\(\\theta\\) and equating to zero, Let, Taking partial derivatives, Vectorizing (10), Similarly, let, Taking partial derivatives, Vectorizing above equations, Substitution (13) and (11) in (8), If \\(X^T X\\) is invertible, then, which is same as (5) REFERENCES: Machine Learning: Coursera - Normal Equation Derivation of the Normal Equation for linear regression Normal Equation and Matrix Calculus",
    "tags": "machine learning mathematics andrew ng",
    "url": "/2017/08/28/normal-equation/"
  },

  
  
  
  {
    "title": "Multivariate Linear Regression",
    "text": "Introduction Multivariate linear regression is the generalization of the univariate linear regression seen earlier i.e. Cost Function of Linear Regression. As the name suggests, there are more than one independent variables, \\(x_1, x_2 \\cdots, x_n\\) and a dependent variable \\(y\\). Notation \\(x_1, x_2 \\cdots, x_n\\) denote the n features \\(y\\) denotes the output variable to be predicted \\(n\\) is number of features \\(m\\) is the number of training examples \\(x^{(i)}\\) is the \\(i^{th}\\) training example \\(x_j^{(i)}\\) is the \\(j^{th}\\) feature of the \\(i^{th}\\) training example Hypothesis The hypothesis in case of univariate linear regression was, Extending the above function to multiple features, hypothesis of multivariate linear regression is given by, Where, \\(\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\\\ \\end{bmatrix} \\in \\mathbb{R}^{n+1}\\) and \\(x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\\\ \\end{bmatrix} \\in \\mathbb{R}^{n+1} \\) Cost Function The cost function for univariate linear regression was, Extending the above function to multiple features, the cost function for multiple features is given by, Where \\(\\theta\\) is a vector give by \\(\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\\\ \\end{bmatrix} \\) Gradient Descent Note: simultaneous update only Evaluating the partial derivative \\({\\partial \\over \\partial \\theta_j} J(\\theta)\\) gives, It can be easily seen that (4) is generalization of the update equation for univariate linear regression, because if we take only two features \\(\\theta_0\\) and \\(\\theta_1\\) and substitute in (4) the values, it results in the same equations as in Univariate Linear Regression Feature Scaling It is found that during gradient descent if the features are on the same scale then the algorithm tends to work better than when the features are not appropriately scaled in the same range. The plot below shows the effect of feature scaling on the contour plot of the cost function of hypothesis based on these features. As seen above, if the contours are skewed then learning steps would take longer to converge as the steps would be more prone to oscillatory behaviour as shown in the left plot. Whereas if the features are properly scaled, then the plot is evenly distributed and the steps of gradient descent have better profile of convergence. Scaling of features between 0 and 1 is achieved by dividing the features by max. This helps in keeping all the features in appropriate ranges. The aim is to ideally keep the features around the range -1 to 1. Different ways of achieving feature scaling: Normalization: Divide each feature by max of the feature column. Mean Normalization: Replace a feature \\(x_i\\) with \\(x_i - \\mu_i\\) so that the approximate mean of the features is 0 which is then normalized. Not applied to the feature \\(x_0\\) Where \\(x_i\\) is the feature value \\(\\mu_i\\) is the mean \\(S_i\\) is the standard deviation or the range i.e. \\(max - min\\) Learning Rate There are several ways of debugging the gradient descent algorithm. One of the ways is to plot the graph of cost function as a function of number of epochs. If the value is decreasing for every epoch then the descent is working fine. If the curve is plotted as shown one can easily infer that a saturation is reached after 150 iterations. Automatic Convergence Test: Gradient descent can be considered to be converged if the drop in cost function is not more than a preset threshold say \\(10^{-3}\\) Looking at the plot can point out if the algorithm is not working properly. For example, plot A is a proper learning curve but if the plot shows that value of cost function is increasing as in plot C, then this indicates the algorithm is diverging. It generally happens if the value of learning rate \\(\\alpha\\) is too high. Also, if the plot shows that the value is oscillating or fluctuating then the learning rate needs to be reduced as the steps are not small enough to proceed to the minima. For sufficiently small \\(\\alpha\\), gradient descent should decrease on every iteration. Very small learning rate is not advisable as the algorithm will be slow to converge as seen in plot B. In order to choose optimum value of \\(\\alpha\\) run the algorithm with different values like, 1, 0.3, 0.1, 0.03, 0.01 etc and plot the learning curve to understand whether the value should be increased or descreased. Feature Engineering Sometimes it might be fruitful to generate new features by combining the existing ones. For example, given width and length of a property to predict price it might be helpful to use area of the property i.e. width * length as an additional feature. Polynomial Regression The concept of feature engineering can be used to achieve polynomial regression. Say the polynomial hypothesis chosen is, This function can be addressed as multivariate linear regression by substitution and is given by, Where \\(x_n = x^n\\) Note: if using features like this then it is very important to apply feature scaling in order to avert issues related to feature range imbalance. This technique can be very powerful because one can fit all types of features using the substitution model. For example one can get a non-decreasing function as opposed to quadratic function which comes back down by using the following function Implementation import random import matplotlib.pyplot as plt import math import numpy as np \"\"\" Dummy Data for Multivariate Regression \"\"\" data = [(1, 1), (2, 2), (3, 4), (4, 3), (5, 5.5), (6, 8), (7, 6), (8, 8.4), (9, 10), (5, 4)] \"\"\" Plot the line using theta_values \"\"\" def plot_line(formula, x_range, order_of_regression): x = np.array(x_range).tolist() y = [formula(update_features(x_i, order_of_regression)) for x_i in x] plt.plot(x, y) \"\"\" Hypothesis Function \"\"\" def h(x, theta): return np.matmul(theta.T, x)[0][0] \"\"\" Partial Derivative w.r.t. theta_i \"\"\" def j_prime_theta(data, theta, order_of_regression, i): result = 0 m = len(data) for x, y in data : x = update_features(x, order_of_regression) result += (h(x, theta) - y) * x[i] return (1/m) * result \"\"\" Update features by order of the regression \"\"\" def update_features(x, order_of_regression): features = [1] for i in range(order_of_regression): features.append(math.pow(x, i+1)) return np.atleast_2d(features).T \"\"\" Cost Function \"\"\" def j(data, theta, order_of_regression): cost = 0 m = len(data) for x, y in data: x = update_features(x, order_of_regression) cost += math.pow(h(x, theta) - y, 2) return (1/(2*m)) * cost \"\"\" Simultaneous Update \"\"\" def update_theta(data, alpha, theta, order_of_regression): temp = [] for i in range(order_of_regression+1): temp.append(theta[i] - alpha * j_prime_theta(data, theta, order_of_regression, i)) theta = np.array(temp) return theta \"\"\" Gradient Descent For Multivariate Regression \"\"\" def gradient_descent(data, alpha, tolerance, theta=[], order_of_regression = 2): if len(theta) == 0: theta = np.atleast_2d(np.random.random(order_of_regression+1) * 100).T prev_j = 10000 curr_j = j(data, theta, order_of_regression) print(curr_j) cost_history = [] theta_history = [] while(abs(curr_j - prev_j) &gt; tolerance): try: cost_history.append(curr_j) theta_history.append(theta) theta = update_theta(data, alpha, theta, order_of_regression) prev_j = curr_j curr_j = j(data, theta, order_of_regression) print(curr_j) except: break print(\"Stopped with Error at %.5f\" % prev_j) return theta theta = gradient_descent(data, 0.001, 0.001) The above plot shows the working of multivariate linear regression to fit polynomial curve. The higher order terms of the polynomial hypothesis are fed as separate features in the regression. The plot is the shape of a parabola which is consistent with the shape of curves of second order polynomials. Note: The implementation above does not have scaled features. It would be harder to make the algorithm converge if the features are not scaled. But if they are scaled properly, not only does the algorithm converges better but also faster. Below is the plot of the curve fitting by gradient descent when the features are scaled appropriately. A rough implementation of the feature scaling used to get the plot above can be found here. REFERENCES: Machine Learning: Coursera - Multivariate Linear Regression Machine Learning: Coursera - Gradient Descent for Multiple Variables Machine Learning: Coursera - Gradient Descent in Practice I - Feature Scaling Machine Learning: Coursera - Gradient Descent in Practice II - Learning Rate Machine Learning: Coursera - Feature and Polynomial Regerssion",
    "tags": "machine learning andrew ng",
    "url": "/2017/08/23/multivariate-linear-regression/"
  },

  
  
  
  {
    "title": "Linear Algebra Review",
    "text": "Matrices In mathematics, a matrix is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. For example, A is a matrix below, Dimension of a matrix is given by n * m where n is number of rows and m is number of columns. So the matrix above is a 2*3 matrix. It is also sometimes represented as \\(\\mathbb{R}^{2*3}\\). Matrices are generally represented with uppercase. Also, if A is a matrix, \\(A_{ij}\\) is the i,j entry i.e. the element in \\(i^{th}\\) row and \\(j^{th}\\) column. For example, Vectors In mathematics, vector is a matrix that has only one column. For example, x is a vector below, So effectively vector is a n * 1 matrix where n is the number of rows. It is termed as n-dimensional vector. It is also sometimes represented as \\(\\mathbb{R}^n\\). Vectors are generally represented with lowercase. Also, if x is a matrix, \\(x_{i}\\) is the element in \\(i^{th}\\) row. For example, Vectors can be 0 or 1 indexed i.e. the start of index numbering may begin with 0 or 1. Generally in mathematics, 1-indexed notation is followed while in computer science 0-indexed notation is more popular. Matrix Addition Matrix addition is nothing but adding them element by element. Only matrices of same dimensions can be added The resultant matrix has same dimension. The operation is commutative, associative and distributive. For example, Scalar Multiplication Scalar multiplication is multiplication of a real number with each element of the matrix. The resultant matrix has the same dimension. The operation is commutative, associative and distributive. For example, Matrix operation follow the BODMAS rule for the order of precedence. Matrix Vector Multiplication Consider matrix A and vector x, then the matrix-vector multiplication is given by, Following properties are inferred: Operation is not always commutative. Number of columns in matrix A has to match the number of rows in vector x. The resultant vector of the multiplication is of dimension n as can be seen above. So if A is m*n and x is n-dimensional then, resultant is m-dimensional vector. Application A hypothesis \\(h_\\theta(x) = -40 + 0.25\\,x\\) can be applied to a set of x’s such as (2104, 1416, 1534, 852) then it can be done as follows, Matrix-Matrix Multiplication It is a binary operation that produces a matrix from two matrices. So, if A is \\(n * m\\) matrix and B is a \\(m * p\\) matrix, then their product AB is a \\(n * p\\) matrix. The m enteries along rows of A are multiplied with the m enteries along the column of B and summed to produce elements of AB. When two linear transformations are represented by matrices, then the matrix product represents the composition of the two transformations. Following properties are inferred: Operation is not always commutative. Number of columns in matrix A has to match the number of rows in vector x. Application Consider three hypothesis as follows, All three can be applied to a set of inputs as shown in matrix-vector multiplication. Here, each column corresponds to a specific hypothesis. Matrix Multiplication Properties Commutative Property: Scalar multiplication is commutative while matrix multiplication is not commutative. Associative Property: Scalar and matrix multiplication are both associative. Identity Matrix: Denoted by I (or \\(I_{n*n}\\)) has all elements zero except for the main diagonal elements which are set to 1. It has the following properties. Inverse In the space of real numbers each number is said to have an inverse if the product of the number and the inverse results in the identity i.e. 1. Also not all the real numbers have an inverse, for example, the number 0 does not have an inverse, because 1/0 is undefined. Similarly, a matrix A is said to have an inverse if there exists a \\(A^{-1}\\) such that Only square matrices have inverses. Matrices that do not have an inverse are called singular or degenerate matrices. Transpose Given a matrix A, having dimension m * n and let \\(B = A^T\\) be its transpose, then B is a n * m matrix such that, It is basically the operation where each row is sequentially replaced as a column in the resultant matrix. REFERENCES: Machine Learning: Coursera - Matrices and Vectors Machine Learning: Coursera - Addition and Scalar Multiplication Machine Learning: Coursera - Matrix Vector Multiplication Machine Learning: Coursera - Matrix Matrix Multiplication Machine Learning: Coursera - Matrix Multiplication Properties Machine Learning: Coursera - Inverse and Transpose Matrix Multiplication: Wikipedia",
    "tags": "machine learning mathematics andrew ng",
    "url": "/2017/08/20/linear-algebra/"
  },

  
  
  
  {
    "title": "Gradient Descent for Linear Regression",
    "text": "Introduction The posts Cost Function of Linear Regression and Gradient Descent introduced the linear regression cost function and the gradient descent algorithm individually. If the gradient descent is applied to the linear regression cost function would help reach an optimal solution without much manual intervention. Gradient Descent Gradient descent algorithm can be summarized as, Where := is the assignment operator \\(\\alpha\\) is the learning rate which basically defines how big the steps are during the descent \\( \\frac {\\partial} {\\partial \\theta_j} J(\\theta_0, \\theta_1)\\) is the partial derivative term j = 0, 1 represents the feature index number Linear Regression Linear regression hypothesis is given by, And the corresponding cost function is given by, Gradient Descent for Linear Regression Gradient descent is applied to the optimazation problem of the cost function of the linear regression given in (2) in order to find parameters that minimize the cost. In order to apply gradient descent, the derivative term i.e. \\(\\frac {\\partial} {\\partial \\theta_j} J(\\theta_0, \\theta_1)\\) needs to be calculated. Updating the values of the derivative term in the gradient descent algorithm given in (1) where the update must be done simultaneously, The cost function for a linear regression is a convex function i.e. a bow-shaped function and has only one global minimum and no local minima. So it does not face the issue of getting stuck in local minima. It can be understood better with the below plots. The gradient descent technique that uses all the training examples in each step is called Batch Gradient Descent. This is basically the calculation of the derivative term over all the training examples as it can be seen it the equation above. The equation of linear regression can also be solved using Normal Equations method, but it poses a disadvantage that it does not scale very well on larger data while gradient descent does. Implementation import random import matplotlib.pyplot as plt import math import numpy as np \"\"\" Dummy Data for Linear Regression \"\"\" data = [(1, 1), (2, 2), (3, 4), (4, 3), (5, 5.5), (6, 8), (7, 6), (8, 8.4), (9, 10), (5, 4)] \"\"\" Plot the line using theta_values \"\"\" def plot_line(formula, x_range): x = np.array(x_range) y = formula(x) plt.plot(x, y) \"\"\" Hypothesis Function \"\"\" def h(x, theta_0, theta_1): return theta_0 + theta_1 * x \"\"\" Partial Derivative w.r.t. theta_1 \"\"\" def j_prime_theta_1(data, theta_0, theta_1): result = 0 m = len(data) for x, y in data : result += (h(x, theta_0, theta_1) - y) * x return (1/m) * result \"\"\" Partial Derivative w.r.t. theta_0 \"\"\" def j_prime_theta_0(data, theta_0, theta_1): result = 0 m = len(data) for x, y in data : result += (h(x, theta_0, theta_1) - y) return (1/m) * result \"\"\" Cost Function \"\"\" def j(data, theta_0, theta_1): cost = 0 m = len(data) for x, y in data: cost += math.pow(h(x, theta_0, theta_1) - y, 2) return (1/(2*m)) * cost \"\"\" Simultaneous Update \"\"\" def update_theta(data, alpha, theta_0, theta_1): temp_0 = theta_0 - alpha * j_prime_theta_0(data, theta_0, theta_1) temp_1 = theta_1 - alpha * j_prime_theta_1(data, theta_0, theta_1) theta_0 = temp_0 theta_1 = temp_1 return theta_0, theta_1 \"\"\" Gradient Descent For Linear Regression \"\"\" def gradient_descent(data, alpha, tolerance, theta_0=None, theta_1=None): if not theta_0: theta_0 = random.random() * 100 if not theta_1: theta_1 = random.random() * 100 prev_j = 10000 curr_j = j(data, theta_0, theta_1) theta_0_history = [] theta_1_history = [] cost_history = [] while(abs(curr_j - prev_j) &gt; tolerance): try: cost_history.append(curr_j) theta_0_history.append(theta_0) theta_1_history.append(theta_1) theta_0, theta_1 = update_theta(data, alpha, theta_0, theta_1) prev_j = curr_j curr_j = j(data, theta_0, theta_1) except: break print(\"Stopped with Error at %.5f\" % prev_j) return theta_0, theta_1 theta_0, theta_1 = gradient_descent(data, alpha=0.01, tolerance=0.00001) The plot shows the adjustment of the values of \\(\\theta_0\\) and \\(\\theta_1\\) for fitting the best line through the given data points. REFERENCES: Machine Learning: Coursera - Gradient Descent for Linear Regression",
    "tags": "machine learning andrew ng",
    "url": "/2017/08/17/gradient-descent-for-linear-regression/"
  },

  
  
  
  {
    "title": "Gradient Descent",
    "text": "Gradient Descent Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent. Gradient descent is also known as steepest descent. What and How ? In the post Cost Function of Linear Regression, it was deduced that in order to find the best hypothesis, it is required to minimize the cost fuction \\(J(\\theta_0, \\theta_1)\\). And it is not always possible to achieve this goal manually as the complexity and dimensionality of the problem increases. To summarize, given the cost function \\(J(\\theta_0, \\theta_1)\\), the objective is, This is where gradient descent steps in. There are two basic steps involved in this algorithm: Start with some random value of \\(\\theta_0\\), \\(\\theta_1\\). Keep updating the value of \\(\\theta_0\\), \\(\\theta_1\\) to reduce \\(J(\\theta_0, \\theta_1)\\) until minimum is reached. Actually graident descent is a much more robust algorithm capable of solving higher dimensionality problems as well i.e. given a cost fuction \\(J(\\theta_0, \\theta_1, … , \\theta_n)\\), it can help achieve, Depending on initialization gradient descent can end up in different local minimas and a unique solution is not guaranteed. It can be seen in the plot above, if the initialization is at point A then it leads to B while if the initialization is at point C then it would lead to point D after execution. Definition Where := is the assignment operator \\(\\alpha\\) is the learning rate which basically defines how big the steps are during the descent \\( \\frac {\\partial} {\\partial \\theta_j} J(\\theta_0, \\theta_1)\\) is the partial derivative term j = 0, 1 represents the feature index number Also the parameters should be updated simulatenously, i.e. , The notion of simulatenous update is introduced because that is how the gradient descent would work naturally i.e. in nature the path taken at a point would be defined by the gradient along components at a point. But if the update is not simulatenous then the gradient is not computed at the same point because the updated value of one parameter is used in calculating the update of another. In practice this method might also work without any issues or behave strangely in some other cases, so by definition and the intuition of the gradient descent it would be incorrect implementation and would represent some other algorithm with properties different from those of gradient descent. Understanding Gradient Descent Consider a simpler cost function \\(J(\\theta_1)\\) and the objective, As defined earlier, start off by random initialization say at point A as shown in the plot below. Then according to the update definition of the algorithm, the derivative term is represented by the slope at any point also shown in the plot. In this case it would be a positive slope and since \\(\\alpha\\) is a positive real number the overall term \\(- \\alpha \\frac {d} {d \\theta_1} J(\\theta_1)\\) would be negative. This means the value of \\(\\theta_1\\) will be decreased in magnitude as shown by the arrows. Similary, if the initialization was at point B, then the slope of the line would be negative and then update term would be positive leading to increase in the value of \\(\\theta_1\\) as shown in the plot. So, no matter where the \\(\\theta_1\\) is initialized, the algorithm ensures that parameter is updated in the right direction towards the minima, given proper value for \\(\\alpha\\) is chosen. This is intuitive because that is exactly what is needed for minimizing the cost function. The term \\(alpha\\), as it can be seen from the equation will determine the magnitude of the update term, \\(- \\alpha \\frac {d} {d \\theta_1} J(\\theta_1)\\) i.e. if the value is higher the steps of update would be proportionally larger. Learning Rate, \\(\\alpha\\) A proper value of \\(\\alpha\\) plays an important role in gradient descent. Choose an alpha too small and the algorithm will converge very slowly or get stuck in the local minima. Choose an \\(\\alpha\\) too big and the algorithm will never converge either because it will oscillate between around the minima or it will diverge by overshooting the range. All these cases can be adequately understood by the plots below. The plot on the left shows a large learning rate. This leads to overshooting because the steps taken by the algorithm in updating the parameters are so big that the optimization problem can never converge to the minima. Instead it overshoots with every iteration to either start diverging or to oscillate between points that are not the optimum solution. The plot on the right is the case where learning rate is too small. As a result the steps taken i.e. the updates to the parameter \\(\\theta_n\\) are so small that it would take a very long time to converge. More so because as we approach the minima the value of the slope i.e. the value of the differential term will also decrease and as a result the update term i.e. the product of small \\(\\alpha\\) and small differential term would effect only a minute change. The other issue associated with the small learning rate is that of getting stuck in a local minima and hence never reaching the global minima. Gradient descent can converge to a local optimum, even with a fixed learning rate. Because as we approach the local minimum, gradient descent will automatically take smaller steps as the value of slope i.e. derivative decreases around the local minimum. The plot above tries to summarize the effect of \\(\\alpha\\) value on the convergence of the graident descent algorithm. The yellow plot shows the divergence of the algorithm when the learning rate is really high wherein the learning steps overshoot. The green plot shows the case where learning rate is not as large as the previous case but is high enough that the steps keep oscillating at a point which is not the minima. The red plot would be the optimum curve for the cost drop as it drops steeply initially and then saturates very close to the optimum value. The blue plot is the least value of \\(\\alpha\\) and converges very slowly as the steps taken by the algorithm during update steps are very small. Summary Gradient Descent is an optimization algorithm. It can be applied to minimize any cost function J, and not just to linear regression. It is a rather generalized powerful technique widely used in learning problems to reach the optimum parameter sets. It is coupled with various learning techniques and works perfectly well with all of them. For example gradient descent works equally well with linear regression and neural networks. REFERENCES: Machine Learning: Coursera - Gradient Descent",
    "tags": "machine learning andrew ng",
    "url": "/2017/08/15/gradient-descent/"
  },

  
  
  
  {
    "title": "Cost Function of Linear Regression",
    "text": "Linear Regression Linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression or univariate linear regression. For more than one explanatory variable, the process is called multiple linear regression. In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Hypothesis The hypothesis for a univariate linear regression model is given by, Where \\(h_\\theta (x)\\) is the hypothesis function, also denoted as \\(h(x)\\) sometimes \\(x\\) is the independent variable \\(\\theta_0\\) and \\(\\theta_1\\) are the parameters of the linear regression that need to be learnt Parameters of the Hypothesis In the above case of the hypothesis, \\(\\theta_0\\) and \\(\\theta_1\\) are the parameters of the hypothesis. In case of a univariate linear regression, \\(\\theta_0\\) is the y-intercept and \\(\\theta_1\\) is the slope of the line. Different values for these parameters will give different hypothesis function based on the values of slope and intercepts. Cost Function of Linear Regression Assume we are given a dataset as plotted by the ‘x’ marks in the plot above. The aim of the linear regression is to find a line similar to the blue line in the plot above that fits the given set of training example best. Internally this line is a result of the parameters \\(\\theta_0\\) and \\(\\theta_1\\). So the objective of the learning algorithm is to find the best parameters to fit the dataset i.e. choose \\(\\theta_0\\) and \\(\\theta_1\\) so that \\(h_\\theta (x)\\) is close to y for the training examples (x, y). This can be mathematically represented as, Where \\(h_\\theta(x^{(i)}) = \\theta_0 + \\theta_1\\,x^{(i)} \\) \\((x^{(i)},y^{(i)})\\) is the \\(i^{th}\\) training data m is the number of training example \\({1 \\over 2}\\) is a constant that helps cancel 2 in derivative of the function when doing calculations for gradient descent So, cost function is defined as follows, which is basically \\( {1 \\over 2} \\bar{x}\\) where \\(\\bar{x}\\) is the mean of squares of \\(h_\\theta(x^{(i)}) - y^{(i)}\\), or the difference between the predicted value and the actual value. And learning objective is to minimize the cost function i.e. This cost function is also called the squared error function because of obvious reasons. It is the most commonly used cost function for linear regression as it is simple and performs well. Understanding Cost Function Cost function and Hypthesis are two different concepts and are often mixed up. Some of the key differences to remember are, Hypothesis \\(h_\\theta(x)\\) Cost Function \\(J(\\theta_1)\\) For a fixed value of \\(\\theta_1\\), function of x Function of parameter \\(\\theta_1\\) Each value of \\(\\theta_1\\) corresponds to a different hypothesis as it is the slope of the line For any such value of \\(\\theta_1\\), \\(J(\\theta_1)\\) can be calculated using (3) by setting \\(\\theta_0 = 0\\) It is a linear line or a hyperplane Squared error cost function given in (3) is convex in nature Consider a simple case of hypothesis by setting \\(\\theta_0 = 0\\), then (1) becomes which corresponds to different lines passing through the origin as shown in plots below as y-intercept i.e. \\(\\theta_0\\) is nulled out. For the given training data, i.e. x’s marked on the graph, one can calculate cost function at different values of \\(\\theta_1\\) using (3) which can be expressed in the following form using (5), At \\(\\theta_1 = 2\\), At \\(\\theta_1 = 1\\), At \\(\\theta_1 = 0.5\\), On plotting points like this further, one gets the following graph for the cost function which is dependent on parameter \\(\\theta_1\\). In the above plot each value of \\(\\theta_1\\) corresponds to a different hypothesis. The optimization objective was to minimize the value of \\(J(\\theta_1)\\) from (4), and it can be seen that the hypothesis correponding to the minimum \\(J(\\theta_1)\\) would be the best fitting straight line through the dataset. The issue lies in the fact that we cannot always find the optimum global minima of the plot manually because as the number of dimensions increase, these plots would be much more difficult to visualize and interpret. So there is a need of an automated algorithm that can help achieve this objective. REFERENCES: Machine Learning: Coursera - Cost Function Machine Learning: Coursera - Cost Function Intuition I Linear Regression: Wikipedia - Cost Function",
    "tags": "machine learning andrew ng",
    "url": "/2017/08/11/cost-function-of-linear-regression/"
  },

  
  
  
  {
    "title": "Model Representation And Hypothesis",
    "text": "Notations \\(m\\): Number of training examples \\(x\\)’s: Input variables / features \\(y\\)’s: Output / Target variables \\((x,y)\\): One training example \\((x^{(i)},y^{(i)})\\): \\(i^{th}\\) training example Supervised Learning Formally stated, the aim of the supervised learning algorithm is to use the training Dataset and output a hypothesis function, h where h is a function that takes the input instance and predicts the output based on its learnings from the training dataset. As shown above, say given a dataset where each training instance consists of the area of a house and its price, the job of the learning algorithm would be to come up with a hypothesis, h such that it takes the size of house as input and predicts its price Hypothesis Hypothesis is the function that is to be learnt by the learning algorithm by the training progress for making the predictions about the unseen data. For example, for Linear Regression in One Variable or Univariate Linear Regression, the hypothesis, h is given by Where \\(h_\\theta (x)\\) is the hypothesis function, also denoted as \\(h(x)\\) sometimes \\(x\\) is the independent variable \\(\\theta_0\\) and \\(\\theta_1\\) are the parameters of the linear regression that need to be learnt REFERENCES: Machine Learning: Coursera - Model Representation",
    "tags": "machine learning andrew ng",
    "url": "/2017/08/10/model-representation-and-hypothesis/"
  },

  
  
  
  {
    "title": "Supervised and Unsupervised Learning",
    "text": "Supervised Learning Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). Regression: When the target variable is continuous. Regression in supervised learning is different from regression in statistics. Also, logistic regression is a classification technique despite its name as its response variable is categorical. Example: Given a picture of a person, predict the age on the basis of the given picture. Classification: When the target variable is categorical. Example: Given a patient with a tumor, predict whether the tumor is malignant or benign. Unsupervised Learning Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data. Clustering: Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). Applications: Organizing large computing clusters Social network analysis Market segmentation Astronomical data analysis Cocktail Party Algorithm: It is an example of source seperation algorithm or Independent Component Analysis (ICA). It can be considered to be opposite of clustering in some sense. REFERENCES: Supervised Learning: Wikipedia Machine Learning: Coursera - Supervised Learning Unsupervised Learning: Mathworks Machine Learning: Coursera - Unsupervised Learning Cluster Analysis: Wikipedia",
    "tags": "machine learning andrew ng",
    "url": "/2017/08/09/supervised-learning/"
  },

  
  
  
  {
    "title": "Introduction to Machine Learning",
    "text": "What is Machine Learning ? Machine learning is the science of getting computers to learn, without explicitly being programmed. It has developed as a subset of the larger problem of building AI i.e. Artificially Intelligent systems. Machine learning aims at developing new capabilities for computers wherein they can learn the objective intelligently without persistent human intervention, trying to mimic the way human brain learns. Application of Machine Learning Database Mining: Large Datasets are abundantly available which cannot be easily interpretted by human analysis. A machine learning algorithm can give better insights into such datasets such as web search click through data. Similarly, machine learning can help convert medical records to structured data for running various analysis on it such as survival analysis, disease prediction etc. It can also be applied to biology, engineering etc. Non-programmable Applications: For example, one cannot write a program for autonomous driving cars, handwriting recognitions, most of the NLP problems such as word-sense disambiguation, computer vision etc. Self-customizing Programs: For example, the recommender systems from websites like amazon and netflix are machine learning algorithms because it would be impossible to write programs manually to serve each consumer personalized recommendations. Understanding human learning: Understanding how the human brain works will help in the building of AI. Definations Field of study that gives the computers the ability to learn without being explicitly programmed. Well-Posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. Example: Playing chess E = the experience of playing many games of chess T = the task of playing chess P = the probability that the program will win the next game Types of Machine Learning Algorithms Supervised Learning Unsupervised Learning Others: Reinforcement Learning, Recommender Systems REFERENCES: Machine Learning: Coursera - Welcome Machine Learning: Coursera - What is Machine Learning",
    "tags": "machine learning andrew ng",
    "url": "/2017/08/09/introduction-to-machine-learning/"
  },

  
  
  
  {
    "title": "The Normal Distribution",
    "text": "Introduction The concept of normal distribution as explained using the game of darts is easier to understand and explain. Consider a game of dart with aim of throwing the dart at the origin of a cartesian plane. The errors in throwing the dart at the origin will have random errors and produce varying results in different trials. Some of the assumptions one can make in this game are: The errors do not depend on the orientation of the cartesian plane. Errors in perpendicular directions are independent i.e. dart hitting too high does not alter the probability of it being off to the right. Large errors are less likely to occur than the small errors. Determining the Shape of the Distribution The probability of dart falling in a region that lies in the vertical strip from \\(x\\) to \\(x + \\Delta x\\) can be given by, Similarly, the probability of dart landing in horizontal strip from \\(y\\) to \\(y + \\Delta y\\) can be given by, Because the two events are assumed to be independent, the probability of dart falling in the shaded region is given by Also, since orientation does not alter probability of an error, any region r units from the origin and with area \\(\\Delta x \\cdot \\Delta y\\) has the same probability and hence can be expressed as, This results in the inference Differentiating on both the sides, From the figure above, So the derivative in (1) can be expressed as, Using (2) and (3), (4) can be rewritten as, Differential equation can be solved by seperating the variables, so (5) becomes, Differential equation (6) is true for any x and y, and x and y are independent. This leads to the result that the ratio must be a constant, i.e., So, Integrating (7), So, Since, large errors are less likely than smaller errors, C must be negative, so, where k is positive. Determining the Coefficient A If p is the probability density function of a random variable following normal distribution, then the total area under the curve must be 1. So value of A should be such that this property is satisfied. The equation to ve evaluated is, Dividing both sides by A, Since the distribution is symmetric, changing the limits of distribution, Then, Since x and y are independent, (8) can be rewritten as double integral, The double integral in (9) can be evaluated as polar coordinates, Applying u-substitution to (9), differentiating w.r.t. r, So, Substituting (11) and (12) in (10), Using (9), So finally, Substituting A in the \\(p(x)\\) for normal distribution, Determining the value of k Probably k can be calculated using the formulae for mean or variance. The mean, \\(\\mu\\), is defined as the following integral, Since function \\(x\\,p(x)\\) is an odd function, \\(\\mu\\) is zero. The variance, \\(\\sigma^2\\), is given by following integral, Since mean is zero, above equation becomes, Changing the limits of integral, Evaluating the integral on left by parts where u and v are given by, Then v can be evaluated by substitution, So, Substituting (16) and (17) in (15), Applying the parts to (14) , Simplifying it further, Now consider the second part of the integral in (18), Let \\(k_1 = {k \\over 2}\\), and \\(u = x \\sqrt{k}\\) which means \\(du = dx \\sqrt{k} \\), and using gaussian integral. Substituting (19) and (20) in (18), So, Substituting A and k from (13) and (21) in the basic equation, The general equation for the normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is a simple horizontal shift of this basic distribution, REFERENCES: The Normal Distribution: A derivation from basic principles Derivation of univariate normal distribution",
    "tags": "concept mathematics probability",
    "url": "/2017/07/31/normal-distribution/"
  },

  
  
  
  {
    "title": "Random Projection in Dimensionality Reduction",
    "text": "Introduction Random Projections have emerged as a powerful method for dimensionality reduction. Theoretical results indicate that it preserves distances quite nicely but empirical results are sparse. It is often employed in dimensionality reduction in both noisy and noiseless data especially image and text data. Results of projecting on random lower-dimensional subspace yields results comparable to conventional methods like PCA etc but using it is computationally less expensive than the traditional alternatives. Curse of Dimensionality High dimensional data restricts the choice of data processing methods. A statistically optimal way of dimensionality reduction is to project the data onto a lower-dimensional orthogonal subspace that captures as much of the variations of the data as possible. The most widely used method of this sort is PCA (Principal Component Analysis). Drawback of PCA is however that it is computationally expensive to calculate as the dimensions of data increases. Johnson-Lindenstrauss Lemma If points in vector space are projected onto a randomly selected subspace of suitably high dimensions, then the distances between the points are approximately preserved. Random Projection (RP) In RP, a higher dimensional data is projected onto a lower-dimensional subspace using a random matrix whose columns have unit length. RP is computationally efficient, yet accurate enough for this purpose as it does not introduce a significant distortion in the data. It is not sensitive to impulse noise. So RP is promising alternative to some existing methods in noise reduction (like mean filtering) too. The original d-dimensional data is projected to a k-dimensional \\((k \\lt \\lt d)\\) through the origin, using a random \\(k * d\\) matrix R whose columns have unit lengths. It is given by Where \\(X_{k*N}^{RP}\\) is the k-dimensional random projection \\(R_{k*d}\\) is the random matrix used for transformation \\(X_{d*N}\\) are the original set of N d-dimensional observations The key idea of random mapping arises from Johnson-Lindenstrauss Lemma Complexity Forming a random matrix R and projecting d * N data matrix X into k dimensions is of the order O(dkN) If X is a sparse matrix with c non-zero values per column, then the complexity is O(ckN) Theoretically, equation (1) is not a projection because R is generally not orthogonal. A linear mapping like (1) can cause significant distortion in data if R is not orthogonal. Orthogonalizing R is computationally expensive. Instead of orthogonalizing, RP relies on the result presented by Hecht-Neilsen i.e. In a high dimensional space, there exists a much larger number of almost orthogonal than orthogonal directions. Thus the vectors with random directions might be sufficiently close to orthogonal, and equivalently \\(R^TR\\) would approximate an identity matrix. Experimental results show the mean squared difference between \\(R^TR\\) and identity matrix is around 1/k per element. Euclidean distances between \\(x_1\\) and \\(x_2\\) in the original large-dimensional space is given by After RP, this distance can be approximated by scaled Euclidean distance of these vectors in reduced spaces: Where d is the original and k is the reduced dimensionality of the data set and scaling factor \\(\\sqrt{d \\over k}\\) takes into account the decrease in dimensionality of the data set. According to Johnson-Lindenstrauss Lemma, the expected norm of a projection of unit vector onto a random subspace through origin is \\(\\sqrt{k \\over d}\\). The choice of R matrix is generally such that \\(r_{ij}\\) of R are often Gaussian Distributed although many other choices are available. There is a peculiar result which says one can use the following distribution which is much simpler. Practically, all zero mean, unit variance distributions of \\(r_{ij}\\) would give a mapping that satisfies the Johnson-Lindenstrauss Lemma. Equation (2) helps reduce the computational expense even further. Principal Component Analysis (PCA) PCA is eigenvalue decomposition of the data covariance matrix. It is computed as Where columns of E are the eigenvectors of the data covariance matrix \\(E{XX^T}\\) \\(\\Lambda\\) is a the diagonal matrix containing the respective eigenvalues. Dimensionality reduction of data set is obtained by projection on a subspace spanned by most important eigenvectors, given by Where The d * k matrix \\(E_k\\) contains the k eigenvectors corresponding to the k larges eigenvalues. PCA is the optimal way to project data in the mean-square sense, i.e. the squared error introduced in the projection is minimized over all projections onto a k-dimensional space. Eigen value decomposition of the data covariance matrix is very expensive. Computational Complexity : \\(O(d^2N) + O(d^3)\\) Singular Value Decomposition (SVD) Closely related to PCA, singular value decomposition is given by Where Orthogonal matrices U and V contain the left and right singular vectors of X respectively The diagonal matrix S contains the singular values of X. Using SVD, the dimensionality of data can be reduced by projecting data onto the space spanned by the left singular vectors corresponding to the k largest singular values, given by Where \\(U_k\\) is of size d * k and contains k singular vectors. Like PCA, SVD is also expensive to compute. For sparse data matrix \\(X_{d*N}\\) with about c non-zero entries per column, the computational complexity is of the order O(dcN). Latent Semantic Indexing (LSI) It is a dimensionality reduction method for text document data. Using LSI, the document data is represented in a lower-dimensional “topic” space: the documents are characterized by some underlying (latent, hidden) concepts referred to by the terms. LSI can be computed either by PCA or SVD of the data matrix of N d-dimensional document vectors. Discrete Cosine Transform (DCT) Widely used for image compression and can be used for dimensionality reduction of image data. Computational more efficient than PCA and has performance approachable to PCA. DCT is optimal for human eye: the distortions introduced occur at the highest frequencies only, neglected by human eye as noise. DCT can be performed by simple matrix operations: Image is first transformed to DCT space and dimensionality reduction is achieved during inverse transform by discarding the transform coefficients corresponding to highest frequencies. Computing DCT is not data-dependent, unlike PCA that needs the eigenvalue decomposition of data covariance matrix, which is why DCT is orders of magnitude cheaper to compute than PCA. Computational Complexity : \\(O(dN\\,log_2(dN))\\) for data matrix of size d * N. Notes About Random Projection RP does not distort the data significantly more than PCA. Also at smaller dimensions PCA seems to distort data more than RP because of removal of significantly important eigenvectors by elimination. Computational complexity of RP is significantly lesser than other methods like PCA, but more than DCT. So it can be inferred that RP are a better choice given the trade off of DCT accuracy for reduced complexity. At smaller dimension RP outperforms DCT both on accuracy and complexity. Use case can often be a factor in considering the most optimal way of dimensionality reduction. Say, even though RP outperforms DCT, it cannot be used for purposes where aim is to transmit the minimized dataset and re-obtain original data on the other end for human viewing. Psuedoinverse computation of R is expensive to compute but because R is almost orthogonal, \\(R^T\\) would be a good approximation of psuedoinverse. So the image can be computed from RP as, Where \\(X_{k*N}^{RP}\\) is the result of random projection. But the obtained image is visually worse than a DCT compressed image, to a human eye. So RP would serve very well in applications where distance or similarity between data vectors should be preseved under dimensionality reduction as well as possible, but where data is not intended to be visualized for the human eye, eg. machine vision. In case of text dataset, the error in dimensionality reduction is calculated by calculating the inner product among randomly chosen data pairs before and after transform. It is observed that RP is not as accurate as SVD but the error may be neglectable in various use cases. The possible cause could be that the Johnson-Lindenstrauss makes statement about Euclidean distance, but Inner product is a different metric even if Euclidean distance is maintained well. REFERENCES Random projection in dimensionality reduction: Applications to image and text data Various Embeddings on Digits Data - Python Sklearn",
    "tags": "concept machine learning mathematics",
    "url": "/2017/07/28/random-projection-in-dimensionality-reduction/"
  },

  
  
  
  {
    "title": "Continuous Random Variables",
    "text": "Probability Series Basic Probability Concepts Conditional Probability and Bayes’ Rule Discrete Random Variables Continuous Random Variables · · · Continuous Random Variables A continuous random variable is a function that maps the sample space of a random experiment to an interval in real value space. A random variable is called continuous if there is an underlying function f(x) such that Where f(x) is a non negative function called probability density function (pdf) Probability Density funtion can be considered analogous to Probability Mass function of Discrete random variable but differs in that pdf does give probability directly at a value like in case of pmf. Hence the rules of probability do not apply to f(x). Pdf takes value 0 for values outside range(X). Also the following property can be inferred from rules of probability, Probability of a continuous random variable taking a range of values is given by the area under the curve of f(x) for that range. Cumulative Distribution Function (cdf) Cdf for continuous random variable is same as the one for discrete random variable. It is given by, Properties of cdf: Unlike f(x), cdf(x) is probability and follows the laws and hence, As probability is non-negative, cdf is a non-decreasing function. Differential of cdf is given by Limits of cdf are given by Some Specific Distributions Uniform Distribution \\(X = Uniform(N)\\) is used to model a scenario where all outcomes are equally likely. Uniform([c, d]) is when all the values of \\(x(c \\leq x \\leq d)\\) are equally probable. It is given by Exponential Distribution Defined using a parameter \\(\\lambda\\) and has the pdf given by It is used to model the waiting time for an event to occur eg. waiting time for nuclear decay of radioactive isotope distributed exponentially and \\(\\lambda\\) is known as the half life of the isotope. This distribution exhibits lack of memory i.e. when waiting time is modeled using exponential distributions, the probability of it happening in next N minutes remains same irrespective of the time passed. Proof: According to lack of memory property, prove \\(P(X \\gt n+w | X \\gt w ) = P(X \\gt n)\\). Normal Distribution Most commonly used distribution Also known as Gaussian distribution It is denoted by \\(N(\\mu, \\sigma^2))\\) where \\(\\mu\\) is the mean and \\(\\sigma^2\\) is the variance fo the given distribution. Standard Normal Distribution, denoted by Z is a normal distribution with mean = 0 and variance = 1. It is symmetric about the y-axis and follows the bell-curve. It is given by Summary of Distributions Distribution pdf(x) cdf(x) \\(Uni(c, d)\\) \\({1 \\over d-c}\\) \\({x-c \\over d-c}\\) \\(Exp( \\lambda ), \\, x \\geq 0 \\) \\( \\lambda e^{- \\lambda x} \\) \\(1 - e^{- \\lambda x}\\) \\(N(\\mu, \\sigma^2)\\) \\({1 \\over \\sigma \\sqrt{2 \\pi} } e^{ \\frac {- (x - \\mu)^2} {2 \\sigma^2} }\\) \\({1 \\over 2}[1 + erf({ x - \\mu \\over \\sigma \\sqrt{2} })]\\) Expected Value Gives the average or the mean value over all the possible outcomes of the variable. Used to measure the centrality of a random variable. For a continuous random variable X, whose pdf is \\(f(x)\\), the expected value in the interval [c, d] is given by, Expected value is often denoted by \\(\\mu\\). \\(f(x)dx\\) denotes the probability value with which X can take the infinitesimal range dx. Some other properties of expected value of a random variable: \\[E(X+Y) = E(X) + E(Y) \\tag{3}\\] \\[E(cX + d) = c * E(X) + d \\tag{4}\\] Variance and Standard Deviation For a continuous random variable X, with Expected value \\(\\mu\\), variance is given by, \\[Var(X) = E((X-\\mu)^2) \\tag{5}\\] \\[\\sigma = \\sqrt (Var(X)) \\tag{6}\\] Some other properties of variance of a random variable: \\[Var(X) = E(X^2) - (E(X))^2 \\tag{7}\\] \\[Var(aX + b) = a^2 Var(X) \\tag{8}\\] \\[Var(X+Y) = Var(X) + Var(Y) \\text { iff X and Y are independent } \\tag{9}\\] Quartiles The value of \\(x\\) for which \\(cdf(x) = p\\) is called \\(p^{th}\\) quartile of X. So, median for the continuous random variable is the \\(0.5^{th}\\) quartile REFERENCES: Continuous Random Variables",
    "tags": "mathematics probability",
    "url": "/2017/07/28/continuous-random-variable/"
  },

  
  
  
  {
    "title": "Distributed Representations of Sentences and Documents",
    "text": "Drawbacks of Bag-of-Words Models Loss of word order which may be of importance in infering the meaning of sentence. Ignoring the semantics of the words i.e. different words may mean different things in different contexts. Bag-of-n-grams considers word only only in a short context and brings in data sparsity and high dimentionality. REFERENCES Distributed Representations of Sentences and Documents",
    "tags": "NLP",
    "url": "/2017/07/27/distributed-representation-of-sentences-and-documents/"
  },

  
  
  
  {
    "title": "Improvements on Word2Vec",
    "text": "Distributed Vector Representation Series Word2Vec Improvements on Word2Vec · · · Skip-Gram Model Training objective of skip-gram model is to deduce word representations that help in predicting the surrounding words in a sentence or a document, i.e. give a sequence of training words \\(w_1, w_2, w_3, … , w_T\\), the objective is to maximize the average log probability, Where c is the size of the training context Larger c results in more training examples and thus higher accuracy, at the expense of the training time. Basic skip-gram formulation defines \\(p(w_{t+j}|w_t)\\) using softmax function Where \\(v_w\\) and \\(v_w^{'}\\) are the input and output vector representations of \\(w\\) \\(W\\) is the number of words in the vocabulary cost of computing \\(\\nabla log\\,p(w_O| w_I)\\) is proportional to \\(W\\) which is quite large in order of \\(10^5 - 10^7\\) terms Drawbacks of Initial Word2Vec Proposed Training time Indifference to word order and inability to represent idiomatic phrases. Improvements Hierarchical Softmax Computationally efficient approximation of full softmax. Advantageous because instead of evaluating W output nodes in the neural network, only need to evaluate \\(log_2(W)\\) nodes. Uses binary tree representation of the output layer with W nodes as its leaves. Each node represents the relative probability of its child nodes. So, probability is assigned to a leaf node through random walk from root node Each word can be reached by following an appropriate path from the root. Let \\(n(w, j)\\) be the j-th node on the path from the root to w, and let \\(L(w)\\) be the length of this path. So, \\[n(w, 1) = root\\] \\[n(w, L(w)) = w\\] For any inner child n, let \\(ch(n)\\) be arbitrary fixed child of n and let \\(\\unicode{x27E6} x \\unicode{x27E7}\\) be 1 if x is true and -1 otherwise, then hierarchical softmax defines \\(p(w_O|w_I)\\) as Where \\(\\sigma (x) = {1 \\over (1 + exp(-x))}\\) \\(\\sum_{w=1}^W p(w|w_I) = 1\\) is verifiable. Negative Sampling Alternative to Hierarchical softmax. Based on Noise Contrastive Estimation(NCE) which posits that a good model should be able to differentiate data from noise by means of logistic regression. NCE approximately maximizes the log probability of softmax, but skip-gram only aims at learning high quality word representations and hence NCE can be simplified. Negative Sampling (NEG) is defined as It replaces the \\(log\\, p(w_O | w_I)\\) term in skip-gram objective Aiming at distinguishing between \\(w_O\\) from draws from noise distribution \\(P_n(w)\\) using logistic regression, where k is the number of negative samples for each data sample, because this use case does not require maximization of the softmax log probability. k value ranges 5-20 for small datasets and 2-5 for large datasets. NCE differs from NEG in that NCE needs the sample and numerical probabilities of the noise distribution, but NEG uses only samples. Both NCE and NEG have \\(P_n(w)\\) as a free parameter but unigram distribution U(w) raised to 3/4 power i.e. \\(U(w)^{3/4}/Z\\) is found to outperform other options like unigram and uniform distribution. Subsampling of frequent words In a corpus, most frequent words can occur hundreds of millions of time such as the stopwords. These words give little information by co-occuring with other words. Consequently, vector representations of frequent words do not change significantly after training on several million examples. Imbalance between rare and frequent words are thus countered using sub-sampling approach. Each word \\(w_i\\) in the training set is discarded with a probability given by Where \\(f(w_i)\\) is frequency of word \\(w_i\\) t is a chosen threshold, around \\(10^{-5}\\) Subsampling formula is chosen heuristically Learning Phrases Aim is to learn phrases where in individual words meaning is entirely different when compared to the group of words. Start off by finding words that frequently occur together and infrequently in other contexts. Theoretically, skip-gram model can be trained with all n-grams, but would be a very memory intensive operation. A data driven approach is put forward based on unigram and bigram counts, given by Where \\(\\delta\\) is a discounting coefficient and prevents phrases formed by very infrequent words. The bigrams with score above a given threshold only are used as phrases. Often it is needed to run the process 2-4 times changing the threshold values and seeing the quality of phrases formed. Conclusions Subsampling results in faster training and significantly better representations of uncommon words. Negative sampling helps accurately train for frequent words using a simple method. REFERENCES Distributed Representations of Words and Phrases and their Compositionality",
    "tags": "NLP machine learning papers",
    "url": "/2017/07/26/improvements-on-word2vec/"
  },

  
  
  
  {
    "title": "Discounted Cumulative Gain",
    "text": "Discounted cumulative gain DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks. Measure of ranking quality. Used to measure effectiveness of search algorithms in information retrieval. Underlying Assumptions Highly relevant documents are more useful if appearing earlier in search result. Highly relevant documents are more useful than marginally relevant documents which are better than non-relevant documents. DCG accumulated at a particular rank position \\(p\\) is given by Alternative formulation of DCG that places stronger emphasis on retrieving relevant documents is given by Both the alternatives are same if relevance values are binary i.e \\(rel_i \\in \\{0, 1\\}\\) Various formulations use \\(log_e\\) instead of \\(log_2\\). Logarithmic scale for reduction provides a smooth reduction curve and hence is used. DCG is a successor of Cumulative Gain. Cumulative Gain Does not include the position of a result in the calculation of gain of the result set. CG at a particular rank position \\(p\\) is given by \\[CG_p = \\sum_{i=1}^p rel_i\\] Where \\(rel_i\\) is the graded relevance of result at position \\(i\\). So, CG is unaffected by changes in ordering of search results and hence, DCG is used for more accurate measure. Normalized DCG Comparing a search algorithms performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of \\(p\\) should be normalized across queries. This is done by sorting all relevant documents in the corpus by their relative relevance, producing the maximum possible DCG through position \\(p\\), also called Ideal DCG (IDCG) through that position. Normalized DCG (nDCG) is given by Where Where \\(|REL|\\) is the list of documents ordered by relevance in the corpus up to position p. The average of nDCG for all queries gives a measure of average performance of ranking algorithms in the search process. For a *perfect ranking algorithm, \\[DCG_p = IDCG_p\\] \\[nDCG_p = 1.0\\] Since all nDCG are relative values between 0.0 and 1.0 they are cross-query comparable. The main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial relevance feedback is available. Example and Calculations Given a list of documents, each document is judged on a scale of 0 to 3 where 3 is the most relevant scaling down to 0 which is not relevant. Let a set of documents, S be Where relevance score by user survey is given by Relevance Set, R in the same order, CG is given by Changing the order of documents does not change the score. DCG using Logarithmic scale for reduction is given by i \\(rel_i\\) \\(log_2(i+1)\\) \\(\\frac {rel_i} {log_2(i+1)}\\) 1 3 1 3 2 2 1.585 1.262 3 3 2 1.5 4 0 2.322 0 5 1 2.585 0.387 6 2 2.807 0.712 Changing order of documents say \\(D_3\\) and \\(D_4\\) would decrease the score. The performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized. nDCG calculation using the ideal order and decrease sort of relevance score i \\(rel_i\\) \\(log_2(i+1)\\) \\(\\frac {rel_i} {log_2(i+1)}\\) 1 3 1 3 2 3 1.585 1.893 3 2 2 1 4 2 2.322 0.861 5 1 2.585 0.387 6 0 2.807 0 Drawbacks: Normalized DCG metric does not penalize for bad documents in the result, because results with relevance {1, 1, 1} and {1, 1, 1, 0} both given same nDCG while later is clearly the worse of the two. This can be fixed by adjusting values of relevance. If the relevance scores are 1, 0, -1 instead of 2, 1, 0 for Good, Fair, Bad the issue will be resolved nDCG does not penalize missing documents, because results with relevance {1, 1, 1} has same score as {1, 1, 1, 1, 1}. This can be fixed by considering fixed set size and use minimum score for missing documents which would lead to {1, 1, 1, 0, 0} and {1, 1, 1, 1, 1} where later has a higher nDCG. This would be written as nDCG@5 nDCG does not work well for query comparison when there are several equally good results. This affects the metrics when limited to only first few results. For example, for queries such as “restaurants” nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive. REFERENCES: Discounted cumulative gain",
    "tags": "concept machine learning mathematics",
    "url": "/2017/07/24/discounted-cumulative-gain/"
  },

  
  
  
  {
    "title": "Merge Sort",
    "text": "Merge Sort Merge sort is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Merge sort is a divide and conquer algorithm that was invented by John von Neumann in 1945. Time Complexity: \\(O(n\\,log\\,n)\\) Pseudocode Merge(A, p, q, r): n1 = p - q + 1 n2 = r - q L = arr[n1] R = arr[n2] for k = 1 to n1: L[k] = A[p + k - 1] for k = 1 to n2: R[k] = A[q + k] i = 1 j = 1 for k = p to r: if L[i] &lt;= R[j]: A[k] = L[i] i++ else A[k] = R[j] j++ Java Code public class MergeSort { public static int[] merge(int[] arr_l, int[] arr_r) { int[] arr = new int[arr_l.length + arr_r.length]; int k; int l = 0; int r = 0; for(k = 0; k &lt; arr.length; k++) { if(arr_l[l] &lt;= arr_r[r]) { arr[k] = arr_l[l]; l++; } else { arr[k] = arr_r[r]; r++; } if(l==arr_l.length || r==arr_r.length) { k++; break; } } for(int j = l; j &lt; arr_l.length; j++) { arr[k] = arr_l[j]; k++; } for(int j = r; j &lt; arr_r.length; j++) { arr[k] = arr_r[j]; k++; } return arr; } public static int[] merge_sort(int[] arr) { if(arr.length == 1) return arr; int mid = (int) arr.length/2; int left = mid; int[] arr_l = new int[left]; for(int k = 0; k &lt; left; k++) arr_l[k] = arr[k]; int right = arr.length-mid; int[] arr_r = new int[right]; for(int k = 0; k &lt; right; k++) arr_r[k] = arr[mid+k]; arr_l = merge_sort(arr_l); arr_r = merge_sort(arr_r); arr = merge(arr_l, arr_r); return arr; } public static void print_arr(int[] arr) { for(int i: arr) { System.out.printf(\"%d \", i); } System.out.println(\"\\n\"); } public static void main(String[] args) { int[] arr = {1, 9, 8, 7, 6, 10, 5, 9}; MergeSort.print_arr(MergeSort.merge_sort(arr)); } } Java Code for Inplace Merge Sort public class MergeSortInplace { static int[] arr; public static void merge(int start, int mid, int end) { int[] aux_arr = new int[end-start+1]; int k; for(k = 0; k &lt; end-start+1; k++) { aux_arr[k] = arr[start + k]; } k=start; int i = 0; int j = mid-start+1; while(i &lt;= mid-start &amp;&amp; j &lt;= end-start) { if(aux_arr[i] &lt;= aux_arr[j]) { arr[k] = aux_arr[i]; i++; } else { arr[k] = aux_arr[j]; j++; } k++; } while(i&lt;= mid-start) { arr[k] = aux_arr[i]; i++; k++; } while(j&lt;=end-start) { arr[k] = aux_arr[j]; j++; k++; } } public static void merge_sort(int start, int end) { if(start &gt;= end) return; int mid = start + (end-start)/2; merge_sort(start, mid); merge_sort(mid+1, end); merge(start, mid, end); } public static void print_arr(int[] arr) { for(int i: arr) System.out.printf(\"%d \", i); System.out.println(); } public static void main(String[] args) { int[] arr = {1, 3, 2, 5, 4, 7, 6, 10, 5, 8, 9}; MergeSortInplace.arr = arr; merge_sort(0, MergeSortInplace.arr.length-1); print_arr(MergeSortInplace.arr); } } REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 2 Merge Sort - Wikipedia",
    "tags": "algorithms DSA",
    "url": "/2017/07/21/merge-sort/"
  },

  
  
  
  {
    "title": "UMLS Knowledge Sources",
    "text": "UMLS The Unified Medical Language System (UMLS) is a compendium of many controlled vocabularies in the biomedical sciences (created 1986). It provides a mapping structure among these vocabularies and thus allows one to translate among the various terminology systems. It may also be viewed as a comprehensive thesaurus and ontology of biomedical concepts. UMLS further provides facilities for natural language processing. It is intended to be used mainly by developers of systems in medical informatics. UMLS consists of Knowledge Sources (databases) and a set of software tools. UMLS Knowledge Sources Metathesaurus: The Metathesaurus forms the base of the UMLS and comprises over 1 million biomedical concepts and 5 million concept names, all of which stem from the over 100 incorporated controlled vocabularies and classification systems. Some examples of the incorporated controlled vocabularies are ICD-10, MeSH, SNOMED CT, DSM-IV, LOINC, WHO Adverse Drug Reaction Terminology, UK Clinical Terms, RxNorm, Gene Ontology, and OMIM etc. Semantic Network: Each concept in the Metathesaurus is assigned one or more semantic types (categories), which are linked with one another through semantic relationships. The semantic network is a catalog of these semantic types and relationships. This is a rather broad classification; there are 127 semantic types and 54 relationships in total. Specialist Lexicon: The SPECIALIST Lexicon contains information about common English vocabulary, biomedical terms, terms found in MEDLINE and terms found in the UMLS Metathesaurus. Each entry contains syntactic (how words are put together to create meaning), morphological (form and structure) and orthographic (spelling) information. A set of Java programs use the lexicon to work through the variations in biomedical texts by relating words by their parts of speech, which can be helpful in web searches or searches through an electronic medical record. UMLS Metathesaurus to MySQL The setup procedure can be broken down into two steps mainly: Creating the MySQL Script Loading the MySQL Script to SQL Server. Creating MySQL Script (Current Version - 2017AA): Download the Full Release from UMLS Knowledge Sources which would be over 4.5GB in size and requires licence to login and download. Extract umls-2017AA-full.zip which requires over 30GB free space cd 2017AA-full Extract mmsys.zip Copy contents of folder mmsys to parent folder 2017AA-full Go to Terminal and run .\\run_linux.sh or .\\run_mac.command based on platform After MetaMorphosys starts click Install UMLS Select Source : &lt;path_to_folder&gt;/2017AA-full, Destination, and Metathesaurus only from Knowledge Source List. Click New Configuration and Accept Licence Agreement Notice Select Level 0 among options in Default Subset Configuration Input Data Format should be NLM Data File Format in Input Options tab Select Database to MySQL in Write Database Load Script section of Output Options tab Select all ENG souces from Source List after selecting INCLUDE in subset option Click Done in the Action Bar and let the process finish. Shortcut: Can skip the above step by downloading the 2017AA.tar.gz directly Loading From MySQL Script: Go to Destination folder provided in previous section cd 2017AA/META Edit populate_mysql_db.sh to provide details of MYSQL_HOME, user, password and db_name Save and run .\\populate_mysql_db.sh Run tail -f mysql.log to follow the logs. REFERENCES: Unified Medical Language System Unified Medical Language System - Knowledge Sources Downloads",
    "tags": "setup UMLS",
    "url": "/2017/07/19/umls-knowledge-sources/"
  },

  
  
  
  {
    "title": "Divide and Conquer",
    "text": "Recursion The process in which a function calls itself directly or indirectly is called recursion and the corresponding function is called as recursive function. Base Condition: In a recursive problem, solution to the base case is provided and solution to bigger problem is expressed in terms of smaller problems. Stack Overflow: In a recursion if the base condition is not reached and the function call stack reaches its limit, a stack overflow error is thrown. Direct vs Indirect Recursion: A function is direct recursive if it calls the same function, but a indirect recursive function calls a different method which inturn calls the original function. Tail Recursion: If the recursive call is the last thing executed by the function. Disadvantage: Greater space requirements and additional overhead for function calls and return values. Divide and Conquer Approach The divide and conquer paradigm involves three steps: Divide: Divide a problem into a number of smaller subproblems. Conquer: Solve the subproblems recursively. Combine: Combine solution to subproblems to get the solution of original problem. For example merge sort is based on divide and conquer paradigm. It follows the following steps: Divide array of size n to be sorted into two of size n/2. Sort the sub-arrays recursively. Combine the two sorted sub-arrays to get the sorted array. Base case is when the sub-array has only a single element and is trivially sorted. Auxiliary procedure Merge(A, p, q, r) where A is the array and p, q and r are indices into the array such that p &lt;= q &lt;= r. Merge procedure assumes that the sequence A[p .. q] and A[q+1 .. r] are in sorted order and return A[p .. r] in sorted order. Time Complexity of the Merge(A, p, q, r) procedure is O(n). REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 2 Recursion - GeeksforGeeks",
    "tags": "algorithms DSA",
    "url": "/2017/07/18/recursion-and-divide-and-conquer/"
  },

  
  
  
  {
    "title": "Basics of Linguistics",
    "text": "Dimensions of human language: Discrete infinity: A discrete set, a set with finite number of elements which can be used to make infinitely many combinations is called a discrete infinity. For example, the alphabets in english are 26 and form a discrete set but can be combined to form infinitely many sentences and hence is a discrete infinity. Displacement: It is the ability to communicate about time, space or any other abstract notion. For example, conversation about future or about good vs bad. Joint Attention: Human anguages can express shared goals. For example, conversation about voting for a leader. Sign language is also a human language as it serves the same purposes as a spoken language. Aphasia Aphasia is an inability to comprehend and formulate language because of damage to specific brain regions. This damage is typically caused by a cerebral vascular accident (stroke), or head trauma, however these are not the only possible causes. To be diagnosed with aphasia, a person’s speech or language must be significantly impaired in one (or several) of the four communication modalities following acquired brain injury or have significant decline over a short time period (progressive aphasia). The four communication modalities are auditory comprehension, verbal expression, reading and writing, and functional communication. Evolution of Language There are two different types of theories prevailing among linguistic communities, namely Continuity Based Theories: Based on the premise that human language is a complicated form of animal languages i.e. language exhibits so much complexity that one cannot imagine it simply appearing from nothing in its final form; therefore it must have evolved from earlier pre-linguistic systems among our primate ancestors. Discontinuity Based Theories: Based on theory that there is no connection among two but some drastic development occured that led to human langugages i.e. language, as a unique trait which cannot be compared to anything found among non-humans, must have appeared fairly suddenly during the course of human evolution. Ethnologue Ethnologue: Languages of the World is a web-based publication that contains information about the 7,099 living languages in its 20th edition, which was released in 2017. Ethnologue provides information on the number of speakers, location, dialects, linguistic affiliations, autonym of the language, availability of the Bible in each language and dialect described, a cursory description of revitalization efforts where reported, and an estimate of language viability using the Expanded Graded Intergenerational Disruption Scale (EGIDS). Phonetics vs Phonology Phonetics is about the physical aspect of sounds, it studies the production and the perception of sounds, called phones. Phonetics has some subcategories, but if not specified, we usually mean articulatory phonetics: that is, the study of the production of speech sounds by the articulatory and vocal tract by the speaker. Phonology is about the abstract aspect of sounds and it studies the phonemes. Phonology is about establishing what are the phonemes in a given language, i.e. those sounds that can bring a difference in meaning between two words. A phoneme is a phonic segment with a meaning value. International Phonetic Alphabet The International Phonetic Alphabet (IPA) is an alphabetic system of phonetic notation based primarily on the Latin alphabet. It was devised by the International Phonetic Association in the late 19th century as a standardized representation of the sounds of spoken language. Vowels vs Consonants A vowel is a speech sound made with your mouth fairly open, the nucleus of a spoken syllable. A consonant is a sound made with your mouth fairly closed. Dimensions of Consonants Major Dimensions of the consonants are: Place of articulation - Where in the vocal tract the obstruction of the consonant occurs, and which speech organs are involved. Places include bilabial (both lips), alveolar (tongue against the gum ridge), and velar (tongue against soft palate). Manner of articulation - How air escapes from the vocal tract when the consonant or approximant (vowel-like) sound is made. Manners include stops, fricatives, and nasals Voicing or Phonation - How the vocal cords vibrate during the articulation. When the vocal cords vibrate fully, the consonant is called voiced; when they do not vibrate at all, it is voiceless. REFERENCES: Miracles of Human Language Aphasia - Wikipedia Origin of Language Ethnologue What’s the difference between phonetics and phonology? International Phonetic Alphabet The difference between consonants and vowels Consonants",
    "tags": "concept references",
    "url": "/2017/07/14/introduction-to-linguistics/"
  },

  
  
  
  {
    "title": "Pseudocode",
    "text": "Pseudocode Logic of algorithms is often expressed as pseudocode which is similar in many respects to languages like C, C++, Java etc. Major difference lies in that pseudocode uses the most concise and meaningful way of expressing the algorithm which can be sometimes as simple as plain english. Also pseudocode does not get into the issues of software engineering like abstraction, modularity or error handling. Pseudocode for insertion sort can be written as: InsertionSort(A): for i = 1 to A.length-1 key = A[i] j = i - 1 while j &gt; 0 and a[j] &gt; key: A[j+1] = A[j] j-- A[j+1] = key Loop Invariant and Correctness of Algorithm A loop invariant is a condition [among program variables] that is necessarily true immediately before and immediately after each iteration of a loop. (Note that this says nothing about its truth or falsity part way through an iteration.) Loop invariants are used to prove the correctness of an algorithm. There are three properties of loop invariant that must hold for the correctness of algorithm. Initialization: It is true before the first iteration of the loop. Maintenance: If it is true before an iteration of the loop, it remains true before the next iteration. Termination: It is true when the loop terminates. Loop invariant works in a way similar to mathematical induction. So to prove a algorithm works, invariant must work before first iteration (base step), and it must hold between consecutive iterations (inductive step). General Pseudocode Conventions Indentation indicates block stucture similar to python language. General looping constructs used are while, for, if-else etc and loop counter retains its value after loop terminates. Variables are local to the given procedures. Parameters are passed by value. Boolean operators and and or are short-circuiting. REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 2 Algorithm - What is loop invariant?",
    "tags": "algorithms DSA",
    "url": "/2017/07/13/pseudocode/"
  },

  
  
  
  {
    "title": "Insertion Sort",
    "text": "Insertion Sort Given an array A[1…n] of length n to be sorted. The algorithm sorts the list inplace. It picks an element i as key and places it in the correct position in the sorted A[1..(i-1)]. Time Complexity: \\(O(n^2)\\) Space Complexity: \\(O(n)\\) Pseudocode InsertionSort(A): for i = 1 to A.length-1 key = A[i] j = i - 1 while j &gt; 0 and a[j] &gt; key: A[j+1] = A[j] j-- A[j+1] = key Java Code public class InsertionSort { public int[] insertionSort(int[] arr) { int length = arr.length; int key, j, i; for(j = 1; j &lt; length; j++) { key = arr[j]; i = j-1; while(i &gt;= 0 &amp;&amp; arr[i] &gt; key) { arr[i+1] = arr[i]; i--; } arr[i+1] = key; } return arr; } public static void print_arr(int[] arr) { for(int i: arr) { System.out.printf(\"%d \", i); } } public static void main(String[] args) { InsertionSort i = new InsertionSort(); int[] arr = {9, 8, 7, 6, 5}; int[] sorted = i.insertionSort(arr); InsertionSort.print_arr(sorted); } } REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 2",
    "tags": "algorithms DSA",
    "url": "/2017/07/13/insertion-sort/"
  },

  
  
  
  {
    "title": "Algorithms and Data Structures",
    "text": "Algorithm Any well-defined computational procedure that takes some value, or set of values, as input and produces some value, or set of values, as output. So, algorithm is a sequence of computational steps that transform the input into the output. An algorithm is correct if, for every input instance of a problem, it halts with the correct output. Incorrect algorithms can sometimes be useful, if the error rate is controlled. Measures of a good algorithm Time Complexity: Time taken to run. Space Complexity: Auxilliary space needed to run the algorithm. Data Structures A data structure is a way to store and organize data in order to facilitate access and modifications. No single data structure works well for all purposes, and all of them have their strengths and weeknesses. Hard Problems General measure of efficiency is speed for all algorithms. But some problems there are no efficient solutions known. A subset of these problems are also known as NP-complete problems. These problems have the 3 interesting properties: Although no efficient algorithm for an NP-complete problem has been found, its neither been proved that an efficient algorithm for one cannot exist i.e. no one knows whether or not efficient algorithm exist for NP-complete problems. If an efficient algorithm exists for any one of them, then efficient algorithms exist for the rest of them. Several NP-complete problems are similar, but not identical, to problems for which efficient algorithms are known i.e. small changes in problem statements cause a big change in efficiency of the best known algorithm. Travelling Salesman Problem is a classic example of a NP-complete problem encountered in everyday application. REFERENCES: Introduction to Algorithms 3rd Edition - Chapter 1",
    "tags": "algorithms DSA",
    "url": "/2017/07/12/algorithms-in-computing/"
  },

  
  
  
  {
    "title": "Word2Vec",
    "text": "Distributed Vector Representation Series Word2Vec Improvements on Word2Vec · · · Introduction Computing the continuous vector representations of words from very large data sets. Current state-of-the-art performance on semantic and syntactic word similarities. Classical techniques treat words as atomic units without any notion of similarities between them because they are represented using indices in a vocabulary (bag-of-words). Advantages of classical techniques lie in simplicity, robustness and accuracy of simple model when trained on large data sets over complex models trained on less data. Disadvantage of these methods is observed when the amount of data available to train is limited in certain fields like say, automatic speech recognition and machine translations. Previous Works Neural Network Language Model (NNLM): Consists of input, projection, hidden and output layers. Input layer has N previous words encoded using 1-in-V coding, where V is the size of Vocabulary. Projection layer, P has a projection of input layer has a dimensionality of \\(N * D\\) and uses a projection matrix. High complexity between projection and hidden layer due to dimensions of the dense projection layer. Computational complexity of NNLM per training example is given by \\[Q = N * D + N * D * H + H * V\\] Where Q is the computational cost N is the number of previous words used for learning D is the dimensionality of the projection layer H is the size of hidden layer V is the size of the vocabulary and output layer. \\(H * V\\) is the dominating term above which was proposed to be reduced to as less as \\(H * log_2(V)\\) using Hierarchical softmax Avoiding normalized models for training Binary tree representations of the vocabulary using Huffman Trees So, the major complexity is dominated by \\(N * D * H\\) Recurrent Neural Network Language Model (RNNLM): Overcome the limitations of NNLM such as need to specify the context length, N (order of the model N) Theoretically RNNs can efficiently represent more complex patterns than shallow neural networks. No projection layer Consists of Input, hidden and output layers. Develops a short term memory of seen data in the self-fed time delayed hidden layer. Computational complexity of NNLM per training example is given by \\[Q = H * H + H * V\\] Where Q is the computational cost H is the size of hidden layer V is the size of the vocabulary and output layer. Word representations D have the same dimensionality as the hidden layer H. Again, \\(H * V\\) will be reduced to \\(H * log_2(V)\\) using Hierarchical softmax. So, the major complexity is dominated by \\(H * H\\) It’s observed that most complexity is contributed by the non-linearity of the hidden layer in the networks. Continuous Bag-of-Words Model (CBOW) Similar to feedforward NNLM, but the non-linear hidden layer is removed. Projection layer is shared for all the words. So all words are projected into the same position and their vectors are averaged. Model is called bag-of-words model because the order of words in the history or future does not influence the projections. Unlike NNLM, words from future are used to with the best result found with 4 history and 4 future words in context. Training criterion is the correct classification of the current(middle) word. Training complexity is given by \\[Q = N * D + D * log_2(V)\\] Model is continuous bag-of-words because unlike standard bag-of-words it uses continuous distributed representations of the context. Weights between the input and the projection layer is shared for all words positions in the same way as in NNLM. Continuous Skip-Gram Model Similar to CBOW but slight changes in training criterion. Instead of predicting current word from the surrounding words in the window, current word is used to predict the words surrounding the current word. Accuracy and quality of vector is found to increase as the number of context words predicted is increased, but that increased the computational complexity as well. Training complexity is given by \\[Q = C * (D + D * log_2(V))\\] Where C is the maximum distance of the words. Say, C=5 is chosen then a number \\(R \\in [1, C]\\) is selected randomly and then R words from history and R from future are correct labels of the current word. Model Architectures Results Algebraic operations on the vector representations actually give meaningful results like cosine similary of \\(vector(X)\\) is closest to \\(vector(‘smallest’)\\) where \\[vector(X) = vector(‘biggest’) - vector(‘big’) + vector(‘small’)\\] Subtle relationships are learnt when accurate data is used. For example, France is to Paris as Germany is to Berlin. After a certain point adding more dimensionality to the word vectors or adding more training data provides diminishing improvements. NNLM vectors work better than RNNLM because word vectors in RNNLM are directly connected to non-linear hidden layer. CBOW is better than NNLM on syntactic tasks and about the same on semantic tasks. Skip-Gram works slightly worse than CBOW but better than NNLM on syntactic tasks and much better on semantic tasks. Training time for Skip-Gram model is greater than CBOW model. REFERENCES: Efficient Estimation of Word Representations in Vector Space",
    "tags": "NLP machine learning papers",
    "url": "/2017/07/11/word-to-vector-word-representations/"
  },

  
  
  
  {
    "title": "Building Stopword List for Information Retrieval System",
    "text": "What are stopwords ? Words in a document that are frequently occuring but meaningless in terms of Information Retrieval (IR) are called stopwords. Use of a fixed set of stopwords across various documents of different kinds is not suggested because as the context changes so does the utility of a word. For example, a word like economy might not be a stopword in context of automobiles but would be a stopword in an Economic Times Newspaper. Also the pattern of words changes over time as the trends change, so the list of stopwords used should keep up with the trends in word usages. They are also called noise words or the negative dictionary. Zipf’s law The law states that given some corpus of natural language, the frequency of any word is inversely proportional to its rank in the frequency table i.e. the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc. This law can be seen in action in the Brown Corpus of English text, where the is the most frequently occuring word and accounts for 7% of the word occurences and of is the second most occuring word which is approximately 3.5% of the corpus, followed by and. And only 135 words in the vocabulary account for half the Brown Corpus. The same relationship can be seen in other rankings unrelated to language, such as population rank of cities etc. It is an empirical law formulated using mathematical statistics that states that many types of data in physical and social sciences can be approximated with a Zipfian Distribution (ZD). ZD belongs to a family of discrete power law probabiliy distributions. Observations It can be seen from Zipf’s Law that a relatively small number of words account for a very significant fraction of all text’s size. These terms make very poor index terms because of their low discriminative value. Kullback–Leibler Divergence It is the measure of how on probability distribution diverges from a second expected probability distribution. Applications lie in finding the relative (Shannon) Entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. In summary it can help find the amount of information a word provides in a corpus. And the lesser the information a word has the more likely it is to be a stopword. Classical Methods Zipf’s Law can be mathematically represented by \\[F(r) = \\frac{C}{r_\\alpha}\\] Where \\(\\alpha \\approx 1\\) \\(C \\approx 0.1\\) \\(r\\) is the rank frequency Four different classical methods exist by replacing the term frequency \\(r\\) above with one of the four refinements given below. Term Frequency (TF): The number of times a term occurs in a specific collection. Normalized TF: Generated by normalizing the term frequency by the total number of tokens in the collection i.e. the size of the lexicon file given by Where TF is the term frequency \\(v\\) is total number of tokens in the lexicon file Inverse Document Frequency (IDF): Calculated using the TF distribution where IDF of term k is given by \\[idf_k = log (\\frac{N_{Doc}}{D_k})\\] Where \\(N_{Doc}\\) is the total number of documents in the corpus \\(D_k\\) is the number of documents containing term k So infrequently occuring terms have a greater probability of occuring in relevant documents and hence are more informative. Normalized IDF: Many normalizing techniques are used in this case but one of the most frequently used ones is given by Robertson and Sparck-Jones which normalizes with respect to number of documents not containing the term and adds a constant to both numerator and denominator to moderate extreme values. Where \\(N_{Doc}\\) is the total number of documents in the corpus \\(D_k\\) is the number of documents containing term k A threshold value is to be determined to produce the best average precision. It cannont be chosen at random. It is needed to check the difference between the frequency of consecutive ranks say \\(F(r)\\) and \\(F(r+1)\\) because if the difference is big enough threshold can be set as \\(frequency \\geq F(r)\\) for choosing the stopwords. # imports import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer # documents is the list of documents in the collection vocab = list(set([i for sub_doc in documents for i in sub_doc.strip().split()])) # TfidfVectorizer from sklearn vectorizer = TfidfVectorizer( stop_words=None, norm=None, min_df=0, sublinear_tf=True, token_pattern=r'(?u)\\b\\w+\\b', vocabulary=vocab ) # calculate tf-idf def tf_idf(documents): return vectorizer.fit_transform(documents) # create dataframe from pandas tf_idf = pd.DataFrame({'word': vectorizer.vocabulary, 'idf': vectorizer.idf_}, index=None) tf_idf.sort_values('idf') def get_word_idf(word): return tf_idf.loc[tf_idf.word==word] Term Based Random Sampling Approach Based on how informative a particular term is. Importance of term is determined using Kullback–Leibler divergence measure. Approach is similar to idea of query expansion in which a query is expanded based on a particular query term. The idea is to find terms that complement the initially chosen query terms which follows from the idea that a individual term might be inadequate to express a concept accurately. It differs from the standard approach in that instead of finding the similar terms, find all the documents containing the current term and use them as the new sample and then extract the least informative term from the sample by measuring divergence of a given term distribution within the sampled document set from its distribution in the collection. After this Kullback–Leibler divergence can be used to measure the importance of each term. Weight of a term t in the sampled document set is given by Where \\(P_x = \\frac{tf_x}{l_x}\\) \\(P_c = \\frac{F}{token_c}\\) \\(tf_x\\) is the frequency of the query term in the sampled documents \\(l_x\\) is the sum of the length of the sampled document set F is the term frequency of the query term in the collection \\(token_c\\) is the total number of tokens in the whole collection Issues and Solutions Selecting a random term has the possibility of finding only one document containing that term which would result in a relatively small sample. This problem can be solved by repeating selection step Y times which would theoretically result in a better sample, creating a better view of term distribution and their importance. Advantages: Easier to implement than the classical methods even though algorithm looks complex. Because all steps here are automatic and do not require manual interventions like in the classical techniques for choosing proper threshold. Classical techniques need to check \\(F(r)-F(r+1)\\) listing one by one and tf-idf graph is needed for zipf’s law. # imports import pandas as pd from collections import Counter, defaultdict import re from math import log # documents is the list of documents in the collection # Collection Analysis tokens = Counter(re.findall(r'\\w+', \" \".join(documents))) def F(word): return tokens.get(word) TOKEN_C = len(tokens) def P_c(word): return float(F(word))/TOKEN_C # creating inverted index def create_index(data): index = defaultdict(list) for i, document in enumerate(data): for token in document.strip().split(): index[token].append(i) return index inv_index = create_index(documents) # sample analysis def P_x(word): sample = [documents[i] for i in inv_index[word]] tokens_sample = Counter(re.findall(r'\\w+', ' '.join(sample))) L_x = 0 for k, v in tokens_sample.items(): L_x += v return float(tokens_sample[word])/L_x # kullback leibler divergence def kl_div(word): p_x = P_x(word) p_c = P_c(word) return p_x * log(p_x/p_c, 2) # collection analysis if the dataset is not huge terms = list(tokens.keys()) kl_div_val = [] for t in terms: kl_div_val.append(kl_div(t)) df_kl_div = pd.DataFrame({'term': terms, 'kl_div': kl_div_val}) df_kl_div.sort_values('kl_div') def get_word_kl_metric(word): return df_kl_div.loc[df_kl_div.term == word] REFERENCES: Automatically Building a Stopword List for an Information Retrieval System Zipf’s law Kullback–Leibler Divergence",
    "tags": "NLP machine learning papers",
    "url": "/2017/07/04/building-stopword-list-for-information-retrieval-system/"
  },

  
  
  
  {
    "title": "Zero Sum Subarrays",
    "text": "Q: Given an array find all sub arrays that have sum 0. Input: a = [0, 1, -1, 4, 2, -3, -1, 0, 4] Output: [0] [0, 1, -1] [1, -1] [0] [-3, -1, 0, 4] Algorithms: Logic: Hash sums and ending indices. If a sum is seen previously then the elements in between sum to 0. Complexity: Time: O(n) Space: O(n) a = [0, 1, -1, 4, 2, -3, -1, 0, 4] sum_idx = {} s = 0 for i in range(len(a)): s = s + a[i] l = sum_idx.get(s, []) if s == 0: [print(a[: i+1])] [print(a[j+1: i+1]) for j in l] l.append(i) sum_idx[s] = l REFERENCES: 500 Data Structures and Algorithms practice problems and their solutions Find sub-array with 0 sum",
    "tags": "array algorithms references algorithms",
    "url": "/2017/07/03/zero-sum-subarray/"
  },

  
  
  
  {
    "title": "Sort Binary Array",
    "text": "Q: Sort a given binary array. Input: a = [1, 0, 1, 0, 1, 0, 0, 1] Output: [0, 0, 0, 0, 1, 1, 1, 1] Algorithms: Logic: Iterate and fill available position with 0 if 0 found in list. Fill remaining positions with 1s. Complexity: Time: O(n) Space: O(1) a = [1, 0, 1, 0, 1, 0, 0, 1] k = 0 for i in a: if i==0: a[k] = 0 k += 1 for i in range(k, len(a)): a[i] = 1 Quicksort Logic: Complexity: Time: O(n) Space: O(1) a = [1, 0, 1, 0, 1, 0, 0, 1] pivot = 1 start = 0 end = len(a) - 1 j = 0 while start &lt; end: if a[j] &lt; pivot: a[j], a[start] = a[start], a[j] start += 1 j += 1 elif a[j] &gt;= pivot: a[j], a[end] = a[end], a[j] end -= 1 REFERENCES: 500 Data Structures and Algorithms practice problems and their solutions Sort binary array in linear time",
    "tags": "array algorithms references algorithms",
    "url": "/2017/07/03/sort-binary-array/"
  },

  
  
  
  {
    "title": "Pair Sums In An Array",
    "text": "Q: Given an array find a pair of element with the given sum. Input: a = [8, 7, 2, 5, 3, 1] s = 10 Output: (8, 2) (7, 3) Algorithms: Hashing: Complexity: Time: O(n) Space: O(n) a = [8, 7, 2, 5, 3, 1] s = 10 h = {} for idx, elem in enumerate(a): h[elem] = idx diff = h.get(s-elem, None) if diff != None and diff != idx: print((elem, s-elem)) Sorting: Complexity: Time: O(nlogn) Space: O(1) a = [8, 7, 2, 5, 3, 1] s = 10 a.sort() l = 0 h = len(a) - 1 while (l&lt;h): temp = a[l] + a[h] if temp == s: print((a[l], a[h])) l = l+1 h = h-1 elif temp &gt; s: h = h-1 else: l = l+1 REFERENCES: 500 Data Structures and Algorithms practice problems and their solutions Find pair with given sum in the array",
    "tags": "array algorithms references algorithms",
    "url": "/2017/07/03/pair-sums-in-an-array/"
  },

  
  
  
  {
    "title": "Duplicate In Limited Range Array (XOR)",
    "text": "Q: Find duplicate in limited range (1 to n-1) array of size n using XOR operation. Input: a = [1, 2, 3, 4, 4] Output: 4 Algorithms: Logic: XOR all the numbers XOR with all the numbers between 1 to n-1. a ^ a = 0 0 ^ 0 = 0 a ^ 0 = a Complexity: Time: O(n) Space: O(1) a = [1, 2, 3, 4, 4] xor = 0 for elem in a: xor = xor ^ elem for i in range(len(a)): xor = xor ^ i print(xor) REFERENCES: 500 Data Structures and Algorithms practice problems and their solutions Find a duplicate element in a limited range array",
    "tags": "array algorithms references algorithms",
    "url": "/2017/07/03/duplicate-in-limited-range-array-using-xor/"
  },

  
  
  
  {
    "title": "Blockchain",
    "text": "What is blockchain ? Originally developed as a part of digital currency Bitcoin. Blockchain can support a wide variety of applications such as peer-to-peer payment services, supply chain tracking etc. Digital Record: A blockchain is a record of transactions like a traditional ledger, where a transaction can be any movement of money, goods or data. Secure: It stores data in a way that it is virtually impossible to tamper the data without being detected by other users. Decentralized: Blockchain uses decentralized verification systems that uses consensus of multiple users instead of traditional centralized ones regulated by a verified authority like a government or a credit card clearinghouse. How does it work ? Blockchain Steps: First, gather and order data into blocks. Second, chain them together securely using cryptography. Recording a Transaction: Say A sells car to B. Transaction information is recorded and shared with other systems on the blockchain network. Building transactions into Blocks: On the network, the record is combined with other transactions to form a block and each transaction is time stamped. Upon completion, a block also gets a time stamp. All the data is sequential which helps to avoid duplicate records. Connecting blocks into Chains: Completed block is sent out across the network and appended to the chain. Other participants may also be sending out their blocks but the time stamp ensures the correct order of blocks and participants have the latest versions. Securing the chain: Security is maintained using a hash and the cryptographic math makes the links between blocks made using these hashes virtually unbreakable. A hash function takes the information in the block to create the hash which is a unique string of characters easy to generate but almost impossible to back trace to original data. Locking it down: The hash from one block is added to the data in the next block. So, when the next block goes through the hash function a trace of hash from previous block is woven into the new hash. The same is repeated further down the chain. Raising the Alarm: So if there is any tampering with a previously created block the hash encoded in the next block will not match up anymore. The mismatch will cascade through all subsequent blocks denoting an alteration in the chain. Establishing Trust: Since all the participants have a copy of the block chain they can detect any data tampering. If hashes match up across the chain, all parties know they can trust their records. Blockchain in Action (Examples) Enormous potential. Because they establish trust, they provide simple, paperless way to establish ownership of money, information and objects - like concert tickets. Trusted Concert Tickets: Trusted Seller: It’s hard to tell a real ticket from a counterfeit, especially if bought through a third-party website or a private individual. Going Straight to the Source: Blockchain can help buyers quickly establish that a ticket (and its seller) can be trusted. The event venue will register all tickets to a blockchain which would be accessible online. When a ticket is sold it will be assigned an address - a string of data publically viewable on the blockchain. Owner is given a private key which is a hash of the address data. The key can be used to unlock the address. So by producing the correct key the buyer can prove that the item is theirs without checking with the event venue. If they choose to sell it, it is assigned a new address, and new owner gets a new private key and the transaction is added to the blockchain. The ticket can be sold multiple number of times and when a seller unlocks the ticket with their private key, the buyer knows that the ticket they are getting is authentic. More Efficient Markets: Removing the Bottlenecks: In the financial markets the trade happens in a fraction of second, but the actual exchanging of assets and payments can take days, involving multiple banks and clearinghouses which can cause errors, delays and other unnecessary risks. Smart Contract: A piece of computer code that describes a transaction step by step. It can connect to multiple blockchains, tracking multiple assets so it can swap those assets as needed to execute transactions. A broker only needs to buy stock on behalf of a client. The order will be placed with the private keys of both the buyer and seller. This will trigger the execution of a smart contract. It connects to multiple blockchains, verifies the availability of the stock and the payment and then makes the transfers between the seller and the buyer. Digital ID: Digitally issued IDs via a blockchain would be more secure mechanism than the traditional ones issued by governments. Internation ID Blockchain, accessible anywhere in the world, allows people to prove identity, connect with family member or receive money without a bank account. A person is a fingerprint. Fingerprint is digitized and added to blockchain with other data like name etc. To prove identity they need to give their fingerprint which can be used to unlock and verify their ID. Improvements Needed: Early stage of the technology. Has various hurdles to overcome: Departure from manual work for businesses would add new costs and new risks which leads to reluctance to adopt the new tech. Current blockchain technologies like bitcoin can support only 5-8 transactions per second which cannot keep up with applications like credit card transactions which amount to be around 10000 times what is supported. Even though its transparent in ledgering there are no real standardization of implementation, which is required for relaibility and other legal issues. Even though it uses business grade cryptography, it is not 100% secure. Large sums of money transaction therefore would be reluctant to adopt this technology. REFERENCES: Blockchain by Goldman Sachs",
    "tags": "concept references",
    "url": "/2017/07/02/blockchain/"
  },

  
  
  
  {
    "title": "Discrete Random Variables",
    "text": "Probability Series Basic Probability Concepts Conditional Probability and Bayes’ Rule Discrete Random Variables Continuous Random Variables · · · Discrete Random Variables Even though it is named variable, discrete random variable is actually a function that maps the sample space to a set of discrete real values. Where X is the random variable S is the sample space R is the set of real numbers Probability Mass Function Probability mass function is the probability defined over a given random variable, i.e., it gives the probability that a discrete random variable is exactly equal to some value in the sample space, S. For a random variable X, \\(p(X=c)\\) is denoted as \\(p(c)\\) and the mapping of each value in sample space to their respective probabilities is known as pmf. For all values c that are not is sample space \\(p(c) = 0\\) because it is pointing to an empty set. Commutative Distribution Function Probability defined over an inequality such as \\(X \\leq c\\) gives probabilities of all the events that satisfy the condition from \\(-\\infty\\) to c i.e. the probability value estimate for X less than or equal to c. Mathematically, Explaination Set has 5 boys and 5 girls. 3 kids selected at random but gender not known. X is the random variable that denotes number of girls selected. Event c such that \\(X(c) = 3\\) is given by set \\(c = \\{(GGG)\\}\\) i.e. all three girls selected are girls. The pmf for random variable X will be as follows: a p(a) CD(a) 0 1/12 1/12 1 5/12 6/12 2 5/12 11/12 3 1/12 1 Calculations: Number of ways of selecting k kids from total of N kids is given by \\(C_k^N = \\frac{N!}{k! (N-k)!}\\) implies \\(C_3^{10} = \\frac{10!}{3! (10-3)!} = 120\\) for N = 10 and k = 3. value 1 : a = 0 means that no girl is selected which implies that k boys are selected out of the total B boys. The number of ways this can be accomplished is given by \\(C_k^B = \\frac{B!}{k! (B-k)!}\\) implies \\(C_3^5 = \\frac{5!}{3! (5-3)!} = 10\\) for B = 5 and k = 3. For commutative distribution \\(CD(2) = p(0) + p(1) + p(2) = 11/12\\). Similarly the value of \\(p(a)\\) and \\(CD(a)\\) at other values of \\(a\\) can be calculated. Random Variable Metrics Expected Value The average or mean value calculated over all the possible outcomes of the random variable. \\[E(X) = \\sum_n v_i * p(v_i) \\tag{3}\\] X is the random variable. \\(v_i\\) is the value the random variable takes with probability \\(p(v_i)\\). sample space size is n. often represented by \\(\\mu\\). it is a measure of central tendency of the random variable. Some other properties of expected value of a random variable: Variance and Standard Deviation Variance gives the dispersion of probability mass around the mean value (i.e. E(X), expected value) of the random variable. \\[Var(X) = E((X-\\mu)^2) \\tag{6}\\] \\[\\sigma = \\sqrt (Var(X)) \\tag{7}\\] \\(\\sigma\\) is the standard deviation. Some other properties of variance of a random variable: \\[Var(X) = E(X^2) - (E(X))^2 \\label{8} \\tag{8}\\] \\[Var(aX + b) = a^2 Var(X) \\label{9} \\tag{9}\\] \\[Var(X+Y) = Var(X) + Var(Y) \\text{ iff X and Y are independent } \\tag{10}\\] Derivation of equation \\eqref{8} Derivation of equation \\eqref{9} using equations \\eqref{5}, \\eqref{8} Some Specific Distributions Uniform Distribution All outcomes are eqully possible. Eg: Probability of getting a heads or tails for a fair coin. Uniform(N) implies N outcomes and each has probability 1/N. Bernoulli Distribution Used to model the random experiment where each trail has exactly 2 possible outcomes. One possible outcome is termed as success and other as failure. Parameter \\(p\\) is the probability of success in an experiment. Random variable \\(X \\in \\{0, 1\\}\\). X = 1 has probability \\(p\\) and X = 0 has probability \\(1-p\\) where 0 is denoting failure and 1 denoting success. Binomial Distribution Used to model \\(n\\) independent traits of Bernoulli Distribution. If X follows Binomial(n, p) then, X = k implies, the event of k successes in n independent Bernoulli trials. \\[P(X=k) = C_k^n * (p^k) * ((1-p)^{n-k})\\] \\(C_k^n\\) is the number of ways of picking k success events in n total trials. \\((p^k) * ((1-p)^{n-k})\\) gives the combined probability of the n Bernoulli trails. Geometric Distribution Also defined over Bernoulli distribution. Models the event of k failures before first success. \\(geometric(p)\\) is given by \\[P(X=k) = (1-p)^k * p\\] where \\(X = k\\) is the event where first success occured after k failures. If X and Y follows geometric distribution with same probability p, then \\(X + Y\\) is also a geometric distribution. Expected Value and Variance for Distributions Distribution E(X) Var(X) Uniform \\(\\frac{n+1}{2}\\) \\(\\frac{n^2-1}{12}\\) Bernoulli \\(p\\) \\(p(1-p)\\) Binomial \\(np\\) \\(np(1-p)\\) Geometric \\(\\frac{1-p}{p}\\) \\(\\frac{1-p}{p^2}\\) REFERENCES: Discrete Random Variables Expectation and Variance Variance",
    "tags": "mathematics probability",
    "url": "/2017/06/23/discrete-random-variables/"
  },

  
  
  
  {
    "title": "Conditional Probability and Bayes’ Rule",
    "text": "Probability Series Basic Probability Concepts Conditional Probability and Bayes’ Rule Discrete Random Variables Continuous Random Variables · · · Conditional Probability The probability of event X given that Y has already occurred is denoted by \\(P(X|Y)\\) If X and Y are independent: \\(P(X|Y) = P(X)\\) because event X is not dependent on event Y. If X and Y are mutually exclusive: \\(P(X|Y) = 0\\) because X and Y are disjoint events. Product Rule From \\eqref{1}, following can be concluded, \\(X \\subseteq Y\\) implies \\(P(X|Y) = P(X)/P(Y)\\) because \\(X \\cap Y = X\\) \\(Y \\subseteq X\\) implies \\(P(X|Y) = 1\\) because \\(X \\cap Y = Y\\) The distributive, associative and De Morgan’s laws are valid for conditional probability. \\[P(X \\cup Y|Z) = P(X|Z) + P(Y|Z) - P(X \\cap Y|Z)\\] \\[P(X^{c}|Z) = 1-P(X|Z)\\] Chain Rule Bayes’ Theorem Where \\(P(X) = P(X \\cap Y) + P(X \\cap Y^{c})\\) from the sum rule. Derivation of Bayes’ Theorem From \\eqref{1}, Using the commutative law, From \\eqref{2}, \\eqref{3} and, \\eqref{4}, Hence, EXAMPLE \\(p_ot\\) is probability of reaching on time when no car trouble. \\(p_ct\\) is probability of car trouble. Commute by train if car trouble occurs. N is the number of trains available. Only 2 of the N trains would reach on time. What is the probability of reaching on time. Explaination: \\(O\\): reach on time \\(C^c\\): car not working \\(P(O) = P(O \\cap C) + P(O \\cap C^c)\\) \\(P(O \\cap C) = P(O|C) * P(C) \\text{ where } P(C) = 1 - P(C^c)\\) \\(P(O \\cap C^c) = P(O|C^c) * P(C^c) \\text{ where } P(O|C^c) = 2/N\\) p_ct = float(input()) # P(Car Trouble) p_ot = float(input()) # P(On Time | No Car Trouble) N = float(input()) # Number of trains p_rt = 2.0/N # P(Correct Train) p_o = p_ct * p_rt + (1-p_ct) * p_ot # P(On Time) print(\"%.6f\" % p_o) REFERENCES: Bayes’ rules, Conditional probability, Chain rule",
    "tags": "mathematics probability",
    "url": "/2017/06/22/bayes-rule-conditional-probability-chain-rule/"
  },

  
  
  
  {
    "title": "The One With Github Pages",
    "text": "GitHub Pages is a web hosting service offered by GitHub for hosting static web pages for GitHub users, user blogs, project documentation, or even whole books. It is integrated with the Jekyll software for static web site and blog generation. The Jekyll source pages for a web site can be stored on GitHub as a Git repository, and when the repository is updated, github servers will automatically regenerate the site. GitHub Pages was launched in late 2008. As with the rest of GitHub, it includes both free and paid tiers of service, instead of being supported by web advertising. Web sites generated through this service are hosted either as subdomains of the github.io domain, or as custom domains bought through a third-party domain name registrars. Steps for setup: Fork this repository. Update repository name to &lt;user_name&gt;.github.io. Customize _config.yml to match user credentials. In _includes/comments.html edit disqus_shortname. Update index.html bio. Commit the changes. Done! This site is build on top of the customizations by psteadman on lanyon theme which derives from poole. Head over to the post Using Jekyll, Poole and Lanyon to setup my github user page for further references. Additions: Added MathJax for adding equations. Added Facebook Sidebar Icon using font-awesome. Added Search bar to search through the posts using Tipue Search. Added Social sharing buttons for posts sharing on platforms like Reddit, Facebook etc. Added Typed.js developed by Matt Boldt for text typing using javascript. Added Travis CI Build and Deployment as it helps debug issues in deployment easier. REFERENCES: Github Pages - Wikipedia Using Jekyll, Poole and Lanyon to setup my github user page",
    "tags": "setup github pages jekyll featured",
    "url": "/2017/06/16/initial-site-setup/"
  },

  
  
  
  {
    "title": "Basic Probability Concepts",
    "text": "Probability Series Basic Probability Concepts Conditional Probability and Bayes’ Rule Discrete Random Variables Continuous Random Variables · · · Introduction Trail or Experiment - The act that leads to a result with certain possibility. Sample Space - Set of all possible outcomes of an experiment. Event - Non empty subset of a sample space. Basic Probability Formula \\[P(A) = \\sum_{i=1}^n P(E_i) \\label{1} \\tag{1} \\] Where A is an event, S is the sample space, \\(E_1 … E_n\\) are the n outcomes in A. If \\(E_1 … E_n\\) are equally likely to occur, then \\eqref{1} can be written as, \\[P(A) = {\\text{number of outcomes in A} \\over \\text{total number of possible outcomes}} \\label{2} \\tag{2}\\] From \\eqref{2}, following results can be inferred, \\(0 \\leq P(A) \\leq 1\\), \\(P(S) = 1\\) Complement of an Event Compliment of an event A is defined as all the outcomes of the sample space, S that are not in A, i.e. \\[P(A^c) = 1 - P(A) \\tag{3} \\] Where \\(A^c\\) is used to denote the compliment of A. Union and Intersection of Events \\[P(A \\cup B) = P(A) + P(B) + P(A \\cap B) \\label{4} \\tag{4}\\] Mutually Exclusive Events Two events A and B are mutually exclusive if there are no overlapping outcomes, i.e., the intersection of the two experiments is a null set. \\[P(A \\cap B) = 0 \\label{5} \\tag{5}\\] Using \\eqref{4} and \\eqref{5}, Independent Events Two events A and B are independent if occurence of one does not affect the probability of the other occuring and is mathematically given by, \\[P(A \\cap B) = P(A) * P(B)\\] Sum Rule or Marginal Probability \\[P(A) = \\sum_{B} P(\\text{A and B})\\] EXAMPLE M wants to go fishing this weekend to nearby lake. His neighbour A is also planing to go to the same spot for fishing this weekend. The probability that it will rain this weekend is \\(p_1\\). There are two possible ways to reach the fishing spot (bus or train). The probability that M will take the bus is \\(p_{mb}\\) A will take the bus is \\(p_{ab}\\). Travel plans of both are independent of each other and rain. What is the probability \\(p_{rs}\\) that M and A meet each other only (should not meet in bus or train) on a lake in rain ? p_mb = float(input()) # P(M taking Bus) p_ab = float(input()) # P(A taking Bus) p_1 = float(input()) # P(Rain) p_rs = p_1 * (1 - p_ab*p_mb - (1-p_ab)*(1-p_mb)) # P(Meet at lake only) print(\"%.6f\" % p_rs) REFERENCES: Basic Probability Models and Rules",
    "tags": "mathematics probability",
    "url": "/2017/06/16/basic-probability-models-and-rules/"
  }

]};
